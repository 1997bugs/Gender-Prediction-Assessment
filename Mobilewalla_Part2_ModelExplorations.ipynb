{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Model Exploration \n",
    "## Feature Extraction - Dimensionality Reduction\n",
    "## Model Building \n",
    "## Class imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adarsh_23/mobilewalla_dataset/app_meta.txt\n",
      "/Users/adarsh_23/mobilewalla_dataset/Mobilewalla_Part1_EDA.ipynb\n",
      "/Users/adarsh_23/mobilewalla_dataset/app_metadata_two.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/app_propensity_data.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/Untitled (2).ipynb\n",
      "/Users/adarsh_23/mobilewalla_dataset/mice_imputed_values.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/Mobilewalla_Test_ModelTraining.ipynb\n",
      "/Users/adarsh_23/mobilewalla_dataset/app_metadata.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/clean_df.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/train_data.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/ifa_catscores.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/test_data.csv\n",
      "/Users/adarsh_23/mobilewalla_dataset/Mobilewalla_Part1_EDA.html\n",
      "/Users/adarsh_23/mobilewalla_dataset/catboost_info/learn_error.tsv\n",
      "/Users/adarsh_23/mobilewalla_dataset/catboost_info/time_left.tsv\n",
      "/Users/adarsh_23/mobilewalla_dataset/catboost_info/catboost_training.json\n",
      "/Users/adarsh_23/mobilewalla_dataset/catboost_info/learn/events.out.tfevents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import skew\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected = True)\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "colors = ['#494BD3','#E28AE2','#F1F481','#79DB80','#DF5F5F',\n",
    "          '#69DADE','#C2E37D','#E26580','#D39F49','#B96FE3']\n",
    "\n",
    "sns.palplot(sns.color_palette(colors))\n",
    "plt.title('Notebook Colors', size = 12)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#/Users/adarsh_23/mobilewalla_dataset\n",
    "filepath = '/Users/adarsh_23/mobilewalla_dataset'\n",
    "for dirname, _, filenames in os.walk('/Users/adarsh_23/mobilewalla_dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading two dataframes -\n",
    "# one - clean_df.csv - cleaned dataframe - train \n",
    "# two - ifa_catscores.csv - integrated app data\n",
    "df1 = pd.read_csv(os.path.join(filepath, 'clean_df.csv'))\n",
    "df2 = pd.read_csv(os.path.join(filepath, 'ifa_catscores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>platform</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>...</th>\n",
       "      <th>dev_cat_iab9_23</th>\n",
       "      <th>dev_cat_iab9_30</th>\n",
       "      <th>dev_cat_books</th>\n",
       "      <th>dev_cat_finance</th>\n",
       "      <th>dev_cat_games</th>\n",
       "      <th>dev_cat_lifestyle</th>\n",
       "      <th>dev_cat_navigation</th>\n",
       "      <th>dev_cat_productivity</th>\n",
       "      <th>dev_cat_reference</th>\n",
       "      <th>dev_cat_utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf648e9f9bbe18fe6125713360be1e34dfa544b4fd91c8...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-01-02 16:07:14</td>\n",
       "      <td>2020-03-07 13:51:43</td>\n",
       "      <td>13.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4.15385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7317fe8366d24119d193d4c7ce5b5c9762ab70015171d...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-03-01 12:45:55</td>\n",
       "      <td>2020-01-17 17:51:51</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>27.46923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab5cc2d010deaf23dfb33a18361e40c6658c12087f4e48...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-06-06 09:46:05</td>\n",
       "      <td>2020-03-30 06:22:17</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4517</td>\n",
       "      <td>39.97345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048361</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7a05851bf3482a062d67c243960c4d34879ff1df44860...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-09-10 07:33:22</td>\n",
       "      <td>2020-03-31 08:27:46</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6186</td>\n",
       "      <td>55.23214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7ee413f5f44783ebd036b30f62a7d8cd94a5a844e27416...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-09-10 06:34:52</td>\n",
       "      <td>2020-03-31 15:01:39</td>\n",
       "      <td>50.0</td>\n",
       "      <td>269</td>\n",
       "      <td>5.38000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa platform  \\\n",
       "0  bf648e9f9bbe18fe6125713360be1e34dfa544b4fd91c8...  ANDROID   \n",
       "1  c7317fe8366d24119d193d4c7ce5b5c9762ab70015171d...  ANDROID   \n",
       "2  ab5cc2d010deaf23dfb33a18361e40c6658c12087f4e48...  ANDROID   \n",
       "3  c7a05851bf3482a062d67c243960c4d34879ff1df44860...  ANDROID   \n",
       "4  7ee413f5f44783ebd036b30f62a7d8cd94a5a844e27416...  ANDROID   \n",
       "\n",
       "            first_seen            last_seen  num_days   brq  brq_engagement  \\\n",
       "0  2020-01-02 16:07:14  2020-03-07 13:51:43      13.0    54         4.15385   \n",
       "1  2019-03-01 12:45:55  2020-01-17 17:51:51     130.0  3571        27.46923   \n",
       "2  2019-06-06 09:46:05  2020-03-30 06:22:17     113.0  4517        39.97345   \n",
       "3  2019-09-10 07:33:22  2020-03-31 08:27:46     112.0  6186        55.23214   \n",
       "4  2019-09-10 06:34:52  2020-03-31 15:01:39      50.0   269         5.38000   \n",
       "\n",
       "   gender  distinct_app  skewness_female  ...  dev_cat_iab9_23  \\\n",
       "0     1.0           8.0              0.0  ...         0.000000   \n",
       "1     1.0          29.0              0.0  ...         0.000000   \n",
       "2     1.0           6.0              0.0  ...         0.048361   \n",
       "3     1.0          36.0              0.0  ...         0.000000   \n",
       "4     1.0           3.0              0.0  ...         0.000000   \n",
       "\n",
       "   dev_cat_iab9_30  dev_cat_books  dev_cat_finance dev_cat_games  \\\n",
       "0         0.071429            0.0              0.0      0.071429   \n",
       "1         0.331195            0.0              0.0      0.257537   \n",
       "2         0.007354            0.0              0.0      0.001122   \n",
       "3         0.427170            0.0              0.0      0.078837   \n",
       "4         0.004723            0.0              0.0      0.000000   \n",
       "\n",
       "   dev_cat_lifestyle  dev_cat_navigation  dev_cat_productivity  \\\n",
       "0           0.153061                 0.0              0.081633   \n",
       "1           0.000000                 0.0              0.001604   \n",
       "2           0.000000                 0.0              0.011966   \n",
       "3           0.000000                 0.0              0.006967   \n",
       "4           0.000000                 0.0              0.246753   \n",
       "\n",
       "   dev_cat_reference  dev_cat_utilities  \n",
       "0                0.0           0.112245  \n",
       "1                0.0           0.001710  \n",
       "2                0.0           0.227845  \n",
       "3                0.0           0.006967  \n",
       "4                0.0           0.310508  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>6000</th>\n",
       "      <th>6001</th>\n",
       "      <th>6002</th>\n",
       "      <th>6003</th>\n",
       "      <th>6004</th>\n",
       "      <th>6005</th>\n",
       "      <th>6006</th>\n",
       "      <th>6007</th>\n",
       "      <th>6008</th>\n",
       "      <th>...</th>\n",
       "      <th>PERSONALIZATION</th>\n",
       "      <th>PHOTOGRAPHY</th>\n",
       "      <th>PRODUCTIVITY</th>\n",
       "      <th>SHOPPING</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002c2abb627089988287fa9cb1569b43076bdfdad67d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.81407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00009f893e849715bcc809dd0891e78bc4ef9dd3061eac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.03187</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000ac97665d160ff6c012f0af44a13a83d0065cdd8046...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.39687</td>\n",
       "      <td>0.03031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000bef11e506141ce0f4159e0412038c892477ff7752b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000123a2d98e624b7f11b4fc81f9879bc8b7aab9c7dc75...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.37289</td>\n",
       "      <td>0.27507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa  6000  6001  6002  6003  \\\n",
       "0  00002c2abb627089988287fa9cb1569b43076bdfdad67d...   0.0   0.0   0.0   0.0   \n",
       "1  00009f893e849715bcc809dd0891e78bc4ef9dd3061eac...   0.0   0.0   0.0   0.0   \n",
       "2  0000ac97665d160ff6c012f0af44a13a83d0065cdd8046...   0.0   0.0   0.0   0.0   \n",
       "3  0000bef11e506141ce0f4159e0412038c892477ff7752b...   0.0   0.0   0.0   0.0   \n",
       "4  000123a2d98e624b7f11b4fc81f9879bc8b7aab9c7dc75...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   6004  6005  6006  6007  6008  ...  PERSONALIZATION  PHOTOGRAPHY  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0  ...           0.0000      0.00000   \n",
       "1   0.0   0.0   0.0   0.0   0.0  ...           0.7635      0.03187   \n",
       "2   0.0   0.0   0.0   0.0   0.0  ...           0.0000      0.39687   \n",
       "3   0.0   0.0   0.0   0.0   0.0  ...           0.0000      0.01475   \n",
       "4   0.0   0.0   0.0   0.0   0.0  ...           0.0000      0.37289   \n",
       "\n",
       "   PRODUCTIVITY  SHOPPING   SOCIAL  SPORTS    TOOLS  TRAVEL_AND_LOCAL  \\\n",
       "0       0.00000       0.0  0.00708     0.0  0.17354               0.0   \n",
       "1       0.00000       0.0  0.00646     0.0  0.02586               0.0   \n",
       "2       0.03031       0.0  0.00000     0.0  0.51015               0.0   \n",
       "3       0.00000       0.0  0.00000     0.0  0.00000               0.0   \n",
       "4       0.27507       0.0  0.00000     0.0  0.25724               0.0   \n",
       "\n",
       "   VIDEO_PLAYERS  WEATHER  \n",
       "0        0.81407      0.0  \n",
       "1        0.00000      0.0  \n",
       "2        0.06267      0.0  \n",
       "3        0.00000      0.0  \n",
       "4        0.09481      0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189846, 111)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>platform</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>...</th>\n",
       "      <th>PERSONALIZATION</th>\n",
       "      <th>PHOTOGRAPHY</th>\n",
       "      <th>PRODUCTIVITY</th>\n",
       "      <th>SHOPPING</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf648e9f9bbe18fe6125713360be1e34dfa544b4fd91c8...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-01-02 16:07:14</td>\n",
       "      <td>2020-03-07 13:51:43</td>\n",
       "      <td>13.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4.15385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11166</td>\n",
       "      <td>0.11149</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7317fe8366d24119d193d4c7ce5b5c9762ab70015171d...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-03-01 12:45:55</td>\n",
       "      <td>2020-01-17 17:51:51</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>27.46923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02458</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00518</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab5cc2d010deaf23dfb33a18361e40c6658c12087f4e48...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-06-06 09:46:05</td>\n",
       "      <td>2020-03-30 06:22:17</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4517</td>\n",
       "      <td>39.97345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27266</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63930</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7a05851bf3482a062d67c243960c4d34879ff1df44860...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-09-10 07:33:22</td>\n",
       "      <td>2020-03-31 08:27:46</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6186</td>\n",
       "      <td>55.23214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7ee413f5f44783ebd036b30f62a7d8cd94a5a844e27416...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2019-09-10 06:34:52</td>\n",
       "      <td>2020-03-31 15:01:39</td>\n",
       "      <td>50.0</td>\n",
       "      <td>269</td>\n",
       "      <td>5.38000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18772</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.53382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa platform  \\\n",
       "0  bf648e9f9bbe18fe6125713360be1e34dfa544b4fd91c8...  ANDROID   \n",
       "1  c7317fe8366d24119d193d4c7ce5b5c9762ab70015171d...  ANDROID   \n",
       "2  ab5cc2d010deaf23dfb33a18361e40c6658c12087f4e48...  ANDROID   \n",
       "3  c7a05851bf3482a062d67c243960c4d34879ff1df44860...  ANDROID   \n",
       "4  7ee413f5f44783ebd036b30f62a7d8cd94a5a844e27416...  ANDROID   \n",
       "\n",
       "            first_seen            last_seen  num_days   brq  brq_engagement  \\\n",
       "0  2020-01-02 16:07:14  2020-03-07 13:51:43      13.0    54         4.15385   \n",
       "1  2019-03-01 12:45:55  2020-01-17 17:51:51     130.0  3571        27.46923   \n",
       "2  2019-06-06 09:46:05  2020-03-30 06:22:17     113.0  4517        39.97345   \n",
       "3  2019-09-10 07:33:22  2020-03-31 08:27:46     112.0  6186        55.23214   \n",
       "4  2019-09-10 06:34:52  2020-03-31 15:01:39      50.0   269         5.38000   \n",
       "\n",
       "   gender  distinct_app  skewness_female  ...  PERSONALIZATION  PHOTOGRAPHY  \\\n",
       "0     1.0           8.0              0.0  ...              0.0      0.00000   \n",
       "1     1.0          29.0              0.0  ...              0.0      0.00000   \n",
       "2     1.0           6.0              0.0  ...              0.0      0.27266   \n",
       "3     1.0          36.0              0.0  ...              0.0      0.00000   \n",
       "4     1.0           3.0              0.0  ...              0.0      0.00000   \n",
       "\n",
       "   PRODUCTIVITY  SHOPPING   SOCIAL  SPORTS    TOOLS  TRAVEL_AND_LOCAL  \\\n",
       "0       0.11163       0.0  0.11164     0.0  0.11166           0.11149   \n",
       "1       0.00000       0.0  0.00000     0.0  0.02458           0.00000   \n",
       "2       0.00000       0.0  0.04029     0.0  0.00000           0.00000   \n",
       "3       0.00000       0.0  0.00000     0.0  0.00000           0.00000   \n",
       "4       0.00000       0.0  0.00000     0.0  0.18772           0.00000   \n",
       "\n",
       "   VIDEO_PLAYERS  WEATHER  \n",
       "0        0.00000      0.0  \n",
       "1        0.00518      0.0  \n",
       "2        0.63930      0.0  \n",
       "3        0.00000      0.0  \n",
       "4        0.53382      0.0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = df1.merge(df2, left_on= 'ifa', right_on='ifa')\n",
    "print(merged_df.shape)\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns - (ifa, first_seen, last_seen) don't seem to be contributing much to the gender prediction - let's drop them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>...</th>\n",
       "      <th>PERSONALIZATION</th>\n",
       "      <th>PHOTOGRAPHY</th>\n",
       "      <th>PRODUCTIVITY</th>\n",
       "      <th>SHOPPING</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANDROID</td>\n",
       "      <td>13.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4.15385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.01852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11166</td>\n",
       "      <td>0.11149</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANDROID</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>27.46923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.47797</td>\n",
       "      <td>0.11537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02458</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00518</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANDROID</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4517</td>\n",
       "      <td>39.97345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413549</td>\n",
       "      <td>0.57619</td>\n",
       "      <td>0.14880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27266</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63930</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANDROID</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6186</td>\n",
       "      <td>55.23214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42396</td>\n",
       "      <td>0.24654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANDROID</td>\n",
       "      <td>50.0</td>\n",
       "      <td>269</td>\n",
       "      <td>5.38000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200743</td>\n",
       "      <td>0.50246</td>\n",
       "      <td>0.00985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18772</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.53382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform  num_days   brq  brq_engagement  gender  distinct_app  \\\n",
       "0  ANDROID      13.0    54         4.15385     1.0           8.0   \n",
       "1  ANDROID     130.0  3571        27.46923     1.0          29.0   \n",
       "2  ANDROID     113.0  4517        39.97345     1.0           6.0   \n",
       "3  ANDROID     112.0  6186        55.23214     1.0          36.0   \n",
       "4  ANDROID      50.0   269         5.38000     1.0           3.0   \n",
       "\n",
       "   skewness_female  skewness_male  daypart_home  daypart_other  ...  \\\n",
       "0              0.0       0.037037       0.62963        0.01852  ...   \n",
       "1              0.0       0.008681       0.47797        0.11537  ...   \n",
       "2              0.0       0.413549       0.57619        0.14880  ...   \n",
       "3              0.0       0.000000       0.42396        0.24654  ...   \n",
       "4              0.0       0.200743       0.50246        0.00985  ...   \n",
       "\n",
       "   PERSONALIZATION PHOTOGRAPHY  PRODUCTIVITY  SHOPPING   SOCIAL  SPORTS  \\\n",
       "0              0.0     0.00000       0.11163       0.0  0.11164     0.0   \n",
       "1              0.0     0.00000       0.00000       0.0  0.00000     0.0   \n",
       "2              0.0     0.27266       0.00000       0.0  0.04029     0.0   \n",
       "3              0.0     0.00000       0.00000       0.0  0.00000     0.0   \n",
       "4              0.0     0.00000       0.00000       0.0  0.00000     0.0   \n",
       "\n",
       "     TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  \n",
       "0  0.11166           0.11149        0.00000      0.0  \n",
       "1  0.02458           0.00000        0.00518      0.0  \n",
       "2  0.00000           0.00000        0.63930      0.0  \n",
       "3  0.00000           0.00000        0.00000      0.0  \n",
       "4  0.18772           0.00000        0.53382      0.0  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = merged_df.copy()\n",
    "new_df.drop(['ifa', 'first_seen','last_seen'], axis=1, inplace=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns:  Index(['num_days', 'brq', 'brq_engagement', 'distinct_app', 'skewness_female',\n",
      "       'skewness_male', 'daypart_home', 'daypart_other', 'daypart_work',\n",
      "       'cellular',\n",
      "       ...\n",
      "       'PERSONALIZATION', 'PHOTOGRAPHY', 'PRODUCTIVITY', 'SHOPPING', 'SOCIAL',\n",
      "       'SPORTS', 'TOOLS', 'TRAVEL_AND_LOCAL', 'VIDEO_PLAYERS', 'WEATHER'],\n",
      "      dtype='object', length=105)\n",
      "\n",
      "\n",
      "Categorical Columns: Index(['platform', 'device_category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical = new_df.drop(['gender'], axis=1).select_dtypes('number').columns\n",
    "categorical = new_df.select_dtypes('object').columns\n",
    "print(f'Numerical Columns:  {new_df[numerical].columns}')\n",
    "print('\\n')\n",
    "print(f'Categorical Columns: {new_df[categorical].columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns do not seem very useful. But let's not drop them just yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_dummies = pd.get_dummies(new_df.platform)\n",
    "device_dummies = pd.get_dummies(new_df.device_category)\n",
    "new_df = pd.concat([new_df, platform_dummies], axis=1)\n",
    "new_df = pd.concat([new_df, device_dummies], axis=1)\n",
    "new_df.drop(['platform', 'device_category'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4.15385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.01852</td>\n",
       "      <td>0.35185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11166</td>\n",
       "      <td>0.11149</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>27.46923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.47797</td>\n",
       "      <td>0.11537</td>\n",
       "      <td>0.40666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02458</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_days   brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0      13.0    54         4.15385     1.0           8.0              0.0   \n",
       "1     130.0  3571        27.46923     1.0          29.0              0.0   \n",
       "\n",
       "   skewness_male  daypart_home  daypart_other  daypart_work  ...   SOCIAL  \\\n",
       "0       0.037037       0.62963        0.01852       0.35185  ...  0.11164   \n",
       "1       0.008681       0.47797        0.11537       0.40666  ...  0.00000   \n",
       "\n",
       "   SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  IOS  \\\n",
       "0     0.0  0.11166           0.11149        0.00000      0.0        1    0   \n",
       "1     0.0  0.02458           0.00000        0.00518      0.0        1    0   \n",
       "\n",
       "   SMART PHONE  TABLET  \n",
       "0            1       0  \n",
       "1            1       0  \n",
       "\n",
       "[2 rows x 110 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('easychecker.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataframe is ready for Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataframe - Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(filepath, 'test_data.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49737, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>platform</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>...</th>\n",
       "      <th>dev_cat_iab9_23</th>\n",
       "      <th>dev_cat_iab9_30</th>\n",
       "      <th>dev_cat_books</th>\n",
       "      <th>dev_cat_finance</th>\n",
       "      <th>dev_cat_games</th>\n",
       "      <th>dev_cat_lifestyle</th>\n",
       "      <th>dev_cat_navigation</th>\n",
       "      <th>dev_cat_productivity</th>\n",
       "      <th>dev_cat_reference</th>\n",
       "      <th>dev_cat_utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e932096849088aa76a953ce2f69728db6b6cdc056cd91...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-02-23 07:33:14</td>\n",
       "      <td>2020-02-23 09:25:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...</td>\n",
       "      <td>IOS</td>\n",
       "      <td>2020-02-04 11:15:37</td>\n",
       "      <td>2020-02-04 11:15:37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa platform  \\\n",
       "0  8e932096849088aa76a953ce2f69728db6b6cdc056cd91...  ANDROID   \n",
       "1  9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...      IOS   \n",
       "\n",
       "            first_seen            last_seen  num_days  brq  brq_engagement  \\\n",
       "0  2020-02-23 07:33:14  2020-02-23 09:25:14       1.0  370           370.0   \n",
       "1  2020-02-04 11:15:37  2020-02-04 11:15:37       1.0    1             1.0   \n",
       "\n",
       "   gender  distinct_app  skewness_female  ...  dev_cat_iab9_23  \\\n",
       "0    -1.0           1.0              0.0  ...              0.0   \n",
       "1    -1.0           NaN              0.0  ...              0.0   \n",
       "\n",
       "   dev_cat_iab9_30  dev_cat_books  dev_cat_finance dev_cat_games  \\\n",
       "0         0.000000            0.0              0.0           0.0   \n",
       "1         0.142857            0.0              0.0           0.0   \n",
       "\n",
       "   dev_cat_lifestyle  dev_cat_navigation  dev_cat_productivity  \\\n",
       "0                0.0                 0.0                   0.0   \n",
       "1                0.0                 0.0                   0.0   \n",
       "\n",
       "   dev_cat_reference  dev_cat_utilities  \n",
       "0                0.0                0.0  \n",
       "1                0.0                0.0  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49737, 111)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>platform</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>...</th>\n",
       "      <th>PERSONALIZATION</th>\n",
       "      <th>PHOTOGRAPHY</th>\n",
       "      <th>PRODUCTIVITY</th>\n",
       "      <th>SHOPPING</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e932096849088aa76a953ce2f69728db6b6cdc056cd91...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-02-23 07:33:14</td>\n",
       "      <td>2020-02-23 09:25:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...</td>\n",
       "      <td>IOS</td>\n",
       "      <td>2020-02-04 11:15:37</td>\n",
       "      <td>2020-02-04 11:15:37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2018-09-25 13:37:09</td>\n",
       "      <td>2020-03-05 15:58:05</td>\n",
       "      <td>172.0</td>\n",
       "      <td>6448</td>\n",
       "      <td>37.48837</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-03-13 15:42:34</td>\n",
       "      <td>2020-03-31 21:52:39</td>\n",
       "      <td>16.0</td>\n",
       "      <td>598</td>\n",
       "      <td>37.37500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...</td>\n",
       "      <td>ANDROID</td>\n",
       "      <td>2020-03-18 09:17:21</td>\n",
       "      <td>2020-03-31 07:51:58</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa platform  \\\n",
       "0  8e932096849088aa76a953ce2f69728db6b6cdc056cd91...  ANDROID   \n",
       "1  9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...      IOS   \n",
       "2  17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...  ANDROID   \n",
       "3  c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...  ANDROID   \n",
       "4  9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...  ANDROID   \n",
       "\n",
       "            first_seen            last_seen  num_days   brq  brq_engagement  \\\n",
       "0  2020-02-23 07:33:14  2020-02-23 09:25:14       1.0   370       370.00000   \n",
       "1  2020-02-04 11:15:37  2020-02-04 11:15:37       1.0     1         1.00000   \n",
       "2  2018-09-25 13:37:09  2020-03-05 15:58:05     172.0  6448        37.48837   \n",
       "3  2020-03-13 15:42:34  2020-03-31 21:52:39      16.0   598        37.37500   \n",
       "4  2020-03-18 09:17:21  2020-03-31 07:51:58       3.0     4         1.33333   \n",
       "\n",
       "   gender  distinct_app  skewness_female  ...  PERSONALIZATION  PHOTOGRAPHY  \\\n",
       "0    -1.0           1.0              0.0  ...              0.0      0.00000   \n",
       "1    -1.0           NaN              0.0  ...              NaN          NaN   \n",
       "2    -1.0          12.0              0.0  ...              0.0      0.00000   \n",
       "3    -1.0           2.0              0.0  ...              0.0      0.28409   \n",
       "4    -1.0           2.0              0.0  ...              0.0      0.00000   \n",
       "\n",
       "   PRODUCTIVITY  SHOPPING SOCIAL  SPORTS    TOOLS  TRAVEL_AND_LOCAL  \\\n",
       "0           0.0       0.0    1.0     0.0  0.00000               0.0   \n",
       "1           NaN       NaN    NaN     NaN      NaN               NaN   \n",
       "2           0.0       0.0    0.0     0.0  0.06166               0.0   \n",
       "3           0.0       0.0    0.0     0.0  0.00000               0.0   \n",
       "4           0.0       0.0    0.0     0.0  0.58448               0.0   \n",
       "\n",
       "   VIDEO_PLAYERS  WEATHER  \n",
       "0        0.00000      0.0  \n",
       "1            NaN      NaN  \n",
       "2        0.48771      0.0  \n",
       "3        0.71591      0.0  \n",
       "4        0.41552      0.0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged = test.merge(df2, left_on= 'ifa', right_on='ifa', how = 'left')\n",
    "print(test_merged.shape)\n",
    "test_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948649</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.0</td>\n",
       "      <td>6448</td>\n",
       "      <td>37.48837</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207196</td>\n",
       "      <td>0.56620</td>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.39813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>598</td>\n",
       "      <td>37.37500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.73746</td>\n",
       "      <td>0.08528</td>\n",
       "      <td>0.17726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_days   brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0       1.0   370       370.00000    -1.0           1.0              0.0   \n",
       "1       1.0     1         1.00000    -1.0           NaN              0.0   \n",
       "2     172.0  6448        37.48837    -1.0          12.0              0.0   \n",
       "3      16.0   598        37.37500    -1.0           2.0              0.0   \n",
       "4       3.0     4         1.33333    -1.0           2.0              0.0   \n",
       "\n",
       "   skewness_male  daypart_home  daypart_other  daypart_work  ...  SOCIAL  \\\n",
       "0       0.948649       1.00000        0.00000       0.00000  ...     1.0   \n",
       "1       0.000000       0.00000        0.00000       1.00000  ...     NaN   \n",
       "2       0.207196       0.56620        0.03567       0.39813  ...     0.0   \n",
       "3       0.765886       0.73746        0.08528       0.17726  ...     0.0   \n",
       "4       0.500000       0.25000        0.00000       0.75000  ...     0.0   \n",
       "\n",
       "   SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  IOS  \\\n",
       "0     0.0  0.00000               0.0        0.00000      0.0        1    0   \n",
       "1     NaN      NaN               NaN            NaN      NaN        0    1   \n",
       "2     0.0  0.06166               0.0        0.48771      0.0        1    0   \n",
       "3     0.0  0.00000               0.0        0.71591      0.0        1    0   \n",
       "4     0.0  0.58448               0.0        0.41552      0.0        1    0   \n",
       "\n",
       "   SMART PHONE  TABLET  \n",
       "0            1       0  \n",
       "1            0       0  \n",
       "2            1       0  \n",
       "3            1       0  \n",
       "4            1       0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform_dummies_test = pd.get_dummies(test_merged.platform)\n",
    "device_dummies_test = pd.get_dummies(test_merged.device_category)\n",
    "test_merged = pd.concat([test_merged, platform_dummies_test], axis=1)\n",
    "test_merged= pd.concat([test_merged, device_dummies_test], axis=1)\n",
    "test_ifa_vals = test_merged['ifa']\n",
    "# Dropping unnecessary columns\n",
    "\n",
    "test_merged.drop(['ifa', 'first_seen','last_seen','platform','device_category'], axis=1, inplace=True)\n",
    "\n",
    "test_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49737, 110)\n"
     ]
    }
   ],
   "source": [
    "print(test_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125341"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_merged.isnull().values.any())\n",
    "test_merged.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinct_app         261\n",
       "cellular              11\n",
       "wifi                  11\n",
       "dev_cat_iab1          63\n",
       "dev_cat_iab1_1        63\n",
       "                    ... \n",
       "SPORTS              1815\n",
       "TOOLS               1815\n",
       "TRAVEL_AND_LOCAL    1815\n",
       "VIDEO_PLAYERS       1815\n",
       "WEATHER             1815\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf = test_merged.isnull().sum()\n",
    "xdf[xdf > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fbc62cc8c10>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(test_merged['wifi'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_days', 'brq', 'brq_engagement', 'gender', 'distinct_app',\n",
       "       'skewness_female', 'skewness_male', 'daypart_home', 'daypart_other',\n",
       "       'daypart_work', 'cellular', 'wifi', 'dev_cat_iab1', 'dev_cat_iab1_1',\n",
       "       'dev_cat_iab1_6', 'dev_cat_iab12', 'dev_cat_iab13', 'dev_cat_iab14',\n",
       "       'dev_cat_iab15', 'dev_cat_iab15_10', 'dev_cat_iab17', 'dev_cat_iab19',\n",
       "       'dev_cat_iab19_29', 'dev_cat_iab20', 'dev_cat_iab3', 'dev_cat_iab5',\n",
       "       'dev_cat_iab7', 'dev_cat_iab9', 'dev_cat_iab9_23', 'dev_cat_iab9_30',\n",
       "       'dev_cat_books', 'dev_cat_finance', 'dev_cat_games',\n",
       "       'dev_cat_lifestyle', 'dev_cat_navigation', 'dev_cat_productivity',\n",
       "       'dev_cat_reference', 'dev_cat_utilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.index[:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute - median value\n",
    "for cols_xdf in xdf.index[:]:\n",
    "    test_merged[cols_xdf] = test_merged[cols_xdf].fillna(test_merged[cols_xdf].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(189846, 110)\n",
      "(49737, 110)\n"
     ]
    }
   ],
   "source": [
    "print(test_merged.isnull().values.any())\n",
    "print(new_df.shape)\n",
    "print(test_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePathOrBuffer[AnyStr] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[Hashable] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool_t | list[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool_t'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'IndexLabel | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CompressionOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mline_terminator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool_t'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'str | None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Write object to a comma-separated values (csv) file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_buf : str or file handle, default None\n",
      "    File path or object, if None is provided the result is returned as\n",
      "    a string.  If a non-binary file object is passed, it should be opened\n",
      "    with `newline=''`, disabling universal newlines. If a binary\n",
      "    file object is passed, `mode` might need to contain a `'b'`.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "       Support for binary file objects was introduced.\n",
      "\n",
      "sep : str, default ','\n",
      "    String of length 1. Field delimiter for the output file.\n",
      "na_rep : str, default ''\n",
      "    Missing data representation.\n",
      "float_format : str, default None\n",
      "    Format string for floating point numbers.\n",
      "columns : sequence, optional\n",
      "    Columns to write.\n",
      "header : bool or list of str, default True\n",
      "    Write out the column names. If a list of strings is given it is\n",
      "    assumed to be aliases for the column names.\n",
      "index : bool, default True\n",
      "    Write row names (index).\n",
      "index_label : str or sequence, or False, default None\n",
      "    Column label for index column(s) if desired. If None is given, and\n",
      "    `header` and `index` are True, then the index names are used. A\n",
      "    sequence should be given if the object uses MultiIndex. If\n",
      "    False do not print fields for index names. Use index_label=False\n",
      "    for easier importing in R.\n",
      "mode : str\n",
      "    Python write mode, default 'w'.\n",
      "encoding : str, optional\n",
      "    A string representing the encoding to use in the output file,\n",
      "    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "    is a non-binary file object.\n",
      "compression : str or dict, default 'infer'\n",
      "    If str, represents compression mode. If dict, value at 'method' is\n",
      "    the compression mode. Compression mode may be any of the following\n",
      "    possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If\n",
      "    compression mode is 'infer' and `path_or_buf` is path-like, then\n",
      "    detect compression mode from the following extensions: '.gz',\n",
      "    '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given\n",
      "    and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as\n",
      "    one of the above, other entries passed as\n",
      "    additional compression options.\n",
      "\n",
      "    .. versionchanged:: 1.0.0\n",
      "\n",
      "       May now be a dict with key 'method' as compression mode\n",
      "       and other entries as additional compression options if\n",
      "       compression mode is 'zip'.\n",
      "\n",
      "    .. versionchanged:: 1.1.0\n",
      "\n",
      "       Passing compression options as keys in dict is\n",
      "       supported for compression modes 'gzip' and 'bz2'\n",
      "       as well as 'zip'.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "        Compression is supported for binary file objects.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "        Previous versions forwarded dict entries for 'gzip' to\n",
      "        `gzip.open` instead of `gzip.GzipFile` which prevented\n",
      "        setting `mtime`.\n",
      "\n",
      "quoting : optional constant from csv module\n",
      "    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "    will treat them as non-numeric.\n",
      "quotechar : str, default '\\\"'\n",
      "    String of length 1. Character used to quote fields.\n",
      "line_terminator : str, optional\n",
      "    The newline character or character sequence to use in the output\n",
      "    file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "chunksize : int or None\n",
      "    Rows to write at a time.\n",
      "date_format : str, default None\n",
      "    Format string for datetime objects.\n",
      "doublequote : bool, default True\n",
      "    Control quoting of `quotechar` inside a field.\n",
      "escapechar : str, default None\n",
      "    String of length 1. Character used to escape `sep` and `quotechar`\n",
      "    when appropriate.\n",
      "decimal : str, default '.'\n",
      "    Character recognized as decimal separator. E.g. use ',' for\n",
      "    European data.\n",
      "errors : str, default 'strict'\n",
      "    Specifies how encoding and decoding errors are to be handled.\n",
      "    See the errors argument for :func:`open` for a full list\n",
      "    of options.\n",
      "\n",
      "    .. versionadded:: 1.1.0\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None or str\n",
      "    If path_or_buf is None, returns the resulting csv format as a\n",
      "    string. Otherwise returns None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_csv : Load a CSV file into a DataFrame.\n",
      "to_excel : Write DataFrame to an Excel file.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "...                    'mask': ['red', 'purple'],\n",
      "...                    'weapon': ['sai', 'bo staff']})\n",
      ">>> df.to_csv(index=False)\n",
      "'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "\n",
      "Create 'out.zip' containing 'out.csv'\n",
      "\n",
      ">>> compression_opts = dict(method='zip',\n",
      "...                         archive_name='out.csv')  # doctest: +SKIP\n",
      ">>> df.to_csv('out.zip', index=False,\n",
      "...           compression=compression_opts)  # doctest: +SKIP\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.to_csv('test_input_file.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948649</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.0</td>\n",
       "      <td>6448</td>\n",
       "      <td>37.48837</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207196</td>\n",
       "      <td>0.56620</td>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.39813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>598</td>\n",
       "      <td>37.37500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.73746</td>\n",
       "      <td>0.08528</td>\n",
       "      <td>0.17726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49732</th>\n",
       "      <td>18.0</td>\n",
       "      <td>212</td>\n",
       "      <td>11.77778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.28834</td>\n",
       "      <td>0.07975</td>\n",
       "      <td>0.63190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49733</th>\n",
       "      <td>11.0</td>\n",
       "      <td>64</td>\n",
       "      <td>5.81818</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49734</th>\n",
       "      <td>65.0</td>\n",
       "      <td>2328</td>\n",
       "      <td>35.81538</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28516</td>\n",
       "      <td>0.25223</td>\n",
       "      <td>0.46261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49735</th>\n",
       "      <td>118.0</td>\n",
       "      <td>2637</td>\n",
       "      <td>22.34746</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050057</td>\n",
       "      <td>0.78875</td>\n",
       "      <td>0.00902</td>\n",
       "      <td>0.20223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49736</th>\n",
       "      <td>9.0</td>\n",
       "      <td>659</td>\n",
       "      <td>73.22222</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49737 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_days   brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0           1.0   370       370.00000    -1.0           1.0              0.0   \n",
       "1           1.0     1         1.00000    -1.0           7.0              0.0   \n",
       "2         172.0  6448        37.48837    -1.0          12.0              0.0   \n",
       "3          16.0   598        37.37500    -1.0           2.0              0.0   \n",
       "4           3.0     4         1.33333    -1.0           2.0              0.0   \n",
       "...         ...   ...             ...     ...           ...              ...   \n",
       "49732      18.0   212        11.77778    -1.0           4.0              0.0   \n",
       "49733      11.0    64         5.81818    -1.0           2.0              0.0   \n",
       "49734      65.0  2328        35.81538    -1.0          18.0              0.0   \n",
       "49735     118.0  2637        22.34746    -1.0          39.0              0.0   \n",
       "49736       9.0   659        73.22222    -1.0           7.0              0.0   \n",
       "\n",
       "       skewness_male  daypart_home  daypart_other  daypart_work  ...  SOCIAL  \\\n",
       "0           0.948649       1.00000        0.00000       0.00000  ...     1.0   \n",
       "1           0.000000       0.00000        0.00000       1.00000  ...     0.0   \n",
       "2           0.207196       0.56620        0.03567       0.39813  ...     0.0   \n",
       "3           0.765886       0.73746        0.08528       0.17726  ...     0.0   \n",
       "4           0.500000       0.25000        0.00000       0.75000  ...     0.0   \n",
       "...              ...           ...            ...           ...  ...     ...   \n",
       "49732       0.415094       0.28834        0.07975       0.63190  ...     0.0   \n",
       "49733       0.093750       0.08333        0.08333       0.83333  ...     0.0   \n",
       "49734       0.000000       0.28516        0.25223       0.46261  ...     0.0   \n",
       "49735       0.050057       0.78875        0.00902       0.20223  ...     0.0   \n",
       "49736       0.000000       0.20000        0.00000       0.80000  ...     0.0   \n",
       "\n",
       "       SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  \\\n",
       "0         0.0  0.00000               0.0        0.00000      0.0        1   \n",
       "1         0.0  0.04011               0.0        0.02931      0.0        0   \n",
       "2         0.0  0.06166               0.0        0.48771      0.0        1   \n",
       "3         0.0  0.00000               0.0        0.71591      0.0        1   \n",
       "4         0.0  0.58448               0.0        0.41552      0.0        1   \n",
       "...       ...      ...               ...            ...      ...      ...   \n",
       "49732     0.0  0.00000               0.0        0.00000      0.0        1   \n",
       "49733     0.0  0.04011               0.0        0.02931      0.0        0   \n",
       "49734     0.0  0.00633               0.0        0.00000      0.0        1   \n",
       "49735     0.0  0.00000               0.0        0.00616      0.0        1   \n",
       "49736     0.0  0.00000               0.0        0.00000      0.0        1   \n",
       "\n",
       "       IOS  SMART PHONE  TABLET  \n",
       "0        0            1       0  \n",
       "1        1            0       0  \n",
       "2        0            1       0  \n",
       "3        0            1       0  \n",
       "4        0            1       0  \n",
       "...    ...          ...     ...  \n",
       "49732    0            1       0  \n",
       "49733    1            1       0  \n",
       "49734    0            1       0  \n",
       "49735    0            1       0  \n",
       "49736    0            1       0  \n",
       "\n",
       "[49737 rows x 110 columns]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df = pd.read_csv('test_input_file.csv')\n",
    "check_df\n",
    "#ifa values in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test requirements\n",
    "#### test_merged - dataframe - X = test_merged.drop('gender', axis=1)\n",
    "#### test_ifa_vals - final concat\n",
    "#### -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Input - Making Model ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4.15385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.01852</td>\n",
       "      <td>0.35185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11166</td>\n",
       "      <td>0.11149</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>27.46923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.47797</td>\n",
       "      <td>0.11537</td>\n",
       "      <td>0.40666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02458</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_days   brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0      13.0    54         4.15385     1.0           8.0              0.0   \n",
       "1     130.0  3571        27.46923     1.0          29.0              0.0   \n",
       "\n",
       "   skewness_male  daypart_home  daypart_other  daypart_work  ...   SOCIAL  \\\n",
       "0       0.037037       0.62963        0.01852       0.35185  ...  0.11164   \n",
       "1       0.008681       0.47797        0.11537       0.40666  ...  0.00000   \n",
       "\n",
       "   SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  IOS  \\\n",
       "0     0.0  0.11166           0.11149        0.00000      0.0        1    0   \n",
       "1     0.0  0.02458           0.00000        0.00518      0.0        1    0   \n",
       "\n",
       "   SMART PHONE  TABLET  \n",
       "0            1       0  \n",
       "1            1       0  \n",
       "\n",
       "[2 rows x 110 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = new_df['gender']\n",
    "X = new_df.drop('gender', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------\n",
    "## Ideally on imbalanced classes - several over/under sampling techniques are used to capture better model. Also, in such cases accuracy score is not the correct representative of the actual model performance since F1 scores are pretty low for the minority class.\n",
    "\n",
    "### Let us here, observe the model performances in cases with and without imbalance treatment.\n",
    "\n",
    "### Let's also perform PCA later on and see how that should ideally provide good results with less bulky models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "# Class imbalanced data - Run as is - No PCA, No balancing techniques used -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for Models\n",
    "from sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,classification_report,roc_auc_score,average_precision_score,f1_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision = []\n",
    "recall = []\n",
    "fscore = []\n",
    "rocaucscore = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : DummyClassifier(constant=1, strategy='constant') and  roc_auc_score score is : 0.5\n",
      "model : DummyClassifier(constant=1, strategy='constant') and  accuracy score is : 0.8768\n",
      "model : DummyClassifier(constant=1, strategy='constant') and  average_precision_score score is : 0.8768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accuracy\n",
       "DummyClassifier    0.8768"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Classifier - Baseline Classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "ohe= OneHotEncoder()\n",
    "ss = StandardScaler()\n",
    "\n",
    "model = DummyClassifier(strategy='constant', constant=1)\n",
    "pipe = make_pipeline(ss, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "model_names = ['DummyClassifier']\n",
    "dummy_result_df = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\n",
    "dummy_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.38568306, 0.31777787, 0.35300088, 0.31308484, 0.29583788]),\n",
       " 'score_time': array([0.05810881, 0.04738688, 0.05275798, 0.04264402, 0.04483819]),\n",
       " 'test_score': array([0.87523985, 0.87523985, 0.87527278, 0.87527278, 0.87523516])}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(pipe, X_train, y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752520845834748"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Model Outputs -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models on Original Data Scaled using StandardScaler - with 70-30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.21      0.30      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.72      0.59      0.62     56954\n",
      "weighted avg       0.85      0.88      0.86     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.36      0.42      7015\n",
      "         1.0       0.91      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.70      0.65      0.67     56954\n",
      "weighted avg       0.86      0.88      0.87     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.27      0.34      7015\n",
      "         1.0       0.90      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.87     56954\n",
      "   macro avg       0.68      0.61      0.63     56954\n",
      "weighted avg       0.85      0.87      0.85     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.39      0.45      7015\n",
      "         1.0       0.92      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.72      0.67      0.69     56954\n",
      "weighted avg       0.87      0.88      0.87     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.30      0.39      7015\n",
      "         1.0       0.91      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.63      0.66     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.27      0.36      7015\n",
      "         1.0       0.90      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.72      0.62      0.65     56954\n",
      "weighted avg       0.86      0.88      0.86     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.24      0.33      7015\n",
      "         1.0       0.90      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.73      0.61      0.63     56954\n",
      "weighted avg       0.86      0.88      0.86     56954\n",
      "\n",
      "[16:01:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.35      0.43      7015\n",
      "         1.0       0.91      0.96      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.65      0.68     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.30      0.40      7015\n",
      "         1.0       0.91      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.64      0.67     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc945ac850>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.33      0.42      7015\n",
      "         1.0       0.91      0.96      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.65      0.68     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.8751</td>\n",
       "      <td>0.9114</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>0.3601</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.4154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.8697</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.9542</td>\n",
       "      <td>0.2683</td>\n",
       "      <td>0.6112</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>0.3365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.4471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.8853</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.2971</td>\n",
       "      <td>0.6325</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>0.3894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8820</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>0.9677</td>\n",
       "      <td>0.2721</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.3623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8826</td>\n",
       "      <td>0.9002</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.9733</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>0.3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.8859</td>\n",
       "      <td>0.9115</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.9617</td>\n",
       "      <td>0.3464</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.9366</td>\n",
       "      <td>0.4278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.9071</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.3025</td>\n",
       "      <td>0.6355</td>\n",
       "      <td>0.9373</td>\n",
       "      <td>0.3962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.8863</td>\n",
       "      <td>0.9097</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>0.6464</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.4155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.8803         0.8974          0.1065      0.9742   \n",
       "LDA                        0.8751         0.9114          0.0998      0.9475   \n",
       "KNeighborsClassifier       0.8697         0.9016          0.1042      0.9542   \n",
       "ADABoostClassifier         0.8808         0.9153          0.0985      0.9495   \n",
       "GradientBoostClassifier    0.8853         0.9064          0.1017      0.9679   \n",
       "RandomForestClassifier     0.8820         0.9035          0.1031      0.9677   \n",
       "ExtraTreeClassifier        0.8826         0.9002          0.1049      0.9733   \n",
       "XGBoostClassifier          0.8859         0.9115          0.0996      0.9617   \n",
       "LightGBMClassifier         0.8864         0.9071          0.1013      0.9684   \n",
       "CatBoostClassifier         0.8863         0.9097          0.1003      0.9648   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.2117         0.5930        0.9345   \n",
       "LDA                           0.3601         0.6538        0.9301   \n",
       "KNeighborsClassifier          0.2683         0.6112        0.9277   \n",
       "ADABoostClassifier            0.3913         0.6704        0.9332   \n",
       "GradientBoostClassifier       0.2971         0.6325        0.9367   \n",
       "RandomForestClassifier        0.2721         0.6199        0.9350   \n",
       "ExtraTreeClassifier           0.2372         0.6052        0.9357   \n",
       "XGBoostClassifier             0.3464         0.6540        0.9366   \n",
       "LightGBMClassifier            0.3025         0.6355        0.9373   \n",
       "CatBoostClassifier            0.3280         0.6464        0.9371   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.3035  \n",
       "LDA                             0.4154  \n",
       "KNeighborsClassifier            0.3365  \n",
       "ADABoostClassifier              0.4471  \n",
       "GradientBoostClassifier         0.3894  \n",
       "RandomForestClassifier          0.3623  \n",
       "ExtraTreeClassifier             0.3324  \n",
       "XGBoostClassifier               0.4278  \n",
       "LightGBMClassifier              0.3962  \n",
       "CatBoostClassifier              0.4155  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    pipe = make_pipeline(StandardScaler(), model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "result_df1 = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "result_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use PCA now - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "ss = StandardScaler()\n",
    "X_ss = ss.fit_transform(X)\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_ss)\n",
    "X_transformed = pca.transform(X_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaYAAAGDCAYAAADQyccEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAACAGklEQVR4nO3dd5icVfXA8e+B0IJA6NJ2g0hXQIqiPzQBRAUVVGy49BJQURBRgVhADWBHFMWAoEIsYAFUlGJIUEEpUkR6STb0XgMp5P7+uLPs7Oz7ToHMZjb5fp5nnp1558ydM3PnnZ05e/e8kVJCkiRJkiRJkqShstiCTkCSJEmSJEmStGixMC1JkiRJkiRJGlIWpiVJkiRJkiRJQ8rCtCRJkiRJkiRpSFmYliRJkiRJkiQNKQvTkiRJkiRJkqQhZWFakiSpSkTsGxH/WNB5AETEsRFx9oLOo1UR8bOI+PoCuu+IiDMj4omIuGpB5KBXJiKmRMSBQ33bBaWVnCPifxExtr0ZNZVHT0RcvKDzkCRJw5uFaUmS1FYRsV1EXBERT0XE4xHxz4jYZgHndGxEzImIZyPiyUp+b34Z47yiIljl/vtO8yLi+arLPS933EXcdsBOwNoppTcu6GQ6SSf90WVBGK5/6KmWUto0pTSlA/KYlFJ6x4LOQ5IkDW8WpiVJUttExPLAn4AfACsBawHHAbNaHGfE/M+O36SUXgWsCvwD+H1ERBvup1RK6VV9J6AXeG/VtklDmUuniojFW7xJNzAtpfRcO/KRFnVtej+WJEmLIAvTkiSpnTYASCn9KqX0Ykrp+ZTSxSmlG/sCIuKgiLglIp6JiJsjYsvK9mkR8YWIuBF4LiJGRMS2ldXNT0bEDdX/0h4RK0TETyPigYi4LyK+3kxRM6U0B/g58Gpg5drrI+ItEXF1ZcX31RHxlsr2CcBbgR9WVjj/8JU8UXUsGRG/qDw//4uIratyWzMifhcRj0TEPRHx6bJBKu01TomIP1fG+ndErFe5bnREpOqCU/Vq8MpK239GxPcqz/3dledl34iYEREPR8Q+NXe5SkRcUrmvqRHRXTX2RpXrHo+I2yLiwzV5/jgiLoyI54DtCx7LmhFxQeX2d0bEQZXtBwCnA2+uzMlxJc9F2Wtu48rjfrLyXO9ak9ePIuIvlbH/GRGvjoiTIrcNuTUi3lAVPy0ijq6M/0Tk9iJL1+RwZ+UxXBARa1ZdlyLikIi4o3LbU6r/aBIR+1fyfyIiLqp5bgtvGxEbA6dWPTdPVuJ3qeT4TGW/ObLkOds3Iv4REd+ujHtPROxcdX3d/a9BzjtVnr+nIu9HUXPfL/u2VXHvAo4BPlJ5/DdUthe+lkrGWKry+Hsj4qGIODUilqlcd2FEfKcq9jcRcUbVc/fPiPhBJc9bI2LHkvtYLyImR8RjEfFoREyKiFFV10+LiLdXzh8bEedEyftDzbinRsS3a7adHxFHVM4fFRF3Rf8+8f6quOr9/3Hg2KhZfR8R34/8XvB0RFwbEW+tuq5unhGxTkT8PvL72GNR9V5ab+4lSdLwZ2FakiS10+3AixHx84jYOSJWrL4yIj4EHAvsDSwP7Ao8VhWyB/BuYBSwOvBn4Ovk1ddHAr+LiFUrsT8H5gKvBd4AvANo2GYjIpYC9gXuTSk9WnPdSpX7PJlctP4u8OeIWDmlNB74O3BoZYXzoU08Hy/HrsCvyc/BBcAPK7ktBvwRuIG8En1H4PCIeGedsfYgr1hfEbgTmNBCHm8CbiQ/D7+s5LQN+fnek1ygf1VVfA/wNWAV4HpgUiXvZYFLKmOsVsnpRxGxadVtP1bJbTnyavZavwLuBdYEPggcHxE7ppR+ChwCXFmZk6/U3rDsNRcRS5Cfz4sreX0KmBQRG1bd/MPAFyuPaRZwJfCfyuXfkl8f1XqAdwLrkf9I88VKDjsAJ1TGWwOYXnk+q72H/PxuXol7Z+W27yMXWD9AXu3/98rzUfe2KaVbap6bUZXYnwIHp5SWA14HTK59zqq8Cbit8ni/Cfw04qWCeen+Vy/niFgF+B39z+tdwP/13eEruW21lNJfgeOp/KdESmnzylWFr6WSx/8N8jxuUXmcawFfrly3P7BXROwQuQ3PNsBhNc/d3ZU8v0L+D42VCu4jyK+NNYGNgXXIr9cyhe8PBX5JLsoHQOW9+B30v+7uIv+hbQXye8TZEbFGQf6rUfy+cTX5eVmpcl/nRtUfYsryjPzHiz+R94HR5Of015Xr3kfj17okSRrOUkqePHny5MmTJ09tO5GLKz8jF3/mkosSq1euuwg4rOR204D9qy5/ATirJuYiYB9y0XoWsEzVdXsAl5WMfSwwG3gSeJhcjNuqct2+wD8q5/cCrqq57ZXAvpXzU4AD59PzNA14e0Gel1Zd3gR4vnL+TUBvTfzRwJkl4/8MOL3q8i7ArZXzo4EEjKi6/qXHVnlO7qi67vWV+NWrtj0GbFF1X7+uuu5VwIvkIttHgL/X5PYT4CtVt/1FnedpncpYy1VtOwH4We38ldy+8DVHLso9CCxWte1XwLFVeZ1Wdd2ngFtqnpMna+bzkJrn+67K+Z8C36x5fuYAoyuXE7Bd1fXnAEdVzv8FOKDqusWAmUB3E7cd9NyQW8gcDCzf4PW5L3Bn1eWRlft6NQ32v3o5k/9A8K+q64L8XnHgK71twWM4Fji72ddSzW0DeA5Yr2rbm4F7qi5/AJgBPFozB/sC9wNRte0qYK/afa3gft8HXFfzunp71eMpfH8oyb8XeFvl8kHA5DrzfT2wW1X+te81g15LNdc/AWzeKM/Kc/gIVe89VXF1X+uePHny5MmTp+F/csW0JElqq5TSLSmlfVNKa5NXZK4JnFS5eh3ySr0yM6rOdwMfitxm4cnIrQi2I6847QaWAB6ouu4n5NV9Zc5JKY1KKa2WUtohpXRtQcya5JV81aaTV/U1VPn3+b6DGR7TzG0KPFh1fiawdOSWG93AmjXPxzHkImGzY72qLLDAQ1XnnwdIKdVuqx7vpblLKT0LPE5+PruBN9Xk3UMucA66bYE1gcdTSs9UbWt6Tih/za0JzEgpzaszbu3jrff4YeDjmF65j777eul1VXl+Hqu5r7K56ga+X/XcPU4uOjZz2yK7k4vm0yO3XKl3ENCXxk0pzaycfRWN9796Oa/JwNdKYvB+/3Jv20grr6VVycX4a6ty+Wtle58/AYsDt6WUalf631fJr/p+1qyJISJWi4hfR26H8jRwNnmVdZmy94cBKvf9a/IfDCD/V8JLfewjYu+IuL7qsb2u5n7rPq8R8dlKy42nKrdfoeb2ZXmuA0xPKc0tGLaZ17okSRrGPHCFJEkaMimlWyPiZ+QVmpCLHevVu0nV+RnkFdODesBW/uV8FrBKSYHj5bqfXByp1kUuSNXmN0hK6RBy+4R2mEFerbn+fBir70CBI4GnK+dfXRLbrHX6zlRafKxEfj5nAFNTSjvVuW295/V+YKWIWK6qoNgF3NdkXmWvufuBdSJisaridBe5Hc3LtU7V+a7KffTdV3Wf5GXJLVKaeQwzgAnp5R0cc9DzmlK6Gtit0srkUPIK63Vq45rIqd7+V5pzRKzPwNdK1Nz/K7ltrdrH38pr6VHyHx82TSmVzdME4BZg3YjYI6VU3XZirYiIquJ0F/m/R2qdUMlzs5TSY5V2FvOrf/2vgIsj4kTyf1y8H6DSt/k0cjugK1NKL0bE9Qzs1126T1b6SX+hcvv/pZTmRcQTNbcvMwPoiogRBa+dV/JalyRJw4ArpiVJUttEPsjdZyNi7crldcgr9v5VCTkdODIitorstXUObnU28N6IeGdELB4RS0fE2IhYO6X0ALk38HciYvmIWCzyQcTGvMKHcCGwQUR8LPLBFz9C/jf0P1Wufwh4zSu8j5frKuDpyAeIXKbynLwuIrZpdaCU0iPkYtyelXH2p/4fDJqxS0RsFxFLkntN/zulNIP83G0QEXtFxBKV0zaRD87XTK4zgCuAEyqvgc2AA6ha/dlA2Wvu3+QC/ecrOY0F3svg3s+t+GRErB25l/AxwG8q238J7BcRW0TucX48+fmZ1sSYpwJHR6Und+SDDn6oyXweAtauzAkRsWRE9ETECikfBPRpcmuLljSx/9XL+c/AphHxgcoK2k8z8I8ir+S2RY9/dOT+7C29lip/rDgN+F5ErFbJZa2o9HSPiLcB+5Hbi+wN/CAiqlf2rgZ8uvLa+hC5xdGFBTkuBzwLPFm5/efqPJ6WpJSuI7fNOB24KKX0ZOWqZcmF50cqj2U/8orpZi1HbtP0CDAiIr5M7t/ejKuAB4ATI2LZyjz09Ql/Ja91SZI0DFiYliRJ7fQMeWXevyPiOXJB+ibgswAppXPJqwx/WYk9j7yydpBKEWk3coHvEfJqus/R/3lmb2BJ4GZyf9Pfktt8vGwppcfIB5L7LLnVwueB96T+gyR+H/hgRDwRESe/kvt6Gbm9SC6cbgHcQ17ReTr5X+hfjoPIz+djwKbkgt0r8UvyQd4eB7Yit+ugsjL1HcBHyStWHyQfVG6pFsbeg9wX+37gD+T+1Jc0c8Oy11xKaTb5AG07k5/LHwF7p5RubSGvWr8kF2zvrpy+Xsnhb8CXyAfue4D8R4CPNpn/H8jP168jt3q4qZJzMyYD/wMejIi+1/BewLTKWIeQD2T5cpTuf/VyruxLHwJOJL/21gf+2czjbXTbAudWfj4WEf+pnG/ltfQF8kFD/1XJ5VJgw4hYHvgF+UCo91XaePwUOLOyihvyHz7WJ7+2JgAfrLy/1DoO2BJ4ilx4/32dx/Ny/Ap4O/m1CUBK6WbgO+T++Q+R+6XXex5rXUTuB307uUXJCzTZUqXqfey15B7Y95L70L/S17okSRoGYmCrM0mSJEmvVERMIx/Q7tIFnYsWrIjYl/xa2G5B5yJJktRJXDEtSZIkSZIkSRpSFqYlSZIkSZIkSUPKVh6SJEmSJEmSpCHlimlJkiRJkiRJ0pCyMC1JkiRJkiRJGlIjFnQCrVpllVXS6NGjF3Qa881zzz3Hsssu25b4do7d7vhOyqXV+E7KpdV4c5k/8Z2US6vxnZRLq/GdlEur8Z2US6vxnZRLq/GdlEur8Z2US6vx5jJ/4jspl1bjOymXVuM7KZdW4zspl1bjOymXVuM7KZdW481l/sR3Ui6txndSLq3Gd1IurcZ3Ui6txndSLq3Gd1IuC5trr7320ZTSqoVXppSG1WmrrbZKC5PLLrusbfHtHLvd8Z2US6vxnZRLq/HmMn/iOymXVuM7KZdW4zspl1bjOymXVuM7KZdW4zspl1bjOymXVuPNZf7Ed1IurcZ3Ui6txndSLq3Gd1IurcZ3Ui6txndSLq3Gm8v8ie+kXFqN76RcWo3vpFxaje+kXFqN76RcWo3vpFwWNsA1qaTOaysPSZIkSZIkSdKQsjAtSZIkSZIkSRpSFqYlSZIkSZIkSUPKwrQkSZIkSZIkaUhZmJYkSZIkSZIkDSkL05IkSZIkSZKkIWVhWpIkSZIkSZI0pCxMS5IkSZIkSZKGlIVpSZIkSZIkSdKQsjAtSZIkSZIkSRpSFqYlSZIkSZIkSUPKwrQkSZIkSZIkaUhZmJYkSZIkSZIkDSkL05IkSZIkSZKkIWVhWpIkSZIkSZI0pCxMS5IkSZIkSZKGlIVpSZIkSZIkSdKQsjAtSZIkSZIkSRpSFqYlSZIkSZIkSUPKwrQkSZIkSZIkaUhZmJYkSZIkSZIkDSkL05IkSZIkSZKkIWVhWpIkSZIkSZI0pCxMS5IkSZIkSZKGVNsK0xFxRkQ8HBE3lVwfEXFyRNwZETdGxJbtykWSJEmSJEkaSpMmwejRsMMOYxg9Ol+eH7GdFt9JuQz33Bc17Vwx/TPgXXWu3xlYv3IaB/y4jblIkiRJkiRpIdZJRcZJk2DcOJg+HVIKpk/Pl4tu00psp8V3Ui7DPfdF0Yh2DZxSujwiRtcJ2Q34RUopAf+KiFERsUZK6YF25SRJkiRJkqQFZ9IkGD8eenvH0NUFEyZAT88rj+8rAs6cCdBfBIT2xs+bBzvuCLNnw5w5/T+POqovtt/MmfCZz8Byy8HrXw/rrgtPPAGHH14cO3487LAD/OlPkFK+r5Ty9qL4T30KHnkkX959d1hnHbjttry9bPzXvx4uumjgdccfXxx/6KFw//358iGH5Mfx2c8Wxx59dH4ep0yBq67qv+6EE8rHvu++fHnECDjiiHz+M58pz72nB37/e7jzztbGX2012Hff8ufx8MP7XwOnnw6PP9782AAbbQS77lo+fl/uamNhuglrATOqLt9b2TaoMB0R48irqunq6hqS5CRJkiRJkhY17Soc98W+0mLwQQfBs8/CBz8IK6+c4667rrxAOn48PPggPP00zJoFL7yQf557bnH8/vvDt74Fe+wBX/gCPPdcLiA/+mguCtfGH3MM3Htv+fNT65FHYLfd4Ec/go9/HKZNy2MX6e2FO+7of44aeeKJXMgF2GyzXJi+/vq8vWz8q66Cz3++ufGffLI/do89cmH6oYeKY/uekwsvzM9nM2N/4Qv5/DLL9Bem+wrtRbkD/OIXcP75rY2/+ea5MN03Rq3q+fjOd+DWW5sfG+DDH86F6bLxy7YvihZkYToKtqWCbaSUJgITAbbeeuvCGEmSJEmSJA02lKuODzooF0Lf857c9gJy8XPGDDjssOJi8Cc/CX/8Yz4/c2YuBm+4YV5tWxv//PN5te7vf9+/0vd97ysvkPb25hXAjz8OSy0FSy+dT489Vhw/ezZ0d8Mqq+TLSy2Vi+A/LmlAe999cOqpsOSS+bTEEvnnJz/Zv7q42qtfnVdA96273GgjWGMNeKCgf0BXF7zpTfm5W2wxiMg/t966uBi+9trw3//m88sum39+4AO5QD1jxuD4ri7YZ59cZK628cbF8eusA7fcks8vs0z/GEWF1nXWyT+//nU49tj+7RttVD52UQG4bPy+5+83v4EXX2xt/Ij+MaZPL88d4D//6f+DRLO5L754/fFdc9uvnT2mG7kXqJpq1gYKdllJkiRJkiT1md+9jmfOzAW3z32uvP3E5z4HBx8MH/tYLjrvsktxq4Lnn8+tI7bbrn/bccfl4m5ZMfipp/LK3nvvzSual1sOVlqp/srSQw/tP/+LX+T2DEW6unKBeN68PPaTT+YV1N3dxfHd3XkF7gEH5MsjRuTVzWXxXV35edlvv1y8//CHc6H8m9+EkSMHxo4cCd/+Nmy1Fay6at62zDJ5RXFR7IQJuTC+9tqw5pq5gL366nDiicXxJ54Io0bl0xJL5O1LLJFbUJSNv8QSuYhdfSqLP+GE/pjFKhXF448vjj3++Hx+ySXz5b5TvbGr4/qUjT9hQj6/1FKtj99XVJ8woTy2zzLLtJ77UkvVH78vdy3YwvQFwN6RbQs8ZX9pSZIkSZI03A31QfUOOigXxy6/HC64IBdqTz45t68o63O7zz65IAx5RWtXV/GqXcjtFE45JRdsr7kmx82eXb9w/L3vDTx/ww25uFqkuzuvOP3Pf+Af/4CLL4bvfrd8ZWl3N7z3vf2Xx4zJ8fWKu1Hzf/utFg1bje/pgYkTc64Rie7ufLlo5XkrsZ0W30m5DPfcF0VtK0xHxK+AK4ENI+LeiDggIg6JiEMqIRcCdwN3AqcBn2hXLpIkSZIkSa9Es8XjZlYo18YfdNDA+P33h7337m8b8M9/wvvfD9tvn1fmFq1SPuaYXKDdbbdcdD7ssLxSuKx4/OKL/S0KPvABOO20/vYVtbq68n0++CDcfjtcey1cemn9wvGHPtR/eYMNcs/jslXE86MY3IlFxp6e3EN68uSpTJs2/2I7Lb6TchnuuS9q2laYTintkVJaI6W0REpp7ZTST1NKp6aUTq1cn1JKn0wprZdSen1K6Zp25SJJkiRJklRtfrbDmDsX7rknr/g94ojiFcrjxsE73gHbbAPrr5+LwOefn1c0P//8wPjZs+Gss+Cmm/pvf9dduZg8Z05xjhG5WHzNNXDnnfkAbhtsUL94PGpUPr/ttnDggXDSSfXbMtRq5yrilxtvkVEaPhZkKw9JkiRJkqT5Yn4Umn/yE7jjDvj3v+HCC/sLw0cfXVxs/vSn8/l77oHXvCb3Dn744eL7nDkTnnkm9xbeZhv4yEdy7+CyFc0RuZANsNNOcOONuVVHvV7HO+6Yc1hvPVh55dwHuJNWHffdxmKwJIARCzoBSZIkSZKkWpMm5dXEvb1j6OrKhdSyomRfoTkXj/v7Lj/xRP9B8k47DR56KB+A77TTigvNhxwycNsRR8B3vpMPylfkiSfyz7XWgjPOyAfsO/jgfD+1urvhyisHb+/qygXyou2LLz54+4QJ1Y81a7RKGfqey0RXV9R9Lnt68mnKlKmMHTu2OOgVxEtSHwvTkiRJkiRpSDRbbC4rNN92G2y5ZW5T8eijuR3GgQcWH+Dv+efh85/vL0yPH58P4veqV8Fzz5XneNZZucC80kr9q5PrFY8hF4b32y+ff/bZ1grH7S40993G4rGkTmNhWpIkSZIkvSyvdFXzgQfm63p64De/yQfVe/BBOPdceOGFgbd//nn42tcGbnvrW/MYZe0wqse46SZYYQVYaqnc8qOo0NzdDXvuOXh7K8Xjl7NCuZX4vttYaJY03NljWpIkSZIkAa+8T/NBB8H3vgf//Cf89re5Z3OfQw8dvKr5hRdyQRbgnHPg5JNzH+XaonSfiFy8nj49r3q+/PK8vewAf9XbV1stF6VhaA7aZx9lSarPFdOSJEmSJC3EXkn7jHHj8nU9PXkl8803w/33wwMP5NXLRe0zjjhi4LZ9980F4SefLM6vb7XzL38JSy6Zi89lK5q7unIrj1q2w5Ck4ccV05IkSZIkLaSKVjWPG9e/EnrmTJgzJ5///OeLDwj4+c/n87/8Jey4I+y1V972zDPl9/vXv8INN+SDAC65ZN7W16+5Vt+q5qWWykVpaP+K5r7buEpZkhYcC9OSJEmSJA0jzbTbSCkfHPBznysuNu+7L4waBcsuC//4R97+wAPF99e3fffd4bLL4NZb4emnywvN3d3wznfCZpvl9hkvp9hsoVmSFn6RUlrQObRk6+WWS9dstdWCTmO+efLJJxk1alRb4ts5drvjOymXVuM7KZdW481l/sR3Ui6txndSLq3Gd1IurcZ3Ui6txndSLq3Gd1IurcZ3Ui6txpvL/InvpFxaje+kXFqN76RcWo3vpFxaje+kXFqNbzb2oYfgnnvghVmw9FKw7rqw+urlsbfdDvPm9W9bLGDDDWH55fN1s16AWbNgXoOv+2utCUsulYvHyywNV/4r367W0kvBtts2mctisOEG9fNv9rH26aQ5bTW+k3JpNb6Tcmk1vpNyaTW+k3JpNb6Tcmk1vpNyWdjE1KnXppS2LrrOHtOSJEmSJM1H/cXXUS0Xml+YVSkuz4JXvSpfHjkSRq2QW27ceivU1pvnpXx/b9gyj7PccrDKKrk1xvTpMGfu4PtdeilYf/2B216zbnGhed11i3Pve0ytFJpXXz2fFuUijSSpIqU0rE5bbbVVWphcdtllbYtv59jtju+kXFqN76RcWo03l/kT30m5tBrfSbm0Gt9JubQa30m5tBrfSbm0Gt9JubQa30m5tBpvLvMnvpNyaTW+k3JpNb6Tcmk1vpNyaTW+k3JpJv7ss1MaOTKl3Ewjn0aOzNtTSmnevJQeeyyla69N6be/TWnUqIGxRaeDD863nTu3PCbi5eVTFN/dnVLEvNTdXR7X6vPySuIX9Jy+kvhOyqXV+E7KpdX4Tsql1fhOyqXV+E7KpdX4TsplYQNck0rqvPaYliRJkiSpgWb6OgOMH1/c03n8+Hz+da+DlVeGrbaCD34QnnyyeJwI+Pvfc8/kH/wgb1t88cYHEKzVaq9m+zRLkoaKrTwkSZIkSapj0iQYN66v4BxMnw4HHQS33QbrrQfPPQef+ESOnT69eIze3vzzoINyq4zRo3Pbi113hXvvHRzf1QXbbTd4+4QJ1blkZQcQ7NPTk09Tpkxl7NixjR+wJElDwMK0JEmSJGmRM2lSXsXc2zuGrq5c2K1eHTxzJtx9dz596lODV0E//zx87Wv5/Eor9RemV1oJHn988P31rWg+/PCB2088sbVCc1+OOfdEV1cMyl2SpOHAVh6SJEmSpGGv2VYbfbHjxuXVzSnlFdDjxuXtJ58Ma6wByy4Lr3897LYbPPFE8TgRcMcd8MAD/dtOPjkXlqs1KjS30mqj7za225AkDXeumJYkSZIkDWtFrTbGjYMXX4S3vAXuugve+EZYcUX44x9hv/1gzpyBY/T1gf7+92GXXeA1r8mn9daD3Xcvb7fx2tcO3PZyVjTbakOStChyxbQkSZIkqeM0uwL62Wfhc58rPuDgPvvA+uvDu94FV12Vty+//OCidJ/e3rxC+qc/zYXlPfbIBe0TT2x9FbQrmiVJqs8V05IkSZKkjlJ2sEGA7beHo47Kq6DvvBMefrj+WGeckVc9b7FFvjxmTG6bUXSQwr4+0LXs6yxJ0vznimlJkiRJ0pCotwp67tz884UX4JOfLD7Y4PjxsOSScNllsMQS8N73wvHHwyqrFN9fd3du2/G2t+WV0n0mTGhtBTS4ClqSpPnNwrQkSZIk6WV5pQcc3Gcf2GADWG01OPjgHLfkkvDUU8Vj9PbmIvSMGTBlCpx+Ohx9NJx0UvsPOChJkuYvW3lIkiRJklpWdsBByCuUb74ZbrsNbr89/7zyysGroF98MReb994bdtghb1tssdxSo7d38H3Oz1YbHnBQkqQFy8K0JEmSJAnIxeZc3B1DVxd1i7tHH118wMHx4+E1r8ntNiC30NhwQ3juueJxZs/Oq5WrHX98ddE7a6bVhoVmSZKGDwvTkiRJkqTSFdCzZsFmm8Gtt+bCbwR85jO5nUaR3l44+2yYNy+36Vh99Xyb0aObP+CgBxuUJGnhZ49pSZIkSRLHHFO8AvqAA2CbbWCvveCBB/L27beHFVYoHqerC7bbLrfzePWrc1EaWj/goAcblCRp4WZhWpIkSZIWUmUHJ3z+ebjiCvjRj+Cgg3Lhuainc5/f/z73iV599Xx5113hlFM84KAkSXr5bOUhSZIkSQuhotYcBx6Yr1tjDdhxx3x+pZXgDW/IvaCffnrwON3d8P73D97uAQclSdIr4YppSZIkSRpGylZBA8ydm38+/nheCV3bmuOFF3IheZtt4Pzz8yrpRx+FSy/Nq6dbWQENttuQJEkvn4XpTnfZZblB2zLLwEorsdHxx8NDDw2MmTYtN26rOY3dfnt48sn+uL4GcSutBOutB7/5zeD7++Y3YfPN+z/RDqVjj4XJk+f/uM89lxvirbZafm4OPzxvv/JKeNObYNll8/brry++/XnnsfY55wzePmVKvt2ll87/nFt17LE5l/k1b33jNTDq+utz3JQp8+d+JUmSVFffKujp0yGlvAp6v/1yobm7Gz71qRy3wgq5XUeR3l5YbrncjmOddfo/9tlqQ5IkDSUL053s73+Hd7wDRo2C3/0Ovv99Rt14Y/6fu1mzBscffXQutlZO/znllPyJs8+JJ8Ill8DPfpY/ve65J9xxx0tXL/XII/D1r8OPfwwjFkCXl+OOa09h+pRT4Fe/gm9/Oz83n/lM3n7AAbmQ+8c/5u0bbFB8+/POY51zz53/eUmSJEnUXwHd5/HH80f5ww8fvAp6zpy8xuItb4G3vjVvW3zxXGAu0tVVnosroCVJ0lCxx3QnO+64/GnyvPNeKhT/77nn2OrjH4ef/hQ+8YmB8a95DWy77UsXn37hhfyJtM9f/gKHHpqXRuy6a/7Ee+mlsP76ALz2Bz+AD384f6JdmNxyC6y5Juy9d/+2u+7KR28ZPx522GHoc3rxRUhpwfwBQJIkSR2jqA/0uHH5uojcbuOaa+Duu+uP8+KLeS1GtQkTqsfOGrXmkCRJGiqumO5k//oX7LTTgOLlMxttBCuvDH/4Q+vjzZ6dW4L0GTkyN5kD+OtfGXXDDfCNb7Q+7mmnwZZb5rFXXBHGjMmH+Ib+dhe1rR5+9rO8fdq0fLnv/wcnTOhvQ3LssY3v++yzYfPNeds73gGrrJJbdjzwQP/1Efm+Zszob3Pys58xdscdYd48+NrX8rbRo4vH33df+PnPWerRR/tvXxs7c2Yu+K+yCqy6KhtPmDCwhUpfHuPH51Xr664LSy4J//0vACtcf31eBb/ccrmtyDvfCTfdNPD2F12U/2Cwwgq8deedYcMN4atfHZzvPffAu98Nr3pV/qPGV7+aH2e1227LR68ZNSrP2bbbwl//Wu9Zzh55BD72sXxUnFGjYO+9GfHss41vJ0mSpFJHHz14BfTMmfmj4xVXwFVX5Y/aJ56Y15SsvXbxOEWroG3NIUmSOpmF6U62+OK5gFlrqaUGFy4hf6odMSI3lNt1V5atXVbxpjfBz3+eC7cXXZT/32/bbXNbkE99irvHjctF71YceWRehrHllnDOOblQ/La35cZ1rbjyyvxz333725D0HTK8zMSJuRC98cbc9NWv5k/rF12UC+N9BdMrr8yF3le/ur/Nyfbb85+TT87XH3BA3lZW6P/Sl2CXXZg9alT/7WtjDzssF55/+Uv48pdZ5fLL87ZaP/sZ/PnPuaXIn/+cV3H/+c9s8dnP5kLy2WfnMZ55Jv8P5owZ+XZ3351XuK+7LvzmN/x3wgQ44ojcO7vW+9+fV4Cfdx68733wla/w6osu6r/+/vtzz/IbboAf/jDP2ahRuZj9l7/Uf74/8AH405/g+ONzf/IRI3ht3/MoSZKkl5S15pgzB667rv9j3I9/3P+Rr1ZvL5x0Ul53cO658IUv5LUMJ57Y2gEKbc0hSZI6lX0EOtmGG+ZV01WWevDBXFheYomqjUvBwQfnftSrrgq33grHH88bLr0U3vxm2HjjHPeVr8DOO+eCKMDnPpevP+44WHVVHthlFzZsIb1l7rsPvve93LP5u9/tv+Ld7279sfa1IFlrLdh229yGpGw5COT/VfzSl2DsWPj1r3l8ypR8fqONclH3jDPg05/O466ySn6OqtqcPLPJJvnM2msP2D7IeuvBqquSRowoj3vb2+AHP8jn3/EOHpw8mbV+85v+VeF9UoKLLx64av2ww3hy881Z8fzz+7dtv31uy/Kd7+RvI//5T17t/uMfw/LL8+TSS+fHWuSzn839wwHe/naYPJnVqvt2f/e78MQTucD+2tfmbbvsAptskpflVM9jtUsugX/8I/9/6Ec/mre9850896Y3sfQjjxTfRpIkaRFU1Jpj333zR/H77sv/sHjJJfmj2tZb5zUlTz01eJyuruKub32F5fHjobc30dUVTJhgwVmSJA0/rpjuZIcdlv9374tfhIcfhltvZeMTToDFFsunPmusAaeemle0vvWtcNBBcPnluShavXRirbXyStk774RHH4VvfjOvxv32t+HHP2axWbPgkENg9dXz6ty+YmuJFa+9NreJ6GuC1w4p5QMU9p1efDFvv+22/JzUfgLfbrv8v4pTp7Yvp1o1hfhn1103r0J/6KGBce9618Ci9B13wF138dDb3z7wMY4cmf9gcPnlOW6LLfIfIj76Ufjtb1niiSeazoXXvY6lq/O4/PJcYO8rSkNemb/HHnD99SxetAobciF78cVh990HbH54++3Lc5EkSVpINHNwwoceggsuyIeBqW3NMXcu3Htvvu5Xv8of7wC22SYfp7uVFdDgKmhJkrRwsDDdyXp6clH6O9/JxeJNNmHWKqvkFa5rrFH/tuusw1Ovfz1cffXA7RF5FXBfy45PfSq3zNh8c7rPPjsfWeWmm3K7imOOgb/9rfQuRjz9dD5Tb2XzK/Xzn+eibN9pvfXy9scfzz+LnodXv7r/+qGw0koDLqa+9it9/bv71Ob68MMAbPStbw18jEsskVtmPPZYjnvta3OLknnzYK+9eMvuu+e2LEXF95pcWGopFps9u//y44+XP2cpMeKZZ4of4wMP5P7h1Sv1gTm19ydJkrSQ6VsBPX06pNR/cMJJk+D55/Oxw7u788ep3XaDvo/ItWbPzh/rP/rR/A99fewDLUmSFlUWpjvd176WVzffeCM88AC3fOlLeaXtdts1vm1KA1tJ1PrDH3Kf6cpB9Fa66irYZ5/cDmSLLXJrkDoHxZuz/PL5zH33ld/H0kvnn9XFUegvujby3vfm4nrf6Y9/zNv7CqIPPjj4Ng8+2Hqv7KFQOxeVHO8+6KCBj7H2sUJu7/HXv8KTT3LDt7+dC8Tvfnd+bbRipZXKn7MI5vbNaa011sgtQObMGbB5iaH8A4AkSdICcMwx5QcnXHrp3CN6221z0fnvf4d11ikep+jghH1cAS1JkhZFFqaHg2WXhde/HlZfPRePb701t9yop7eXFW66Ka+sLTJzJhx+eO4Rvdxy/durWzk8+2wubpd4YqutckuRiRPL8+juzj9rD9Z44YWDY5dcMi87qbbyyrn5Xt/p9a/P2zfcMK8i//WvB8ZfcUVezjJmTHlOrapddTy/bLghjB7NyGnTBj7GvtNmmxXm8uSWW8LnP5/n6p57WrvPMWNy3/Jp0/q3vfhiPpjhG97Ai7X/R9rnzW/Ocb/73YDNq112WWv3L0mS1CHK2nM8/HD/cbmh/Jjevb153cGVV+aPUkcckdeOnHBC6605JEmSFkUe/LCTXXcd/OUvsOWW+fI//sHrvvnNXJR8y1v64z772dzm4c1vzqudb7stfyKOyEs8inzta7kw+uEPv7Tpia22Yrkf/jAfQPD++3Mbj89+tjS9F9Zaq//Ah888A7vumvsQX3VVHuMjH8krbceMgRNOYPUDD8ztLc4+G+66a/CAm2wCf/4zvOtdLHf33bDBBv0Haqy1+OJ5pffBB8Oee7LSZpvlMcePh/XX7z8A4PywySYs8fTT+eCDW2+dl8b0FchfiQg45RRW23XX/Fx9+MP5/zofeigX2Lu68jecU0/NvaF32QXWWYdVpk7Nq6nXXBNe97rW7vMzn8kHZdxpp3zQy+WXhx/9CG6/PT/3ZXbaKX/TOvjgvEp7/fXhN79h2VYL45IkSR2g6ACF++yT1208+mj+iPTEE3kNxiqrFP+TWtkKaA9OKEmS1BxXTHeyJZfMK4s/+lF4//vhwgu5/TOfgW98Y2DcppvCP/6Ri4Y77QTHHgv/939c+6Mf5eJzrVtvzUdZOeWUAZun7713vv3+++clHSeemNt51PPtb+fC5r/+lQ+M19MDl1028JP62WfDttuy/g9+kA9J3tWVe2fX+uEP8+rw976XrQ45pP5KbMjfJs46C/77X173xS/mgv1OO+Xey696Vf3btuLAA3lohx1ykf+Nb8ztReaXXXbh+u9/P69+PvBAeOc78+N48MH8hwaAzTfP1x99NLzjHax/8sn54JSTJw88mGIz1lwzv1Y23RQ+/nH44Adz3+nKHwTq+v3vc3H86KNzIX3uXO749Kdf3uOWJEmazxodoDCl/I91v/lN/hhU257jxRfzR65vfSsf7qPvHwdPOsmDE0qSJLWDK6Y7WV/BucqDU6awUW3c/vvnU43np0wpHnejjQqPyvLiMsvAmWe2nuchh9RvLbL22vDHP/KPKVMYO3Zs//YDDxwY93//B9deC8CU2tgye+4Je+7J5fXizz570Ka0+OJ125QMsOyy3PKlL7F67fhjxxaO8eC73sVGJ55Yc4fl9/X0ppvCJz9Zfv9vfjOcf/5LF68seqzHHptPtX72M/41ZQoDojfcEM47r/z+ysZbddV8GPkqj02Z0vzzKEmS1CZFK6DHjcvXbbghfP3r8O9/Fx9qo9oLL8CRRw7c5gpoSZKk9nDFtCRJkqRhrd4BCl98EW65Jf9j3Smn5HUQZW046rXncAW0JEnS/GVhWpIkSVLHqdea49ln4YEH8vmHHqp/gMI3vSkfguUXv4BPfCIfvuX44z1AoSRJ0oJmYVqSJElSR+lrzTF9OqSUW3Psv39e9bzlljBqFBx1VI5dbTVYbrniceqtgJ44Ebq7ISLR3Z0vuxJakiRp6FiYliRJktQxXngBPvvZwa05Zs+Gv/0NVlopH4v5gAPy9gj48Y89QKEkSdJwY2FakiRJUtvVa80xdWouRr/5zbD88rk9R5lLL4WvfQ3e9rb+ba6AliRJGn4sTEuSJElqq6LWHPvsAz/7Wb7+wgvhRz+CJZaAI46AVVctHqesNQe4AlqSJGm4sTAtSZIk6WWptwq6WlFrjhdfhPHj8/ljjoGnnoLLL4cTT4Tvfc+DE0qSJC3sLExLkiRJalnRKugDD4SDD4aDDoJNN4U//jHHPvxw8RgPPJB/rrACLLlk/3Zbc0iSJC38LExLkiRJatnRRw9eBf3CC7mA/Lvf5ZXUyy6bt5e14LA1hyRJ0qLLwrQkSZIkoH5rjocegt//Ho48Mh+kcMaM4jEi4NFH4c9/hh12yNsmTLA1hyRJkgZqa2E6It4VEbdFxJ0RcVTB9StExB8j4oaI+F9E7NfOfCRJkiQVK2rNccAB/cXp7baD3XeHH/4QRoyA5ZcvHqerCxar+ZZhaw5JkiTValthOiIWB04BdgY2AfaIiE1qwj4J3JxS2hwYC3wnIpZEkiRJ0pA68sjBrTlmzcoHJgQ4+WS44op8kMK//x1+9KPWVkHbmkOSJEnV2rli+o3AnSmlu1NKs4FfA7vVxCRguYgI4FXA48DcNuYkSZIkLTLKWnPcdx+ccw585jMwe3be9uCDxWP0tezYeefcwmOppfJlV0FLkiTplWhnYXotoLrz3L2VbdV+CGwM3A/8FzgspTSvjTlJkiRJi4Si1hz77AOrrAJrrw0f+Qj85Cdw1105fu21i8fxAIWSJElqh3YWpqNgW6q5/E7gemBNYAvghxExqFtdRIyLiGsi4ppHHnlkfucpSZIkDQv1Dk7Y55ln4NJL4ZOfHNya48UX4fnn4aST4KqrcluOjTfO1514ogcolCRJ0tBpZ2H6XmCdqstrk1dGV9sP+H3K7gTuATaqHSilNDGltHVKaetVV121bQlLkiRJnapoBfS4cfDzn+frH3wQttwSRo2CnXbKRecizz8Phx0G22wDSyzRv93WHJIkSRpK7SxMXw2sHxHrVg5o+FHggpqYXmBHgIhYHdgQuLuNOUmSJEnD0vjxg1dAz5wJhx6az6+6Kqy5Zo77619hnXUGjwG25pAkSVJnaFthOqU0FzgUuAi4BTgnpfS/iDgkIg6phH0NeEtE/Bf4G/CFlNKj7cpJkiRJ6jT12nM8+yzcdFM+39tbfPvnnss/F18c/vQn+OpX4Z3vhBNOsDWHJEmSOteIdg6eUroQuLBm26lV5+8H3tHOHCRJkqRO1deeI6+Ezu059t8fzjoLHn0Urr8eXv1qmDEjr3SePn3wGGUroPtWO48fD729ia6uYMIEV0FLkiSpM7SzlYckSZKkEvPmwec+N7g9x+zZcPHFsNxycPTRcPrpefuECa2vgLY1hyRJkjpVW1dMS5IkSYuaSZP6VimPoauLAauU77kHfv1r+Mc/4Ior4Mkny8e57LKBl10BLUmSpIWJK6YlSZKk+aSvNcf06ZBSbs2x777wta/l6++4A445JheoP/QhWHnl4nHqtedwBbQkSZIWBhamJUmSpPnkqKMGt+aYOxdOOimfHzMm946++WaYOBG+/30PUChJkqRFk4VpSZIkqY5Jk2D0aNhhhzGMHp0vz54N//43fOc78P73w7HH5tj77ise44kn8s+llhq4SrqnJxeou7shItHdnS+7ElqSJEkLO3tMS5IkSSX6WnPkVdC5Ncc++8B++8GcOTlmvfXgDW/I57u6chuPWmWtOSAXoXt6YMqUqYwdO3Y+PwJJkiSpM7liWpIkSYuUohXQtWbMgF/9Cg45ZHBrjhdfhKWXhnPPhfvvhzvvhC9/OV83YYKtOSRJkqRmWJiWJEnSIqPo4ITjxsEJJ8C8eTnm85/PK5w/9jF49tnicZ59Fj74QVhjjYHbbc0hSZIkNcfCtCRJkhYZ48cPXgE9cyYcc0w+ICHAe96TD0p47bXlLTgateaYNg0mT57KtGkWpSVJkqQiDQvTEbF0RHwwIr4fEedGxC8i4vMRselQJChJkiQ1Uq89x6OPwvnn55Ybvb3lY6y1Vv75trfBpz8NW24Jxx9vaw5JkiSpHeoWpiPiWOCfwJuBfwM/Ac4B5gInRsQlEbFZu5OUJEmSyhS159h/f9h+e9h4Y1h1VXjf++Ccc8pXOnd3w4orDt5uaw5JkiSpPUY0uP7qlNKxJdd9NyJWA+r8I6MkSZLUPnPnwpFHDm7PMXs2TJ0Ku+wC++wD220HW2+dC8zjxg2Mb7QCuqcnn6ZMmcrYsWPb8jgkSZKkRU3dFdMppT/Xbqu09li+cv3DKaVr2pWcJEmSFk31WnNA7gG9004wahQ8+GD5OH/6Exx1VC5ML720K6AlSZKkTtHSwQ8j4kDgIuDPEXF8e1KSJEnSoqyoNce++8Ib39gfM3UqPPJI3r7KKsXjlLXt8OCEkiRJ0oJXt5VHRLw3pfTHqk1vTymNqVx3A3BMO5OTJEnSoiOl/HP8+MGtOebOhRtvhFmzYKml4NxzYfHF83VvfnPr7TkkSZIkLViNVkxvHhHnR8Tmlcs3RsSkiDgb+F+bc5MkSdJCoqg1x5w5cPXV8N3vwgc+AKuvDrffDr29xWPMnp2L0tBflAbbc0iSJEnDUd0V0ymlr0fEq4GvRgTAl4FXASNTSjcOQX6SJEka5vpac+QVzbk1xwEHwH775eI0wGteAzvvnM93deU2HrXKWnOAByiUJEmShptmekw/BxwOnAJMBPYAbm9jTpIkSRoGGh2g8MEH4be/hY9/fHBrjlmz8sEIzzkH7rsP7roLfv5z2HDD3IJj5MiB8bbmkCRJkhYujXpMfx14G7AE8JuU0q4RsSv54Ic/SymdNRRJSpIkqbMUrYI+8EC4+2740pfghRfyCue+FdFFnn0WPvShwdv7WnCMHw+9vYmurmDCBFtzSJIkSQuTRium35NSehvwFmBvgJTSBcA7gZXanJskSZI6VNEBCl94oX9V89JLwxlnwL/+Vd6Co1FrjmnTYPLkqUybZlFakiRJWtg0KkzfFBFnAecCU/s2ppTmppS+39bMJEmSNKTqteZ45hm45BI4+eR8ud4BCvvsuSe86U1w/PG25pAkSZI0UKODH+4ZEa8H5qSUbh2inCRJkjTEylpznHUWPPIIXH89zJsHI0bAvvu2doBCW3NIkiRJqlV3xXREbJdS+m9ZUToilo+I17UnNUmSJA2FlODzny9uzXHxxbDCCrmofPHF8PjjsPzyrR+g0NYckiRJkqo1auWxe0RcERFfjoh3R8QbI+JtEbF/pcXHn4BlhiBPSZIktaheaw6A22+HD3wAVl8d7r+/fJzJk+GrX4WddoLllsvbenpg4kTo7oaIRHd3vmzBWZIkSVIz6hamU0qfAd4NPAB8CPgacASwPvCTlNLbUkpXtz1LSZIktaSvNcf06ZBSbs2x336w6abwk5/kmFe9Cm64AXbZBVYqOay1ByiUJEmS1A51e0wDpJSeAE6rnCRJktThZs3KrTdqW3PMmQN33AFLLpkvr7km3HVXPj+wx3TmAQolSZIktUujVh6SJEnqEEWtOVLKLTnOOAP23x/WXx/e/W7o7S0eY+7cvHK6lq05JEmSJA2lhiumJUmStOANXNGcW3OMGwff/z5cXWmstvLKsN12uRf0nXfmNh61GrXm6OmBKVOmMnbs2DY8CkmSJEnKXDEtSZK0ADU6QOFzz+WDD37yk4Nbc8ycmXs8/+QncPPN8PDDcN55OXbChNyKo5qtOSRJkiR1iqYK0xExMiK+FBGnVS6vHxHvaW9qkiRJC7eiAxQedBB85jPw6KM55owzYMcd4amnisd49NE8xsYbw2JVn+xszSFJkiSpkzW7YvpMYBbw5srle4GvtyUjSZKkRUTRAQqffx5OOgkuuSRffv/74S9/gXXWKR6jUWuOadNg8uSpTJtmUVqSJElS52i2ML1eSumbwByAlNLzQLQtK0mSpGGqXmuOF1+E666DH/wALr64/ACFEfCBD+Tza68N73oXnHCCrTkkSZIkLTyaPfjh7IhYBkgAEbEeeQW1JEmSKsoOUHj++fD003DFFfDMMzn2E5/Iq53LDlC41FIDt/Wtdh4/Hnp7E11dwYQJroKWJEmSNDw1W5j+CvBXYJ2ImAT8H7Bvu5KSJEkajo46qvgAheedBxtuCHvuCdttl09dXfCWt1QXsrN6q6B7evJpypSpjB07tl0PQ5IkSZLarqnCdErpkoj4D7AtuYXHYSmlR9uamSRJUgeYNKlvlfIYurooXKV80klw2mlw773FY8ydC//97+DtroKWJEmStKhqqsd0RLwfmJtS+nNK6U/A3Ih4X1szkyRJWsD6WnNMnw4p5dYc++0Hb34zvOY18NxzOe6FF6C7G0aNKh7HAxRKkiRJ0kDNHvzwKymlp/oupJSeJLf3kCRJWijNmgXHHDO4NcecOXD11fDGN8JTlU9HRx0FF14IP/yhByiUJEmSpGY0W5guimu2P7UkSVLHmDQJRo+GHXYYw+jR+TLAk0/m4vIxx8Bb3worrAAzZhSPMW8e/PrXsOaaA7f39MDEiXn1dESiuztfdhW0JEmSJA3UbHH5moj4LnAKkIBPAde2LStJkqQ26GvNkVdB59Yc48bBo4/CZz4DKcGIEbDllnDoofDLX8IDDwwep1FrDg9QKEmSJEn1Nbti+lPAbOA3wLnAC8An25WUJElSK8pWQVe79Vb49KcHt+aYORO++104/nj429/yyul//xu+/W341rdszSFJkiRJ7dDUiumU0nPAUW3ORZIkqWVlq6Dvugte/3p4//tz3PveB48/XjzGjBm5T3StvhYc48dDb2+iqyuYMMHWHJIkSZL0SjW1YjoiNoiIiRFxcURM7ju1OzlJkqRGxo8vXgX9la/AgQfmftAAp58+uCd0n0atOaZNg8mTpzJtmkVpSZIkSZofmm3lcS5wHfBF4HNVJ0mSpPmuUWuOhx+GP/wBPvtZ6O0tHiMC/vtfWKzyaWe77eCb37Q1hyRJkiR1gmYPfjg3pfTjtmYiSZJEeWuOGTPgzjvhH/+A227LsUstBa9+dfkBCmtXSNuaQ5IkSZI6Q7Mrpv8YEZ+IiDUiYqW+U1szkyRJi6RjjiluzXHSSfC738H668OJJ8I//wlPPdX6AQptzSFJkiRJC16zK6b3qfysbt+RgNfM33QkSdLCaNKkvlXKY+jqonCV8hNP5G1lrTkefhjmzu1vzdHHVdCSJEmSNPw0VZhOKa3b7kQkSdLCqag1xwEHwDnnwOOPwxZbwA9+ACuskFdAv+pV8Oyzg8fp6hpclO7T05NPU6ZMZezYse17MJIkSZKk+aLZVh5ExOsi4sMRsXffqZ2JSZKkhcPRRw9uzTFrFlxwAcyZA+tW/vy92GK5Pcepp3qAQkmSJEla2DVVmI6IrwA/qJy2B74J7NrGvCRJUgebNAlGj4YddhjD6NH5MkBK+cCEP/0p7LsvrLdePmhhkQj417/giCMGbu/pgYkTobsbIhLd3fmyrTkkSZIkaeHRbI/pDwKbA9ellPaLiNWB09uXliRJ6lRFrTnGjcvX3XRTPjAhwKqrwnbb5d7RTzwxeJyurvL7sDWHJEmSJC3cmm3l8XxKaR4wNyKWBx6miQMfRsS7IuK2iLgzIo4qiRkbEddHxP8iYmrzqUuSpKH2/PN5hXNta46ZM/PBBz/wATjtNLj1VnjoIfj973P/aFtzSJIkSZKqNVuYviYiRgGnAdcC/wGuqneDiFgcOAXYGdgE2CMiNqmJGQX8CNg1pbQp8KFWkpckSfNPUXuOJ56AP/0pF5oBrrwSHn64+Pa9vbDNNnDggbDhhrlVB9iaQ5IkSZI0WFOF6ZTSJ1JKT6aUTgV2AvZJKe3X4GZvBO5MKd2dUpoN/BrYrSbmY8DvU0q9lfsp+aorSZLaqa89x/TpkFJuz7HXXrDSSvDe98JZZ+W4bbfNLTqKNGrNMW0aTJ48lWnTLEpLkiRJ0qKubmE6Ijaq/Nyy7wSsBIyonK9nLaD6cEf3VrZV2wBYMSKmRMS1EbF3a+lLkqQyZQco7JNSXgl92mlw2GGD23OkBCusAFOmwBe/mLeNHAnf+56tOSRJkiRJr0yjgx8eAYwDvlNwXQJ2qHPbKLlN7f1vBewILANcGRH/SindPmCgiHGVPOiqtxxLkiQB9Q9QOHcuXHAB/P3v8Mgj9cd5+mkYM2bgtr7VzuPHQ29voqsrmDDBVdCSJEmSpObVLUynlMZFxGLAF1NK/2xx7HuBdaourw3cXxDzaErpOeC5iLgc2BwYUJhOKU0EJgJsvfXWtcVtSZJUY/z48gMUvuUtcN11sPPO8Na35tM73pF7RNcq+3twT08+TZkylbFjx873/CVJkiRJC7eGPaZTSvOAb7+Msa8G1o+IdSNiSeCjwAU1MecDb42IERExEngTcMvLuC9JkhZ6jVpz9LnggtwrukhvL5x5Jtx9N/z85/0HKjz+eNtzSJIkSZKGTqNWHn0ujojdyQcqbGrFckppbkQcClwELA6ckVL6X0QcUrn+1JTSLRHxV+BGYB5wekrpptYfhiRJC7ei1hwHHQRXXZV7Qf/977ngvMUWMGcOLLkkzJ49eJyuLlhqqcHbbc8hSZIkSRpKzRamjwCWBeZGxAvk/tEppbR8vRullC4ELqzZdmrN5W8B32o6Y0mSFkHHHDO4Ncfzz8PJJ8Myy8Cb3wyzZuXtu+8OZ5xRXcjOGq2Atj2HJEmSJGmoNFWYTikt1+5EJEla1Eya1LdCeQxdXQxYoTxvHtxyS14Jffnlxf2fASLgySfzCulqroCWJEmSJHWyZldMExErAusDS/dtSyld3o6kJEla2JW15gDYYw9Yd93+YvQaa+TVzrUrpiG35qgtSvdxBbQkSZIkqVM1PPghQEQcCFxO7hd9XOXnse1LS5KkhdvRRxe35hg/HhZbDA49NPeMvvNOuO8+mDjRgxNKkiRJkhYeTRWmgcOAbYDpKaXtgTcAj7QtK0mShqlJk2D0aNhhhzGMHp0vAzzzDFx2WX/cjBnFt+9bJf25z8G++8J66+V2HT09uTjd3Q0Rie7ufNnWHJIkSZKk4ajZwvQLKaUXACJiqZTSrcCG7UtLkqThp689x/TpkFJuz7Hvvrktx6hRsMMOefUzwOqrF4/R1VU+fk8PTJsGkydPZdo0i9KSJEmSpOGr2cL0vRExCjgPuCQizgfub1dSkiQNNw8+CIcfPrg9x9y58MAD8MUvwiWXwMor5+3f+Y6tOSRJkiRJi66mCtMppfenlJ5MKR0LfAn4KfC+NuYlSVJHKGvN8dxz/SukN9wwH6Dw0UeLx5g9G447Dt7+dli6cghhW3NIkiRJkhZlzR788PsR8RaAlNLUlNIFKaXZ7U1NkqQFq6g1xwEH5O0vvAB77gm/+Q2svz5885vw6lcXj1PWnsPWHJIkSZKkRdWIJuP+A3wxIjYA/gD8JqV0TfvSkiRpwTvssMGtOWbNgvHjcxH5pptgo41g8cXzdWuumQvZ1bexPYckSZIkSYM128rj5ymlXYA3ArcD34iIO9qamSRJbVDUmiMluOsu+OlP4atf7Y997LHiMXp7889NN+0vSoPtOSRJkiRJalazBz/s81pgI2A0cOt8z0aSpDYqas2xzz6w0krw2tfCgQfmQvKcOTl+7bWLxylrzQG255AkSZIkqRnN9pjuWyH9VeAmYKuU0nvbmpkkSfNJSnDHHfDpTw9uzfHii/nghD/6Edx8M8yYAUsska878cTciqOarTkkSZIkSXrlml0xfQ/w5pTSu1JKZ6aUnmxjTpIkNa2oNUefW2/NByhcZx3YYAN4/PHiMZ5/Hj7+cdh4Y4jo325rDkmSJEmS2qPZHtOnppQebXcykiS1oqw1x2GH5esXWwwuuQT+7//gxz/OBycsYmsOSZIkSZKG1ogFnYAkSS/XMccUt+Y46yz4/vdh/fXhwQf7V0Evt1wuZFffxtYckiRJkiQNvVYPfihJUtsVtee4777888AD4dBDc9yMGcW3f/LJ/DPC1hySJEmSJHWipldMR8R2wPoppTMjYlXgVSmle9qXmiRpUdTXniOvas7tOfbaKx/AEGDFFWG33fL5rq7cxqNWo9YcPT0wZcpUxo4dO5+zlyRJkiRJzWhqxXREfAX4AnB0ZdMSwNntSkqStOh57DH4/e/hkEMGt+dIKRek//MfeOQROPPMvH3ChNyKo5qtOSRJkiRJ6nzNtvJ4P7Ar8BxASul+YLl2JSVJWrgUteboc+mlsOWWsOqqsPvu8OyzxWM8+SS84Q2w+OL922zNIUmSJEnS8NRsYXp2SikBCSAilm1fSpKkhUlfa47p0yGl3Jpj773h85/P1y+3HKywAhx3HPzjH+VtOMq29/TAtGkwefJUpk2zKC1JkiRJ0nDQbGH6nIj4CTAqIg4CLgVOa19akqSFxVFHDW7NMW8e/Pzn+fyb3gSXXQZf+hL83//B8cfbnkOSJEmSpIVdU4XplNK3gd8CvwM2BL6cUvpBOxOTJHWuotYcL76Ye0B/+9uw885wxBE59r77isd45JHi7bbnkCRJkiRp4TeimaCI+AxwbkrpkjbnI0nqcH2tOfIq6NyaY999q7fBxhvDdtvl811duY1HrbLWHJCL0D09MGXKVMaOHTt/H4AkSZIkSVrgmm3lsTxwUUT8PSI+GRGrtzMpSVJnuv9+OOywwa055s6FCDj77LxC+uabYfz4fN2ECbbmkCRJkiRJAzXbyuO4lNKmwCeBNYGpEXFpWzOTJA2ZotYc1X73O9h0U1hrLXjsseIxZs7Mq5zXXHPgdltzSJIkSZKkWs2umO7zMPAg8Biw2vxPR5I01Ppac0yfDinl1hz77QcbbABXX51jRo6EddaBb30L1lijeJxGrTmmTYPJk6cybZpFaUmSJEmSFnXN9pj+OPARYFXyQRAPSind3M7EJElDY/z4wa055szJhernnsuXd945nyAXpqv7SYOtOSRJkiRJUmuaXTHdDRyeUto0pfQVi9KS1PnK2nPcfz/84hew117w3e9Cb2/x7efMgaLjDtqaQ5IkSZIkvVJ1V0xHxPIppaeBb1Yur1R9fUrp8TbmJkl6mfrac+RVzf3tOT7/+VyYBlh1VVhvvdyCY/r0wWM0as3R0wNTpkxlbFH1WpIkSZIkqY5GrTx+CbwHuBZIQFRdl4DXtCkvSdLLlBJ87nPF7Tkefzz3iX7722GzzWCxxWD99W3NIUmSJEmShlbdVh4ppfdUfq6bUnpN5WffyaK0JA2hstYcfa64AvbfH9ZcEx54oHiMWbPgyCNhiy1yURpszSFJkiRJkoZeUz2mI+JvzWyTJLVHX2uO6dMhpdya48AD4cMfhnvvzTE33wznnQdjxsDKKxePU9aeo6cHpk2DyZOnMm2aRWlJkiRJktRedQvTEbF0pa/0KhGxYkSsVDmNBtYckgwlSYwfP7g1xwsvwLnnwj//mS/vtRc8/DD8+tfw/e/ndhzVbM8hSZIkSZI6RaMe0wcDh5OL0NfS32P6aeCU9qUlSUoJbr01t9/o7S2OiYCPfCSfX2qp/u19K57Hj4fe3kRXVzBhgiuhJUmSJElSZ2jUY/r7KaV1gSNrekxvnlL64RDlKEkLpaKe0c8/D3/5Cxx6KKy3HmyySS4ul7XgKNsOtueQJEmSJEmdq9GKaQBSSj+IiNcBmwBLV23/RbsSk6SFWV/P6NyeI/eMHjcOvvY1uO223HZjxx3hC1+AXXaByy+vjs9szSFJkiRJkoarpgrTEfEVYCy5MH0hsDPwD8DCtCS1aN48OPLIwT2jZ86EJ56Av/41H8Bw6aX7r7M1hyRJkiRJWpjUbeVR5YPAjsCDKaX9gM2BperfRJIWLUWtOWr96U+w9trw4IPFYzzyCLzznQOL0n1szSFJkiRJkhYWzRamn08pzQPmRsTywMPAa9qXliQNL32tOaZPh5Rya46DDoKDD4YPfCD3jYbcE/r//g9WXrl4nHo9oyVJkiRJkhYWzRamr4mIUcBpwLXAf4Cr2pWUJA0348cPbs3x/PMwcSL8+9/w6KN522abwbnnwve/n3tEV7NntCRJkiRJWlQ0e/DDT1TOnhoRfwWWTynd2L60JGn4uOUW6O0tvi4C7r03/6xmz2hJkiRJkrQoq7tiOiK2rD0BKwEjKuclaaFW1Dc6JbjhBvjyl2HTTWGTTQYXnvt0dZVfZ89oSZIkSZK0qGq0Yvo7da5LwA7zMRdJ6ih9faNzi47cN3rcODjnHLjgAlhsMXjb2+DjH8/xX/jCwHYetuaQJEmSJEkqVrcwnVLafqgSkaROc8wxg/tGz5wJV12Ve0fvthustlr/dSuuaGsOSZIkSZKkZjTVYzoi9i7anlL6xfxNR5IWvCeegOOPL+8b/dBDcNBBg7f39OTTlClTGTt2bFtzlCRJkiRJGs7q9piusk3V6a3AscCubcpJktqmrGf0ddfB5Mk5Zpll4Mwz888iXV1Dla0kSZIkSdLCqakV0ymlT1VfjogVgLPakpEktUlRz+j99oPPfjavgt58c7j+elh6abj3Xvjd76rjM/tGS5IkSZIkvXLNrpiuNRNYf34mIkntNn784J7Rc+bAU0/lntGXXtq/femlc1uOiROhuxsiEt3d+bJ9oyVJkiRJkl6ZZntM/xFIlYuLAZsA57QrKUman3p74Te/genTi6+fNau4ZzTYN1qSJEmSJKkdmipMA9+uOj8XmJ5SurcN+UhSSyZNyiuhe3vH0NWV22z09MDTT8NZZ8GvfgX//GeOXXJJmD178Bj2jJYkSZIkSRpaTbXySClNTSlNBa4DbgFmRsRKbc1Mkhro6xk9fTqklHtGH3hg3v788/DpT8OTT8LXvw533glnnJF7RFezZ7QkSZIkSdLQa7aVxzjga8DzwDwgyK09XtPgdu8Cvg8sDpyeUjqxJG4b4F/AR1JKv206e0mLtKKe0S+8kLf39MBdd8Ho0f3Xrbde/+16exNdXfHSCmtJkiRJkiQNnWZbeXwO2DSl9GizA0fE4sApwE7AvcDVEXFBSunmgrhvABc1O7YkQXnP6N7e/LO6KN3HntGSJEmSJEkLXlOtPIC7gJkNowZ6I3BnSunulNJs4NfAbgVxnwJ+Bzzc4viSFjEPPwzf+x48+2y+vOKKxXH2jJYkSZIkSepszRamjwauiIifRMTJfacGt1kLmFF1+d7KtpdExFrA+4FT6w0UEeMi4pqIuOaRRx5pMmVJw9GkSXml8w47jGH0aPj5z+EPf4DddoO11oIjjoC//S3H/uAH9oyWJEmSJEkajppt5fETYDLwX3KP6WZEwbZUc/kk4AsppRcjisIrN0ppIjARYOutt64dQ9JCou9ghrlvdD6Y4b775uvWWCMXpffZBzbZJG/r6w1tz2hJkiRJkqThpdnC9NyU0hEtjn0vsE7V5bWB+2titgZ+XSlKrwLsEhFzU0rntXhfkhYCRx01+GCGAKutlvtGjyh4x7JntCRJkiRJ0vDTbGH6sogYB/wRmNW3MaX0eJ3bXA2sHxHrAvcBHwU+Vh2QUlq373xE/Az4k0VpadEydy5cfDGceSbce29xzCOPFBelJUmSJEmSNDw122P6Y1T6TAPXVk7X1LtBSmkucChwEXALcE5K6X8RcUhEHPLyU5Y03NT2jZ40qf+6r34V3v1umDIFlluu+PYezFCSJEmSJGnh0tQaxOqVza1IKV0IXFizrfBAhymlfV/OfUjqbEV9o/feG266CU44IfeMfsMbcnH63HOrYzMPZihJkiRJkrTwaaowHRF7F21PKf1i/qYjaWFzzDGD+0bPmwc//WkuTK+3Xj6BBzOUJEmSJElaVDTbtXWbqvNLAzsC/wEsTEsqNHs2LLkkzJhRfP2jjxZv92CGkiRJkiRJC79mW3l8qvpyRKwAnNWWjCQNWy++CJdcAqefDtdeC3fckftDT58+ONa+0ZIkSZIkSYuuZg9+WGsmsP78TETS8FF7MMMf/hCOPRbWXRd23hmmToXdd4fnn8/9oUeOHHh7+0ZLkiRJkiQt2prtMf1HIFUuLgZsApzTrqQkda6igxkeeSTMmgXveAd85zuw2265jQfYN1qSJEmSJEmDNdtj+ttV5+cC01NK97YhH0kd7vOfH3www1mzYK214KKLim9j32hJkiRJkiRVq1uYjojXAqunlKbWbH9rRCyVUrqrrdlJ6hgXXJBbdtx/f/H1ZdslSZIkSZKkWo16TJ8EPFOw/fnKdZIWArU9oydNytvvu68/5pxz4NZbYYUVisfwYIaSJEmSJElqVqPC9OiU0o21G1NK1wCj25KRpCHV1zN6+nRIKfeMPuAA2GwzWGcduLHyDvCDH8A998App3gwQ0mSJEmSJL0yjQrTS9e5bpn5mYikBWP8+OKe0TffDF/6Eqy+et624oqw+OK5V/TEidDdDRGJ7u582YMZSpIkSZIkqVmNCtNXR8RBtRsj4gDg2vakJGmovPgi9PYWXzdvHhx3XH9hulpPD0ybBpMnT2XaNIvSkiRJkiRJak3dgx8ChwN/iIge+gvRWwNLAu9vY16S2ujhh+GMM+AnP4ERI2DOnMEx9oyWJEmSJElSu9RdMZ1Seiil9BbgOGBa5XRcSunNKaUH25+epJer6ICG114Le+wBa68NRx8N664LBx9sz2hJkiRJkiQNrUYrpgFIKV0GXNbmXCTNJ30HNMy9o/MBDceNg733hr/+FT7xCTjkENhooxy/7ba513Rvb6KrK5gwwfYckiRJkiRJap+mCtOShpeiAxrOnAkXXgj33Td4hXRPTz5NmTKVsWPHDlmekiRJkiRJWjQ1OvihpGHm97+H6dOLr5sxY3BRWpIkSZIkSRpqFqalhUD16uhTT4XFFy+O84CGkiRJkiRJ6gQWpqVhouhghjfeCAcdBK9+dV4NDXDWWXDmmR7QUJIkSZIkSZ3LHtPSMFB0MMO994Z582CZZfL5iBy7+uqw116w2GIe0FCSJEmSJEmdycK0NAwUHcxw3jwYNQruugtWWmnwbTygoSRJkiRJkjqVrTykDnfbbeUHM3zqqeKitCRJkiRJktTJLExLHSgluPRSeM97YKONyuM8mKEkSZIkSZKGIwvT0gJSdDDDPqedBjvtBFdfDcceC6ec4sEMJUmSJEmStPCwx7S0ABQdzHC//XIh+qST4EMfgiWWgD32gKWXzrdZYQUPZihJkiRJkqSFgyumpQWg6GCGc+bAmWfm8yuumAvVfUVpyEXoadNg8uSpTJtmUVqSJEmSJEnDl4VpaQHo7S3e/swzQ5uHJEmSJEmStCBYmJaGyAsvwOmn5+Jz2UELPZihJEmSJEmSFgX2mJba7JFH4Ec/ygcwfOQRGDEiH7Swv8d05sEMJUmSJEmStKiwMC3NJ5Mm9R2ccAxdXfDVr8I//wm/+EVeLb3LLvDZz8L220NEvo0HM5QkSZIkSdKiyMK0NB9MmlS9AjqYPh0+/nFYbz3Yc0/4zGdgk00G3qanJ5+mTJnK2LFjF0DWkiRJkiRJ0oJhYVqaD445ZmBbDsiXn3oKTjttweQkSZIkSZIkdSoL09IrMGdOXi3d21t8/YwZQ5uPJEmSJEmSNBwstqATkIazGTPggANgiSWKr+/qGtp8JEmSJEmSpOHAwrRUx6RJMHo07LDDGEaPhokT4Wtfg/32y9e/5jVw7bVw5pkwcuTA244cCRMmDHXGkiRJkiRJUuezlYdUouiAhgcfnK97z3tg9mxYcknYYot8Ahg/Hnp7E11dwYQJ+eCGkiRJkiRJkgZyxbRUYvz4wQc0BFhjDfjjH3NRulpPD0ybBpMnT2XaNIvSkiRJkiRJUhkL01KB228vP6Dhgw8ObS6SJEmSJEnSwsbCtFTl5pvzSueNNx68IrqPBzSUJEmSJEmSXhkL0xJw003w4Q/D614H558PRx4J3/ueBzSUJEmSJEmS2sHCtBYpkybB6NGwww5jGD06Xwa44gr461/hmGNyn+hvfAM+/nGYOBG6uyEi0d2dL9s7WpIkSZIkSXplRizoBKShMmkSjBvXd0DDYPp02G+/fN2++8KHPgQrrjjwNj09+TRlylTGjh07tAlLkiRJkiRJCylXTGuRMX58X1G635w5efuSSw4uSkuSJEmSJElqDwvTWmRMn168vbd3aPOQJEmSJEmSFnUWprVQe+IJeOqpfH611YpjurqGLh9JkiRJkiRJFqa1kJo1C777XVhvPfj61/O2734XRo4cGDdyJEyYMPT5SZIkSZIkSYsyC9Ma1iZNgtGjYYcdxjB6NJx9Nvzyl7DRRvDZz8Ib3wh77plje3pg4kTo7oaIRHd3vtzTsyAfgSRJkiRJkrToGbGgE5BerkmTYNy4vgMaBtOnw377wdy58IY3wGmnwdvfPvA2PT35NGXKVMaOHbsAspYkSZIkSZJkYVrD1vjxfUXpfnPnwsorwzXXwGL+P4AkSZIkSZLUkSzdadjq7S3e/vjjFqUlSZIkSZKkTuaKaQ07zz8PJ51Ufn1X15ClIkmSJEmSJOllaOu60oh4V0TcFhF3RsRRBdf3RMSNldMVEbF5O/PRwmHCBDjmGNhyS1h66YHXjRyZr5ckSZIkSZLUudpWmI6IxYFTgJ2BTYA9ImKTmrB7gDEppc2ArwET25WPhrd//jP3jQb4zGdg8uR8+fTTobsbIhLd3TBxYj64oSRJkiRJkqTO1c4V028E7kwp3Z1Smg38GtitOiCldEVK6YnKxX8Ba7cxHw0DkybB6NGwww5jGD0avvc9+PCHYbvt4LjjcszKK8P22+fzPT0wbRpMnjyVadMsSkuSJEmSJEnDQTt7TK8FzKi6fC/wpjrxBwB/KboiIsYB4wC6bCC80Jo0CcaNg5kzAYLp0+GII2CJJeDYY+HIIxdwgpIkSZIkSZLmi3aumI6CbakwMGJ7cmH6C0XXp5QmppS2Tiltveqqq87HFNVJxo/vK0oPtNpq8JWvwLLLDn1OkiRJkiRJkua/dq6YvhdYp+ry2sD9tUERsRlwOrBzSumxNuajDtfbW7z9/kGvGkmSJEmSJEnDWTtXTF8NrB8R60bEksBHgQuqAyKiC/g9sFdK6fY25qIOdv/9sNdekArX04PdWyRJkiRJkqSFS9sK0ymlucChwEXALcA5KaX/RcQhEXFIJezLwMrAjyLi+oi4pl35qPPMmgXf+AZssAGccw7sthsss8zAmJEjYcKEBZOfJEmSJEmSpPZo54ppUkoXppQ2SCmtl1KaUNl2akrp1Mr5A1NKK6aUtqictm5nPuosl1wCRx0FO+4IN98M550Hp50G3d0QkejuhokToadnQWcqSZIkSZIkaX5qa2FaApg0CUaPhh12GMNaa8GnP523v/vdcOWVcP75sN56eVtPD0ybBpMnT2XaNIvSkiRJkiRJ0sLIwrTaatIkGDcOpk+HlIL774cf/AB+9jOIgG23XdAZSpIkSZIkSRpqFqbVVuPHw8yZg7cfe+yQpyJJkiRJkiSpQ1iYVlv19ra2XZIkSZIkSdLCz8K05rt58+Af/8jnu7qKY8q2S5IkSZIkSVr4WZjWfHX77bDDDvDWt8J118GECTBy5MCYkSPzdkmSJEmSJEmLJgvTmi/mzIETToDNNoPrr4fTT4cttoCeHpg4Ebq7ISLR3Z0v9/Qs6IwlSZIkSZIkLSgWptWySZNg9GjYYYcxjB4NZ58NY8bAMcfAe98Lt9wCBxwAETm+pwemTYPJk6cybZpFaUmSJEmSJGlRZ2FaLZk0CcaNg+nTIaVg+nQ4+GDYdFP4wx/g3HNhjTUWdJaSJEmSJEmSOpmFabVk/HiYOXPgtpkz4ZJL4H3vWyApSZIkSZIkSRpmLEyrJb29rW2XJEmSJEmSpFoWptW0v/61v290ra6uoc1FkiRJkiRJ0vBlYVpNe+45WGstWHrpgdtHjoQJExZMTpIkSZIkSZKGHwvTquu3v4Uf/zif3313uPtuOP106O6GiER3N0ycCD09CzZPSZIkSZIkScOHhWkVeugh+NCH8mnSJJg3L28fMSIXoadNg8mTpzJtmkVpSZIkSZIkSa2xMC0mTYLRo2GHHcbQ3Q2f/CRsuilccAGccAJMmQKL+UqRJEmSJEmSNJ+MWNAJaMGaNAnGjYOZMwGC3l740Y9gvfXg73+HjTde0BlKkiRJkiRJWti4DnYRN358X1F6oDlzLEpLkiRJkiRJag8L04u43t7i7TNmDG0ekiRJkiRJkhYdFqYXYX//e3nv6K6uoc1FkiRJkiRJ0qLDwvQi6utfh7FjYeWVYemlB143ciRMmLBA0pIkSZIkSZK0CLAwvYhaYQXYe2+48044/XTo7oaIRHc3TJwIPT0LOkNJkiRJkiRJC6sRCzoBDY2U4Oyz82ro3XeHQw+FiHxdT08+TZkylbFjxy7QPCVJkiRJkiQt/FwxvQh48kn42MfyCulf/CJv6ytKS5IkSZIkSdJQc8X0QmjSJBg/Hnp7x7DaajB3bi5OT5gAX/jCgs5OkiRJkiRJ0qLOwvRCZtIkGDcOZs4ECB56KK+OPvZYOOaYBZycJEmSJEmSJGErj4XO+PF9Rel+KcEZZyyYfCRJkiRJkiSploXphUxvb2vbJUmSJEmSJGmoWZheiPzyl+XXdXUNXR6SJEmSJEmSVI+F6YXA3Llw5JHQ0wMbbgjLLDPw+pEj84EPJUmSJEmSJKkTWJheCEycCN/5DnzqU3DjjXDaadDdDRGJ7u58fU/Pgs5SkiRJkiRJkrIRCzoBvXxz58KIEXDQQblVx3vek7f39OTTlClTGTt27ALNUZIkSZIkSZJquWJ6mPrVr+B1r4OHH4YllugvSkuSJEmSJElSp7MwPUxMmgSjR8MOO4xhhRXgYx+DVVeFlBZ0ZpIkSZIkSZLUGlt5DAOTJsG4cTBzJkDw9NO5hceBB8Lqqy/o7CRJkiRJkiSpNa6YHgbGj+8rSvebOxe+8pUFk48kSZIkSZIkvRIWpoeB3t7WtkuSJEmSJElSJ7MwPQx0dbW2XZIkSZIkSZI6mYXpYWDCBBg5cuC2kSPzdkmSJEmSJEkabixMDwM9PTBxInR3Q0Siuztf7ulZ0JlJkiRJkiRJUussTA8TPT0wbRpMnjyVadMsSkuSJEmSJEkavixMS5IkSZIkSZKGlIVpSZIkSZIkSdKQsjAtSZIkSZIkSRpSFqYlSZIkSZIkSUPKwrQkSZIkSZIkaUhZmJYkSZIkSZIkDSkL05IkSZIkSZKkIWVhWpIkSZIkSZI0pCxMS5IkSZIkSZKGVFsL0xHxroi4LSLujIijCq6PiDi5cv2NEbFlO/ORJEmSJEmSJC14bStMR8TiwCnAzsAmwB4RsUlN2M7A+pXTOODH7cpHkiRJkiRJktQZ2rli+o3AnSmlu1NKs4FfA7vVxOwG/CJl/wJGRcQabcxJkiRJkiRJkrSAtbMwvRYwo+ryvZVtrcZIkiRJkiRJkhYiI9o4dhRsSy8jhogYR271AfBsRNz2CnPrJKsAj7Ypvp1jtzu+k3JpNb6Tcmk13lzmT3wn5dJqfCfl0mp8J+XSanwn5dJqfCfl0mp8J+XSanwn5dJqvLnMn/hOyqXV+E7KpdX4Tsql1fhOyqXV+E7KpdX4Tsql1XhzmT/xnZRLq/GdlEur8Z2US6vxnZRLq/GdlEur8Z2Uy8Kmu/SalFJbTsCbgYuqLh8NHF0T8xNgj6rLtwFrtCunTjwB17Qrvp1jm/vwyGU4595JuZj78IjvpFzMfXjEd1Iu5m4u5j484jspF3MfHvGdlMtwzr2TcjH34RHfSbmY+/CI76RcFqVTO1t5XA2sHxHrRsSSwEeBC2piLgD2jmxb4KmU0gNtzEmSJEmSJEmStIC1rZVHSmluRBwKXAQsDpyRUvpfRBxSuf5U4EJgF+BOYCawX7vykSRJkiRJkiR1hnb2mCaldCG5+Fy97dSq8wn4ZDtzGAYmtjG+nWO3O76Tcmk1vpNyaTXeXOZPfCfl0mp8J+XSanwn5dJqfCfl0mp8J+XSanwn5dJqfCfl0mq8ucyf+E7KpdX4Tsql1fhOyqXV+E7KpdX4Tsql1fhOyqXVeHOZP/GdlEur8Z2US6vxnZRLq/GdlEur8Z2US6vxnZTLIiMqfU4kSZIkSZIkSRoS7ewxLUmSJEmSJEnSYAv66IuL6gl4F3Abub/2UU3EnwE8DNzUROw6wGXALcD/gMMaxC8NXAXcUIk/ron7WBy4DvhTE7HTgP8C19PEUUiBUcBvgVsrj+HNdWI3rIzbd3oaOLxO/Gcqj/Em4FfA0g1yOawS+7+icYvmBVgJuAS4o/JzxQbxH6qMPw/Yuonxv1V5bm4E/gCMqhP7tUrc9cDFwJrNvKaAI4EErNIgl2OB+6qe/13qjQ18qvK6/x/wzQZj/6Zq3GnA9Q3itwD+1fc6A97YIH5z4Erya/OPwPL19p+yea0TXzivdeIHzWud2MJ5LYsvm9c645fNa+n4tXNbZ+zCea0TXzivdeIHzSsl73F15rQsvmxOy+LL9tWy+EHzWhZbZ07Lxi6b09Lxa+e0wfiD5rVObNmclsUX7qtVeQ74fVQ2ryWxpe+/JfGFc1onvvQ9uCi+wXtw7diFc1pv7KI5rTN+6XtwSXzhvJbEls4pBZ8ZGsxpUXy936tF8aXzWhJf9h48KLbBnBaNXTqvZeOXzWvJ+GXvwUWxW1A+p0Xx9eZ1FDWf7ag/r0XxZe/BRbH15rQovt7npUHxdd6Di8Y+lvI5LRyb8jktGr/e56Wi+MJ5LYkt+6xU+Pmb8t+rZfFlc1oWX/Z7tSy+6Pdq3e8ODJ7TsrGPpfj3aun4DP6sVDZ22X5aFr8FxXNaFl9vXx30XYnyeS2Krff+WxRfb18tii97/y39jkfx+2/R2MdSvq8Wjk/5vlo0ftm8FsUWzmmd+Hpzehg132fL5rROfNm+WhRbb06L4uu9/w6KbzCvReMXzmvZ2HXmtGjseu+/RfH15rUovnpee4FHaKLuQP7u+ywwq/JY3lkndmXggcrcPtbE2DsBjwJzgeeBHRrEvxF4DJhTiX9/E7k/XMn72co8l8WOruTRN/apTby+/1CJf6HyvNZ7j+upyX1eZQ7L4s+sxL1A/j16dJ3YJSvx/yV/9xlb/fpe1E4LPIFF8UT+gnYX8JrKC/IGYJMGt3kbsCUFRcSC2DWALSvnlwNurzc+EMCrKueXAP4NbNvgPo4AfknzhelVGsVVxf8cOLByfklqvvg3eF4fBLpLrl8LuAdYpnL5HGDfOuO9jvzLYSS5H/ulwPqN5oX8QfOoyvmjgG80iN+Y/MFxCoM/wBXFvwMYUTn/jb7xS2KrP5R8msqbdb3XFLngdxEwnYG/6IvGPxY4spnXK7B95TlcqnJ5tWZf38B3gC83GP9iYOfK+V2AKQ3irwbGVM7vD3yt3v5TNq914gvntU78oHmtE1s4r2XxZfNaZ/yyeS2LHzS39XIpmtc6YxfOa534QfNKyXtcnTktiy+b07L4sn21LH7QvJbF1pnTsrHL5rQsvnB/rZdP7bzWGbtsTsviC/fVqvsb8PuobF5LYkvff0viC+e0Tnzpe3BRfIP34NqxC+e0Tnzpe3BZLmXvwSXj13sPro0tnVMKPjM0mNOi+Hq/V4viS+e1JL7sPXhQbIM5LRq7dF5L4uv9bi3Mp2heS8auN6dF8fXmddBnuwbzWhRf9h5cFFtvTovi631eKvxcWjSvJWPXm9Oi+HpzWvczMoM/LxWNX/YeXBRb9/23sv2lz9/15rQkvu57cEF83ffggvhG78EDvjsUzWmdsUvntSS+0Xtw4feY2jktGbt0Xy2JL/sMXPhdqWhe68SW7adl8WWflcriiz4rlX7HK5rTOmMXzmmd+LLPSg2/c9L/Wals7LL9tCy+bE4Lv88WzWmD+EHzWie2bE7L4st+p5Z+Fy+Z17LxB81rndiyOW2mLlD9O7Vs/LJ5LYuvntcTgdNoou4A7EX+bvQ/YF1y7elbJbHLko/7NoGBhemysd8AvJ/83foO4L4G8SMrz+uW5D9YPFx5jGXxfd/bnwLOJRemy2JHA3fTZD2mcr93AR+pPN8rk98bm6nf3AHc3WD8r5Jfl31zOY38HlUU+0ngzL7XGXAtsFi93ykL88lWHgvGG4E7U0p3p5RmA78Gdqt3g5TS5cDjzQyeUnogpfSfyvlnyH+tWatOfEopPVu5uETllMriI2Jt4N3A6c3k04qIWJ688/+0ktvslNKTTd58R+CulNL0OjEjgGUiYgT5zeL+OrEbA/9KKc1MKc0FppLfhF9SMi+7kT/cU/n5vnrxKaVbUkq3FSVQEn9xJR/If3Fdu07s01UXl6VqXuu8pr4HfJ6a10CLr8Gi2I8DJ6aUZlViHm5m7IgI4MPkFQH14hN5dSzAClTNbUn8hsDllfOXALtXYsv2n8J5LYsvm9c68YPmtU5s4bw22PcHzevLeK8oix80t43Grp3XOvGF81onftC81nmPK5vTwvg6c1oWX7avlsUPmtcG789Fc9rS+3md+ML9tdH41fNaJ7ZsTsviC/fVyv0V/T4qnNei2HrvvyXxhXNaJ770PbjO79JB89rq792S+NL34HrjF70Hl8QXzmtJbOmclij9vVqk3ryWxJfOa0l86byWKPy9Op+Uzms9RfNaoPT3aonCea3z2a5sXy2ML5rXOrGFc1onvnBOG3wuHTCvrX6GrRNfOKeNxq+d0zrxg+a1Tmwz+2r15+9m9tWX4pvcV6vjm9lXq+Mb7au13x0a7avNfNcoi2+0rw4au8F+Wh3fzL5aHV9vXou+K5XN66DYBnNaFF9vToviy+a07Dte2Zy28p2wLL7enJaOXzCvRbH15rQovmxOy77Pls1pYXzJvJbFls1pWXzZnNb7Ll40rw2/uzcRWzandccumNOy+LJ5LYuvntdTgLE1j6NsHtcmrwxOKaV7yP+l/+Gi2JTScymlU4Anmhk7pXRdSukP5O/Ws4ClI2KpOvEzU0qXVeKD/jkri78c2BqYTS6s13ucADNpvh7zDvIK9H9X7uuxlNKL9cZP/XWEUfTPb1n8LeRCN8AylcewY0nsJsDfKvfxMPBk5XEvkixMLxhrATOqLt9LnWLQKxERo8l/1fp3g7jFI+J68l+wLkkp1Ys/ifyLYF6TaSTg4oi4NiLGNYh9DflfVM6MiOsi4vSIWLbJ+/kodb5gpZTuA75N/jeYB4CnUkoX1xnvJuBtEbFyRIwk/1VznSbyWD2l9EDlPh8g/wWsXfYH/lIvICImRMQM8r+ifLlB7K7kv3re0EIOh0bEjRFxRkSsWCduA+CtEfHviJgaEds0Of5bgYdSSnc0iDsc+FblsX6b/K8z9dwE7Fo5/yEK5rZm/2k4r83ub03ED5rX2thG81od38y8FuRSd15r4uvObcnjLJ3XmvjDaTCvNfGF81ryHlc6py2+JzYTP2BOy+KL5rUott6c1smlcE5L4kvntMFjHTCvJbGHUzKnJfH19tWTGPz7qGxei2LraRRfu58WxtfZVwfF15nXslzK9tOi+Hr7ab3HWrSvFsUfTvG8FsXWm9Oizwz13n9b+YzRTHztvBbGl8zroNgG779luZTNa1F8vXmt91hr57Uo9nDK33+L4svmteyzXdm8tvJZsJnY6jktjS+Z08L4knmtl0vRnJbFl81po8daO6dl8YczeF7LYht+VmLg5+9mPgPX/bzeQnzZZ+AB8XXegwfENthXy3Jp9Bm4Or7R5+Cix1nvM3B1/OE0/gxcHV84r3W+Kw2a11a/VzUZ/9Kc1ouvndOy2LI5bZDLoDmtE184p0081pfmtU7s4RTMaZ34sn217Pts2b7ayvffZmKr99PS+JL9tDC+zr5aL5/aeS2LLdtPGz3W2n21LP5wivfVsvjaeV2j5jGXzeNa5NdHn3vJK8tbqVE0836+PHBdpZBf73vVm8irxV8LHFIpvhfGV373HEL+ndRMLusCfwbWjYi3NojfgPz55efAehHx+RYfa6Pfdb8lt/LYiLyPfhtYtST2BmC3iBgREesCW9FcrWnhlDpg2faidiK/qZxedXkv4AdN3G40TbTyqIp/FflfAj7Qwm1Gkfu2vq7k+vcAP6qcH0tzrTz6en+tRt4B31Yndmtyn6A3VS5/n4J/Gyy43ZLkXker14lZEZgMrEpeiXcesGeDcQ8A/kP+S+WpwPcazQvwZM31TzQzj5T/K3lZ/HjyX0KjmdcI+RdfbX/al+LJf3H/N7BC5fI0Bv+bbu1jXZ38V8HFyP/+c0ad2JuAk8l/KX0j+d/QGuYO/Bj4bBPP+8nk1bGQ/yJ8aYP4jci/IK8FvkLVvy4V7T9NzGvh/lZnXsvii+a1dF8umdeX4puc19rHWjqvJfGlc1vncZbNa+3Yjea1Nr7RvI6i8h7XaE5r4xvNaZ34QXNaL77OvPbFbtZoTgsea905LYivu7/Weaxl81o9dt05LYgvnFNKfh8VzWtZbNmcNhE/YE4bxdfOaVE8JftqncdZOKd14gvntInHOmBO64w/aF7rxJbupxR8Ziia03rx9fbVBvFF7791P8PUzGtR7qX7akl8vd+rRfH13n/rPdbaeS0au3RfLYkv21cLP9uVzWtZfNG8NhFbu682/JxZM6dF8d8qmtc6j7NsXy2LL9tXGz3W2jktG79oXy2LbfQ7dcDn77I5LYuvt682iC/8vVoWXzuvtbE091mp9rE2+qxUG19vXy17nGW/U2vHbvRZqTa+bF8t/K5UNK9lsWVz2kR87b7a8Htb35yWxO5dNqd1HmfZvloWX7avNnqsL81rnbEL57ROfL3fq4O+zxbNab34OvNaL7bod2rd79YM3k+Lcq/3e7Uovmxei2Lr7af1HuugfbVk/Hq/V4via+f1CZqoO5BXVx9O//f8nwLPNXh/PrLmddPo/Xwn8orp9ZqMH01uh3EVua9zWe7fJre5uIlKG5Y6sUuR23GMJq8Kn0EuIJfFH1mZ0zeQV2NfSV7R3Cj39wEvNPG8/x95n7yJ/PnoNuDpktgRlTm+HjgfuBDYrTp2UTot8AQWxRP5YCIXVV0+Gji6iduNpsnCNPkX1UXAES8jv69Q3gvvBPJf3KaR+5TNBM5uYexjy8auXP9qYFrV5bcCf25i3N2AixvEfAj4adXlval8YW4y9+OBTzSal8ob0BqV82sAtzUzj7RQmAb2qbyRjmz2NULuKVc7zkvxwOvJqxSnVU5zyX/pe3WT49c+D7WX/0pVU39yf6dVGzzOEcBD5LYWje7vKfo/OASDfwnUy30D4Kqqy4P2n3rzWhRfb17L4ovmtd7YRfNaG99oXpsYv/Z5LnpuCue2zuMsnNeSsUvntYncB8xr1favkD+Y1N1Xa+Mb7atF8UVz2mj8sv21KvZL9ea0ibFHF41d89zU3V9LHmvp/lozdt19tUHuL80pJb+Piua1LLZsTuvFF81po/Fr57Qk/ncl83pyE2OPbjD22WVz2uCxDprTOuMPmtcmn5fC/bRy3bG0tq8eS2v76kvxRfPaaPwG++qxtLavFo09umjsmuem2X21+rE22lf7xm52Xy3KvXpfLfxsVzavZfFF81ovtmhOG41dsK8Wxf+tZF5f38TYoxuM/eeyOW3wWIv21bLxi/bVZp6XQfsqNZ+/y+a0LL7RvloUXzSvjcYv2lerY2nuM3C9sUcz+DNs7XNTuq+WPM56n4Frx270Gbhe7tX7auF3paJ5LYstm9N68UVz2mj86jktib2sbE6bHPulOa3zvJTtq/Ue64B5rTN24Zw2mXu936vHA58omtN68Y321drYojltNHbRfloQf1jZvDY5/uii8auel2Z/p1Y/1rq/U2vGb/b3alHuG5CLmA3rDuT60jeqXsMXkftx13t/ri1M1/vuuza5t/PdzcRXP/fkfXPrOrn/nfzZcTa5vcXjlee3mbGnNBj7o8DPquK/BHyuidx/CjzYxPN+CvngpH3P+xnkdi3N7GtX0OC4cwvzyVYeC8bVwPoRsW5ELEneQS6YX4NHRJB3nltSSt9tIn7ViBhVOb8M8HZyY/pBUkpHp5TWTimNruQ9OaW0Z52xl42I5frOk/v63FQWn1J6EJgRERtWNu0I3NzoMQB70PjfAnuBbSNiZOU52pHcB6hURPT9S0kXefVpM/96eAH5lzGVn+c3cZumRcS7gC8Au6aUZjaIXb/q4q6UzCtASum/KaXVUkqjK/N7L/ngcg/WGb/634neT525Jf/1cIfK7Tagf/VGPW8Hbk0p3dsgDvKb/pjK+R3If5EtVTW3iwFfJP9lut7+UzivL2N/K4wvmtc6sYXzWhRfb17rjF84r3Ue63kUz23Z8zJoXuuMXTivdXIfNK913uPK5rTp98R68WX7ap34QfNaEntdnTktG7tsTsse63kUzGmD52bAvNaJLZvTstwL99U6v48GzWurv7vK4svmtE584b5aEr97ybx+umTswjmt81jPo2BOGzw3g/bVOvGD5rXO81L2/lv2maFsX23pM0ZZfJ19tSy+aF8tir26zr5aNnbZvlr2WM+jeF+t99zU7qtlsWX7alnuZftq2We7wnlt5bNgWWydfbUsvmxfLYr/T8m8/rdk7LJ9texxnkfxvlrveSnaV8vii/bVsuelcE6r1H7+bvQZuJnP66XxTXwGro2v9zn4pdgmPwPXjt3oM3DtYz2P8s/BRc9Lvc/AtfGNPgPX5l42r2XflYrmtdXvVYXxdea0LL5oTotif19nTsvGLpvTssd6HsVzWu+5qZ3XstiyOS3LvXRfjeLvs6X7akl8oaLYevtpSXzpfloQ/4t6+2rJ+GW/V4se53mU7Kd1npfCfbUkvnRfLcm9dl4nMVDZPF4AvDffNNYlH0jx3JLYMmWfvUaR/8j5TfKCg0bx60buhw55UdGG5D8qlH0GeCuwHfngjSeRi/S/KBl71Yjo6+m8ROVx3l3nebmI/J+nS1cuj6HO55HKfSxGbq3yVKPHSt4/31K53bL0H/S9KPeR0d9KbCdgbkqpmbrXwqm2Uu1paE7kF/ft5L/CjW8i/lfkPkFzyG/AB9SJ3Y7cO+dG8l/Vrgd2qRO/GXBdJf4mCo78XHK7sTRo5UHuWXdD5fS/Jh/rFsA1lXzOA1ZsED8SeIzKv/Q0iD2O/MvuJuAsKkfcrRP/d/Kb1Q3Ajs3MC/nfSf5G/kXzN2ClBvHvr5yfRf5r4EUN4vv+TaVvbk+tE/u7ymO9Efgj+cB5Tb2mGPyvUUXjnwX8tzL+BfT/NbAodknyqrqbyP+mtEOjXMh/0Tykyed9O/K/Od1A/hevrRrEH0beB28nH+W476/XhftP2bzWiS+c1zrxg+a1TmzhvJbFl81rnfHL5rUsftDc1suFgnmtM3bhvNaJHzSvlLzHUT6nZfFlc1oWX7avlsUPmtey2DpzWjZ22ZyWxRfur/XyqZ3XOmOXzWlZfOG+Wvb7qGxeS2JL339L4gvntE586Xtwo9+l1LwHF4xdOKd14kvfg8tyqZ3TBuOXvgcXxJa9/xZ+Ziib0zrxZftqWXzZvloWX7SvNvy8w8B9tWzssn21LL5sXy3Np3Ze64xdtq+WxZfuqxR8tiub1zrxZfNaFFu6r5bE1/u8NCi+zrwWjV26r5bE1/u8VJhL7Zw2GL9sXoti683poM/fDea0KL7eZ+Ci+HrzWhRf9nmp7ncHBn8GLhq73rwWxZftq4W51JnTorHrfQYuiq83r4O+K5XNa0lsvTktiq83p0XxZXNa9ztewZwWjV1vTovi6+2rhfkUzWvJ2PXmtCi+3pwO+j5bNqd14svef4ti681pUXy9999G38Vr57Vo/LLfq0Wx9ea0MJeiOa0zfr15LYqvntebabLuQP7u+wz5u9IccsuIenP+LPlYIIm8Cv2Ldcb+YiVmTiV+NrltSFn8XuSibl/8o03k3vc4nwbOqRO7eytjV25zRVX8M03E/63yGJt53s8l95jue97r5T6avPL6FnJLre6i30eLyqnvi4AkSZIkSZIkSUNisQWdgCRJkiRJkiRp0WJhWpIkSZIkSZI0pCxMS5IkSZIkSZKGlIVpSZIkSZIkSdKQsjAtSZIkSZIkSRpSFqYlSZLUsSIiRcR3qi4fGRHHzqexfxYRH5wfYzW4nw9FxC0RcVnBdRtExIURcWcl5pyIWL3dObVTRLwvIjZZ0HlIkiSps1mYliRJUiebBXwgIlZZ0IlUi4jFWwg/APhESmn7mjGWBv4M/Dil9NqU0sbAj4FV51+mC8T7AAvTkiRJqsvCtCRJkjrZXGAi8JnaK2pXPEfEs5WfYyNiamX18e0RcWJE9ETEVRHx34hYr2qYt0fE3ytx76ncfvGI+FZEXB0RN0bEwVXjXhYRvwT+W5DPHpXxb4qIb1S2fRnYDjg1Ir5Vc5OPAVemlP7YtyGldFlK6aaIWDoizqyMd11EbF8Zb9+IOC8i/hgR90TEoRFxRCXmXxGxUiVuSkScFBFXVPJ5Y2X7SpXb31iJ36yy/diIOKNyu7sj4tNVj2vPynN3fUT8pK8oHxHPRsSEiLihMtbqEfEWYFfgW5X49SLi0xFxc+U+f93MpEuSJGnhZ2FakiRJne4UoCciVmjhNpsDhwGvB/YCNkgpvRE4HfhUVdxoYAzwbnLxeGnyCuenUkrbANsAB0XEupX4NwLjU0oDVgRHxJrAN4AdgC2AbSLifSmlrwLXAD0ppc/V5Pg64NqS/D8JkFJ6PbAH8PNKbn23+1gllwnAzJTSG4Argb2rxlg2pfQW4BPAGZVtxwHXpZQ2A44BflEVvxHwzsq4X4mIJSJiY+AjwP+llLYAXgR6+sYH/pVS2hy4HDgopXQFcAHwuZTSFimlu4CjgDdU7vOQkscrSZKkRYyFaUmSJHW0lNLT5ALqpxvFVrk6pfRASmkWcBdwcWX7f8nF6D7npJTmpZTuAO4mF2ffAewdEdcD/wZWBtavxF+VUrqn4P62AaaklB5JKc0FJgFvayHfWtsBZwGklG4FpgMbVK67LKX0TErpEeApoG/Fde1j+1Xl9pcDy0fEqJpxJwMrVxX8/5xSmpVSehR4GFgd2BHYCri68nzsCLymEj8b+FPl/LU1913tRmBSROxJXgEvSZIkMWJBJyBJkiQ14STgP8CZVdvmUlloEREBLFl13ayq8/OqLs9j4GfgVHM/CQjgUymli6qviIixwHMl+UWD/Iv8j7xau9XxXuljq9UXVz3ui5WxAvh5SunogtvNSSmlmvgi7yYX6XcFvhQRm1aK95IkSVqEuWJakiRJHS+l9DhwDrnNRp9p5NW8ALsBS7yMoT8UEYtV+k6/BrgNuAj4eEQsARARG0TEsg3G+TcwJiJWqfRg3gOY2uA2vwTeEhHv7tsQEe+KiNeTW2P09N0/0FXJrRUfqdx+O3Jrkqdqxh0LPFpZkV7mb8AHI2K1ym1WiojuBvf7DLBcJX4xYJ2U0mXA54FRwKtafBySJElaCLliWpIkScPFd4BDqy6fBpwfEVeRC6hlq5nruY1cQF4dOCSl9EJEnE5uS/GfykrsR4D31RskpfRARBwNXEZeZXxhSun8Brd5vnLAxZMi4iRgDrntxWHAj8g9r/9LXhm+b0ppVk6naU9ExBXA8sD+lW3HAmdGxI3ATGCfBjneHBFfBC6uFJnnkPtfT69zs18Dp1UOoPhR4KeVdiEBfC+l9GQrD0KSJEkLp+j/7ztJkiRJC4OImAIcmVK6ZkHnIkmSJBWxlYckSZIkSZIkaUi5YlqSJEmSJEmSNKRcMS1JkiRJkiRJGlIWpiVJkiRJkiRJQ8rCtCRJkiRJkiRpSFmYliRJkiRJkiQNKQvTkiRJkiRJkqQhZWFakiRJkiRJkjSk/h+wYUZdXTQHNwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (25,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 110, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 110, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('Scree Plot - The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 78 components about 95% variance is captured!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 78)\n",
    "pca = pca.fit(X_ss)\n",
    "X_pca = pca.transform(X_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.072971</td>\n",
       "      <td>-1.460001</td>\n",
       "      <td>0.843851</td>\n",
       "      <td>-0.031405</td>\n",
       "      <td>-0.198537</td>\n",
       "      <td>0.041218</td>\n",
       "      <td>3.639129</td>\n",
       "      <td>-0.666183</td>\n",
       "      <td>-0.375089</td>\n",
       "      <td>-0.180246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783823</td>\n",
       "      <td>0.597565</td>\n",
       "      <td>-0.698986</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>1.531082</td>\n",
       "      <td>0.153715</td>\n",
       "      <td>-1.173616</td>\n",
       "      <td>-0.983218</td>\n",
       "      <td>1.003078</td>\n",
       "      <td>-0.709471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.729662</td>\n",
       "      <td>-0.076566</td>\n",
       "      <td>0.424369</td>\n",
       "      <td>1.257941</td>\n",
       "      <td>-0.715867</td>\n",
       "      <td>-0.178679</td>\n",
       "      <td>-0.627939</td>\n",
       "      <td>-0.775879</td>\n",
       "      <td>-0.281483</td>\n",
       "      <td>0.125241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089851</td>\n",
       "      <td>1.043801</td>\n",
       "      <td>-1.117287</td>\n",
       "      <td>0.161797</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>-0.329366</td>\n",
       "      <td>0.459452</td>\n",
       "      <td>-1.244365</td>\n",
       "      <td>-1.428857</td>\n",
       "      <td>0.204333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.433413</td>\n",
       "      <td>2.325575</td>\n",
       "      <td>-2.094361</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>-0.155164</td>\n",
       "      <td>0.177166</td>\n",
       "      <td>-0.383571</td>\n",
       "      <td>-0.439822</td>\n",
       "      <td>0.786435</td>\n",
       "      <td>-0.289955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201362</td>\n",
       "      <td>-0.427234</td>\n",
       "      <td>0.176529</td>\n",
       "      <td>-0.014020</td>\n",
       "      <td>0.148487</td>\n",
       "      <td>0.030766</td>\n",
       "      <td>0.077516</td>\n",
       "      <td>0.620877</td>\n",
       "      <td>0.334703</td>\n",
       "      <td>-0.498613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.563926</td>\n",
       "      <td>0.304788</td>\n",
       "      <td>0.653972</td>\n",
       "      <td>1.277377</td>\n",
       "      <td>-0.905252</td>\n",
       "      <td>-0.192487</td>\n",
       "      <td>-0.805909</td>\n",
       "      <td>-1.100770</td>\n",
       "      <td>0.067372</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044589</td>\n",
       "      <td>0.242338</td>\n",
       "      <td>-0.139068</td>\n",
       "      <td>0.052924</td>\n",
       "      <td>0.248148</td>\n",
       "      <td>0.186457</td>\n",
       "      <td>-0.248181</td>\n",
       "      <td>-0.111156</td>\n",
       "      <td>0.046751</td>\n",
       "      <td>0.770481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.309271</td>\n",
       "      <td>-2.069686</td>\n",
       "      <td>-0.797775</td>\n",
       "      <td>-0.065407</td>\n",
       "      <td>0.030175</td>\n",
       "      <td>-0.020236</td>\n",
       "      <td>-0.604636</td>\n",
       "      <td>-0.081661</td>\n",
       "      <td>1.109055</td>\n",
       "      <td>-0.366882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190988</td>\n",
       "      <td>-0.233212</td>\n",
       "      <td>-0.245115</td>\n",
       "      <td>-0.011588</td>\n",
       "      <td>0.258121</td>\n",
       "      <td>0.363643</td>\n",
       "      <td>-0.006449</td>\n",
       "      <td>0.470935</td>\n",
       "      <td>0.686676</td>\n",
       "      <td>0.015295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.072971 -1.460001  0.843851 -0.031405 -0.198537  0.041218  3.639129   \n",
       "1  4.729662 -0.076566  0.424369  1.257941 -0.715867 -0.178679 -0.627939   \n",
       "2 -2.433413  2.325575 -2.094361  0.765568 -0.155164  0.177166 -0.383571   \n",
       "3  5.563926  0.304788  0.653972  1.277377 -0.905252 -0.192487 -0.805909   \n",
       "4 -3.309271 -2.069686 -0.797775 -0.065407  0.030175 -0.020236 -0.604636   \n",
       "\n",
       "         7         8         9   ...        68        69        70        71  \\\n",
       "0 -0.666183 -0.375089 -0.180246  ...  0.783823  0.597565 -0.698986 -0.155929   \n",
       "1 -0.775879 -0.281483  0.125241  ... -0.089851  1.043801 -1.117287  0.161797   \n",
       "2 -0.439822  0.786435 -0.289955  ...  0.201362 -0.427234  0.176529 -0.014020   \n",
       "3 -1.100770  0.067372  0.028372  ...  0.044589  0.242338 -0.139068  0.052924   \n",
       "4 -0.081661  1.109055 -0.366882  ...  0.190988 -0.233212 -0.245115 -0.011588   \n",
       "\n",
       "         72        73        74        75        76        77  \n",
       "0  1.531082  0.153715 -1.173616 -0.983218  1.003078 -0.709471  \n",
       "1  0.086142 -0.329366  0.459452 -1.244365 -1.428857  0.204333  \n",
       "2  0.148487  0.030766  0.077516  0.620877  0.334703 -0.498613  \n",
       "3  0.248148  0.186457 -0.248181 -0.111156  0.046751  0.770481  \n",
       "4  0.258121  0.363643 -0.006449  0.470935  0.686676  0.015295  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run the same above models using the pca data and see the output --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.19      0.28      7015\n",
      "         1.0       0.90      0.98      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.58      0.61     56954\n",
      "weighted avg       0.85      0.88      0.85     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.35      0.41      7015\n",
      "         1.0       0.91      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.70      0.65      0.67     56954\n",
      "weighted avg       0.86      0.88      0.87     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.26      0.33      7015\n",
      "         1.0       0.90      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.87     56954\n",
      "   macro avg       0.67      0.61      0.63     56954\n",
      "weighted avg       0.85      0.87      0.85     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.21      0.30      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.70      0.59      0.62     56954\n",
      "weighted avg       0.85      0.88      0.85     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.21      0.30      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.59      0.62     56954\n",
      "weighted avg       0.85      0.88      0.86     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.18      0.27      7015\n",
      "         1.0       0.89      0.98      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.58      0.60     56954\n",
      "weighted avg       0.85      0.88      0.85     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.17      0.25      7015\n",
      "         1.0       0.89      0.98      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.57      0.59     56954\n",
      "weighted avg       0.85      0.88      0.85     56954\n",
      "\n",
      "[16:27:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.25      0.34      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.61      0.64     56954\n",
      "weighted avg       0.85      0.88      0.86     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.22      0.31      7015\n",
      "         1.0       0.90      0.98      0.94     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.73      0.60      0.62     56954\n",
      "weighted avg       0.86      0.88      0.86     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc7f688fd0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.24      0.33      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.72      0.61      0.63     56954\n",
      "weighted avg       0.86      0.88      0.86     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.8789</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.5834</td>\n",
       "      <td>0.9339</td>\n",
       "      <td>0.2801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.9486</td>\n",
       "      <td>0.3524</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>0.4102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.9534</td>\n",
       "      <td>0.2637</td>\n",
       "      <td>0.6086</td>\n",
       "      <td>0.9271</td>\n",
       "      <td>0.3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8972</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.9698</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>0.3002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.2133</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8790</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.1087</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.2664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.8928</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>0.9341</td>\n",
       "      <td>0.2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.8789</td>\n",
       "      <td>0.9011</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.2513</td>\n",
       "      <td>0.6092</td>\n",
       "      <td>0.9334</td>\n",
       "      <td>0.3383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.8819</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.9354</td>\n",
       "      <td>0.3105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.8806</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>0.2411</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.3322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.8789         0.8952          0.1079      0.9755   \n",
       "LDA                        0.8752         0.9107          0.1001      0.9486   \n",
       "KNeighborsClassifier       0.8685         0.9010          0.1045      0.9534   \n",
       "ADABoostClassifier         0.8768         0.8972          0.1067      0.9698   \n",
       "GradientBoostClassifier    0.8795         0.8974          0.1065      0.9730   \n",
       "RandomForestClassifier     0.8790         0.8940          0.1087      0.9774   \n",
       "ExtraTreeClassifier        0.8788         0.8928          0.1095      0.9789   \n",
       "XGBoostClassifier          0.8789         0.9011          0.1044      0.9671   \n",
       "LightGBMClassifier         0.8819         0.8980          0.1061      0.9754   \n",
       "CatBoostClassifier         0.8806         0.9003          0.1048      0.9704   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.1913         0.5834        0.9339   \n",
       "LDA                           0.3524         0.6505        0.9302   \n",
       "KNeighborsClassifier          0.2637         0.6086        0.9271   \n",
       "ADABoostClassifier            0.2145         0.5922        0.9324   \n",
       "GradientBoostClassifier       0.2133         0.5932        0.9340   \n",
       "RandomForestClassifier        0.1785         0.5779        0.9340   \n",
       "ExtraTreeClassifier           0.1666         0.5727        0.9341   \n",
       "XGBoostClassifier             0.2513         0.6092        0.9334   \n",
       "LightGBMClassifier            0.2160         0.5957        0.9354   \n",
       "CatBoostClassifier            0.2411         0.6057        0.9344   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.2801  \n",
       "LDA                             0.4102  \n",
       "KNeighborsClassifier            0.3306  \n",
       "ADABoostClassifier              0.3002  \n",
       "GradientBoostClassifier         0.3035  \n",
       "RandomForestClassifier          0.2664  \n",
       "ExtraTreeClassifier             0.2530  \n",
       "XGBoostClassifier               0.3383  \n",
       "LightGBMClassifier              0.3105  \n",
       "CatBoostClassifier              0.3322  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "result_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "result_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that although accuracy is good, recall,precision,f1scores are low for class zero which is the minority class.\n",
    "### That is because we have an imbalanced data set.\n",
    "\n",
    "### Let's try and resolve that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------------------------\n",
    "# Class Imbalance techniques - \"imblearn\"\n",
    "\n",
    "## Over Sampling \n",
    "- Random Over Sampling\n",
    "- SMOTE\n",
    "## Under Sampling \n",
    "- Random Under Sampling\n",
    "- Near Miss\n",
    "- Tomek Links\n",
    "## Combined Approaches\n",
    "- Smote + Tomek \n",
    "- Smote + ENN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Over Sampling - Over PCA transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({1.0: 116314, 0.0: 116314})\n"
     ]
    }
   ],
   "source": [
    "#Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "#ros = ros.fit(X_pca, Y)\n",
    "X_resampled_os, y_resampled_os = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_resampled_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.77      0.47      7015\n",
      "         1.0       0.96      0.79      0.86     49939\n",
      "\n",
      "    accuracy                           0.78     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.78      0.82     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.76      0.47      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.62      0.40      7015\n",
      "         1.0       0.94      0.79      0.86     49939\n",
      "\n",
      "    accuracy                           0.77     56954\n",
      "   macro avg       0.62      0.71      0.63     56954\n",
      "weighted avg       0.86      0.77      0.80     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.76      0.45      7015\n",
      "         1.0       0.96      0.77      0.86     49939\n",
      "\n",
      "    accuracy                           0.77     56954\n",
      "   macro avg       0.64      0.77      0.65     56954\n",
      "weighted avg       0.88      0.77      0.81     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.75      0.48      7015\n",
      "         1.0       0.96      0.81      0.88     49939\n",
      "\n",
      "    accuracy                           0.80     56954\n",
      "   macro avg       0.66      0.78      0.68     56954\n",
      "weighted avg       0.88      0.80      0.83     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.31      0.37      7015\n",
      "         1.0       0.91      0.95      0.93     49939\n",
      "\n",
      "    accuracy                           0.87     56954\n",
      "   macro avg       0.69      0.63      0.65     56954\n",
      "weighted avg       0.85      0.87      0.86     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.14      0.22      7015\n",
      "         1.0       0.89      0.98      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.70      0.56      0.58     56954\n",
      "weighted avg       0.84      0.88      0.85     56954\n",
      "\n",
      "[16:49:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.70      0.49      7015\n",
      "         1.0       0.95      0.84      0.89     49939\n",
      "\n",
      "    accuracy                           0.82     56954\n",
      "   macro avg       0.66      0.77      0.69     56954\n",
      "weighted avg       0.88      0.82      0.84     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.75      0.49      7015\n",
      "         1.0       0.96      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.79      0.69     56954\n",
      "weighted avg       0.89      0.81      0.83     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc6396a310>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.71      0.50      7015\n",
      "         1.0       0.95      0.84      0.89     49939\n",
      "\n",
      "    accuracy                           0.83     56954\n",
      "   macro avg       0.67      0.78      0.70     56954\n",
      "weighted avg       0.88      0.83      0.85     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.8645</td>\n",
       "      <td>0.4663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>0.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.9244</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.7883</td>\n",
       "      <td>0.6241</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>0.3986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.9398</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>0.7605</td>\n",
       "      <td>0.7669</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.4507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.7785</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>0.4817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.9060</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.3082</td>\n",
       "      <td>0.6305</td>\n",
       "      <td>0.9296</td>\n",
       "      <td>0.3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8778</td>\n",
       "      <td>0.8904</td>\n",
       "      <td>0.1113</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.1423</td>\n",
       "      <td>0.5617</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>0.2229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.8197</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.8366</td>\n",
       "      <td>0.6996</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.4888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.8091</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>0.7852</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.4930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.8408</td>\n",
       "      <td>0.7139</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.5015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.7838         0.9422          0.1038      0.7862   \n",
       "LDA                        0.7876         0.9420          0.1034      0.7915   \n",
       "KNeighborsClassifier       0.7681         0.9244          0.1005      0.7883   \n",
       "ADABoostClassifier         0.7717         0.9398          0.1037      0.7732   \n",
       "GradientBoostClassifier    0.8019         0.9426          0.1027      0.8096   \n",
       "RandomForestClassifier     0.8734         0.9060          0.1020      0.9528   \n",
       "ExtraTreeClassifier        0.8778         0.8904          0.1113      0.9812   \n",
       "XGBoostClassifier          0.8197         0.9397          0.1006      0.8366   \n",
       "LightGBMClassifier         0.8091         0.9442          0.1028      0.8170   \n",
       "CatBoostClassifier         0.8252         0.9420          0.1010      0.8408   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.7666         0.7764        0.8645   \n",
       "LDA                           0.7599         0.7757        0.8673   \n",
       "KNeighborsClassifier          0.6241         0.7062        0.8563   \n",
       "ADABoostClassifier            0.7605         0.7669        0.8559   \n",
       "GradientBoostClassifier       0.7474         0.7785        0.8776   \n",
       "RandomForestClassifier        0.3082         0.6305        0.9296   \n",
       "ExtraTreeClassifier           0.1423         0.5617        0.9337   \n",
       "XGBoostClassifier             0.6996         0.7681        0.8906   \n",
       "LightGBMClassifier            0.7534         0.7852        0.8824   \n",
       "CatBoostClassifier            0.7139         0.7774        0.8940   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.4663  \n",
       "LDA                             0.4685  \n",
       "KNeighborsClassifier            0.3986  \n",
       "ADABoostClassifier              0.4507  \n",
       "GradientBoostClassifier         0.4817  \n",
       "RandomForestClassifier          0.3749  \n",
       "ExtraTreeClassifier             0.2229  \n",
       "XGBoostClassifier               0.4888  \n",
       "LightGBMClassifier              0.4930  \n",
       "CatBoostClassifier              0.5015  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_resampled_os, y_resampled_os)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "ros_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "ros_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({0.0: 16578, 1.0: 16578})\n"
     ]
    }
   ],
   "source": [
    "#Random Under Sampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "#rus.fit(X_pca, y)\n",
    "X_resampled_us, y_resampled_us = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_resampled_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.77      0.46      7015\n",
      "         1.0       0.96      0.78      0.86     49939\n",
      "\n",
      "    accuracy                           0.78     56954\n",
      "   macro avg       0.65      0.78      0.66     56954\n",
      "weighted avg       0.88      0.78      0.81     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.77      0.47      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.75      0.42      7015\n",
      "         1.0       0.95      0.74      0.83     49939\n",
      "\n",
      "    accuracy                           0.74     56954\n",
      "   macro avg       0.62      0.75      0.62     56954\n",
      "weighted avg       0.87      0.74      0.78     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.76      0.45      7015\n",
      "         1.0       0.96      0.77      0.85     49939\n",
      "\n",
      "    accuracy                           0.77     56954\n",
      "   macro avg       0.64      0.76      0.65     56954\n",
      "weighted avg       0.88      0.77      0.80     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.76      0.48      7015\n",
      "         1.0       0.96      0.80      0.88     49939\n",
      "\n",
      "    accuracy                           0.80     56954\n",
      "   macro avg       0.66      0.78      0.68     56954\n",
      "weighted avg       0.88      0.80      0.83     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.76      0.47      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.77      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.75      0.46      7015\n",
      "         1.0       0.96      0.79      0.86     49939\n",
      "\n",
      "    accuracy                           0.78     56954\n",
      "   macro avg       0.64      0.77      0.66     56954\n",
      "weighted avg       0.88      0.78      0.81     56954\n",
      "\n",
      "[16:54:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.77      0.45      7015\n",
      "         1.0       0.96      0.77      0.86     49939\n",
      "\n",
      "    accuracy                           0.77     56954\n",
      "   macro avg       0.64      0.77      0.66     56954\n",
      "weighted avg       0.88      0.77      0.81     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.77      0.48      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.89      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc62149450>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.78      0.48      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.79      0.67     56954\n",
      "weighted avg       0.89      0.79      0.82     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7682</td>\n",
       "      <td>0.7758</td>\n",
       "      <td>0.8627</td>\n",
       "      <td>0.4640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.7861</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.7662</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.8661</td>\n",
       "      <td>0.4687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.7384</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7451</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.7696</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.4460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.9435</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.8045</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.4820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.7874</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>0.8672</td>\n",
       "      <td>0.4668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.7871</td>\n",
       "      <td>0.7524</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>0.4605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.7716</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.7721</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>0.4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.4772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.9446</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>0.4783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.7814         0.9420          0.1039      0.7833   \n",
       "LDA                        0.7861         0.9425          0.1037      0.7889   \n",
       "KNeighborsClassifier       0.7400         0.9345          0.1038      0.7384   \n",
       "ADABoostClassifier         0.7681         0.9390          0.1036      0.7696   \n",
       "GradientBoostClassifier    0.7990         0.9435          0.1032      0.8045   \n",
       "RandomForestClassifier     0.7874         0.9415          0.1032      0.7919   \n",
       "ExtraTreeClassifier        0.7828         0.9405          0.1032      0.7871   \n",
       "XGBoostClassifier          0.7716         0.9412          0.1042      0.7715   \n",
       "LightGBMClassifier         0.7919         0.9438          0.1039      0.7947   \n",
       "CatBoostClassifier         0.7907         0.9446          0.1043      0.7923   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.7682         0.7758        0.8627   \n",
       "LDA                           0.7662         0.7775        0.8661   \n",
       "KNeighborsClassifier          0.7518         0.7451        0.8328   \n",
       "ADABoostClassifier            0.7578         0.7637        0.8534   \n",
       "GradientBoostClassifier       0.7595         0.7820        0.8753   \n",
       "RandomForestClassifier        0.7555         0.7737        0.8672   \n",
       "ExtraTreeClassifier           0.7524         0.7698        0.8641   \n",
       "XGBoostClassifier             0.7726         0.7721        0.8556   \n",
       "LightGBMClassifier            0.7713         0.7830        0.8701   \n",
       "CatBoostClassifier            0.7790         0.7857        0.8691   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.4640  \n",
       "LDA                             0.4687  \n",
       "KNeighborsClassifier            0.4160  \n",
       "ADABoostClassifier              0.4460  \n",
       "GradientBoostClassifier         0.4820  \n",
       "RandomForestClassifier          0.4668  \n",
       "ExtraTreeClassifier             0.4605  \n",
       "XGBoostClassifier               0.4546  \n",
       "LightGBMClassifier              0.4772  \n",
       "CatBoostClassifier              0.4783  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_resampled_us, y_resampled_us)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "rus_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "rus_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({1.0: 116314, 0.0: 116314})\n"
     ]
    }
   ],
   "source": [
    "# Smote\n",
    "smote = SMOTE(random_state=42,sampling_strategy = \"not majority\")\n",
    "X_sm, y_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.76      0.47      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.75      0.47      7015\n",
      "         1.0       0.96      0.80      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.69      0.39      7015\n",
      "         1.0       0.94      0.74      0.83     49939\n",
      "\n",
      "    accuracy                           0.74     56954\n",
      "   macro avg       0.61      0.72      0.61     56954\n",
      "weighted avg       0.86      0.74      0.78     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.74      0.45      7015\n",
      "         1.0       0.96      0.78      0.86     49939\n",
      "\n",
      "    accuracy                           0.78     56954\n",
      "   macro avg       0.64      0.76      0.66     56954\n",
      "weighted avg       0.88      0.78      0.81     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.73      0.49      7015\n",
      "         1.0       0.96      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.78      0.68     56954\n",
      "weighted avg       0.88      0.81      0.83     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.54      0.47      7015\n",
      "         1.0       0.93      0.90      0.91     49939\n",
      "\n",
      "    accuracy                           0.85     56954\n",
      "   macro avg       0.68      0.72      0.69     56954\n",
      "weighted avg       0.87      0.85      0.86     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.50      0.46      7015\n",
      "         1.0       0.93      0.91      0.92     49939\n",
      "\n",
      "    accuracy                           0.86     56954\n",
      "   macro avg       0.68      0.70      0.69     56954\n",
      "weighted avg       0.87      0.86      0.86     56954\n",
      "\n",
      "[17:21:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.70      0.48      7015\n",
      "         1.0       0.95      0.83      0.89     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.77      0.68     56954\n",
      "weighted avg       0.88      0.81      0.84     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.73      0.49      7015\n",
      "         1.0       0.96      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.78      0.69     56954\n",
      "weighted avg       0.88      0.81      0.84     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc7f297050>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.71      0.49      7015\n",
      "         1.0       0.95      0.83      0.89     49939\n",
      "\n",
      "    accuracy                           0.82     56954\n",
      "   macro avg       0.66      0.77      0.69     56954\n",
      "weighted avg       0.88      0.82      0.84     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.7899</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.8690</td>\n",
       "      <td>0.4706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.7941</td>\n",
       "      <td>0.9418</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.8002</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>0.8721</td>\n",
       "      <td>0.4731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.7361</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.7428</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.7155</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.3911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.7781</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>0.4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.9418</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.8195</td>\n",
       "      <td>0.7326</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.8826</td>\n",
       "      <td>0.4855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8536</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.8983</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.4741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8581</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.9090</td>\n",
       "      <td>0.4954</td>\n",
       "      <td>0.7022</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>0.4623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.8134</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.8292</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8863</td>\n",
       "      <td>0.4809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.7344</td>\n",
       "      <td>0.7787</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.4905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.8192</td>\n",
       "      <td>0.9407</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.8347</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.4914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.7899         0.9421          0.1033      0.7944   \n",
       "LDA                        0.7941         0.9418          0.1029      0.8002   \n",
       "KNeighborsClassifier       0.7361         0.9270          0.1021      0.7428   \n",
       "ADABoostClassifier         0.7781         0.9382          0.1026      0.7837   \n",
       "GradientBoostClassifier    0.8088         0.9418          0.1020      0.8195   \n",
       "RandomForestClassifier     0.8536         0.9267          0.0974      0.8983   \n",
       "ExtraTreeClassifier        0.8581         0.9230          0.0975      0.9090   \n",
       "XGBoostClassifier          0.8134         0.9390          0.1008      0.8292   \n",
       "LightGBMClassifier         0.8120         0.9425          0.1020      0.8229   \n",
       "CatBoostClassifier         0.8192         0.9407          0.1009      0.8347   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.7582         0.7763        0.8690   \n",
       "LDA                           0.7505         0.7754        0.8721   \n",
       "KNeighborsClassifier          0.6882         0.7155        0.8315   \n",
       "ADABoostClassifier            0.7378         0.7608        0.8610   \n",
       "GradientBoostClassifier       0.7326         0.7760        0.8826   \n",
       "RandomForestClassifier        0.5356         0.7169        0.9150   \n",
       "ExtraTreeClassifier           0.4954         0.7022        0.9183   \n",
       "XGBoostClassifier             0.7015         0.7653        0.8863   \n",
       "LightGBMClassifier            0.7344         0.7787        0.8848   \n",
       "CatBoostClassifier            0.7091         0.7719        0.8901   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.4706  \n",
       "LDA                             0.4731  \n",
       "KNeighborsClassifier            0.3911  \n",
       "ADABoostClassifier              0.4503  \n",
       "GradientBoostClassifier         0.4855  \n",
       "RandomForestClassifier          0.4741  \n",
       "ExtraTreeClassifier             0.4623  \n",
       "XGBoostClassifier               0.4809  \n",
       "LightGBMClassifier              0.4905  \n",
       "CatBoostClassifier              0.4914  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_sm, y_sm)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "smote_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "smote_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Near Miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape: Counter({0.0: 16578, 1.0: 16578})\n"
     ]
    }
   ],
   "source": [
    "# Near Miss\n",
    "nm = NearMiss()\n",
    "X_nm, y_nm = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape:', Counter(y_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.16      0.77      0.27      7015\n",
      "         1.0       0.93      0.44      0.60     49939\n",
      "\n",
      "    accuracy                           0.48     56954\n",
      "   macro avg       0.55      0.61      0.43     56954\n",
      "weighted avg       0.84      0.48      0.56     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.81      0.28      7015\n",
      "         1.0       0.94      0.46      0.61     49939\n",
      "\n",
      "    accuracy                           0.50     56954\n",
      "   macro avg       0.56      0.63      0.45     56954\n",
      "weighted avg       0.85      0.50      0.57     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.15      0.68      0.25      7015\n",
      "         1.0       0.91      0.47      0.62     49939\n",
      "\n",
      "    accuracy                           0.49     56954\n",
      "   macro avg       0.53      0.57      0.43     56954\n",
      "weighted avg       0.82      0.49      0.57     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.15      0.78      0.25      7015\n",
      "         1.0       0.92      0.36      0.52     49939\n",
      "\n",
      "    accuracy                           0.41     56954\n",
      "   macro avg       0.53      0.57      0.38     56954\n",
      "weighted avg       0.82      0.41      0.48     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.15      0.80      0.25      7015\n",
      "         1.0       0.92      0.34      0.50     49939\n",
      "\n",
      "    accuracy                           0.40     56954\n",
      "   macro avg       0.54      0.57      0.37     56954\n",
      "weighted avg       0.83      0.40      0.47     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.83      0.24      7015\n",
      "         1.0       0.92      0.27      0.42     49939\n",
      "\n",
      "    accuracy                           0.34     56954\n",
      "   macro avg       0.53      0.55      0.33     56954\n",
      "weighted avg       0.82      0.34      0.40     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.83      0.24      7015\n",
      "         1.0       0.92      0.29      0.44     49939\n",
      "\n",
      "    accuracy                           0.35     56954\n",
      "   macro avg       0.53      0.56      0.34     56954\n",
      "weighted avg       0.83      0.35      0.41     56954\n",
      "\n",
      "[17:27:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.80      0.24      7015\n",
      "         1.0       0.92      0.30      0.46     49939\n",
      "\n",
      "    accuracy                           0.37     56954\n",
      "   macro avg       0.53      0.55      0.35     56954\n",
      "weighted avg       0.82      0.37      0.43     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.80      0.24      7015\n",
      "         1.0       0.92      0.32      0.47     49939\n",
      "\n",
      "    accuracy                           0.38     56954\n",
      "   macro avg       0.53      0.56      0.35     56954\n",
      "weighted avg       0.82      0.38      0.44     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc611de290>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.82      0.24      7015\n",
      "         1.0       0.92      0.31      0.46     49939\n",
      "\n",
      "    accuracy                           0.37     56954\n",
      "   macro avg       0.53      0.56      0.35     56954\n",
      "weighted avg       0.83      0.37      0.43     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.4793</td>\n",
       "      <td>0.9009</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.4382</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>0.5961</td>\n",
       "      <td>0.2675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.4989</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.1103</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>0.2845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4926</td>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>0.4664</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.6172</td>\n",
       "      <td>0.2480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.4109</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.3593</td>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.5168</td>\n",
       "      <td>0.2457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.3968</td>\n",
       "      <td>0.8931</td>\n",
       "      <td>0.1137</td>\n",
       "      <td>0.3397</td>\n",
       "      <td>0.8030</td>\n",
       "      <td>0.5713</td>\n",
       "      <td>0.4969</td>\n",
       "      <td>0.2469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.8888</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>0.8311</td>\n",
       "      <td>0.5530</td>\n",
       "      <td>0.4233</td>\n",
       "      <td>0.2377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.3527</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>0.8285</td>\n",
       "      <td>0.5572</td>\n",
       "      <td>0.4364</td>\n",
       "      <td>0.2397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.3656</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.3754</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>0.5579</td>\n",
       "      <td>0.4699</td>\n",
       "      <td>0.2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.3691</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8153</td>\n",
       "      <td>0.5608</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.2415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.4793         0.9009          0.1106      0.4382   \n",
       "LDA                        0.4989         0.9075          0.1103      0.4554   \n",
       "KNeighborsClassifier       0.4926         0.8932          0.1119      0.4664   \n",
       "ADABoostClassifier         0.4109         0.8925          0.1135      0.3593   \n",
       "GradientBoostClassifier    0.3968         0.8931          0.1137      0.3397   \n",
       "RandomForestClassifier     0.3434         0.8888          0.1158      0.2749   \n",
       "ExtraTreeClassifier        0.3527         0.8898          0.1154      0.2858   \n",
       "XGBoostClassifier          0.3656         0.8886          0.1154      0.3046   \n",
       "LightGBMClassifier         0.3754         0.8899          0.1149      0.3157   \n",
       "CatBoostClassifier         0.3691         0.8906          0.1148      0.3064   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.7719         0.6050        0.5961   \n",
       "LDA                           0.8087         0.6320        0.6144   \n",
       "KNeighborsClassifier          0.6791         0.5728        0.6172   \n",
       "ADABoostClassifier            0.7788         0.5690        0.5168   \n",
       "GradientBoostClassifier       0.8030         0.5713        0.4969   \n",
       "RandomForestClassifier        0.8311         0.5530        0.4233   \n",
       "ExtraTreeClassifier           0.8285         0.5572        0.4364   \n",
       "XGBoostClassifier             0.7999         0.5522        0.4571   \n",
       "LightGBMClassifier            0.8001         0.5579        0.4699   \n",
       "CatBoostClassifier            0.8153         0.5608        0.4600   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.2675  \n",
       "LDA                             0.2845  \n",
       "KNeighborsClassifier            0.2480  \n",
       "ADABoostClassifier              0.2457  \n",
       "GradientBoostClassifier         0.2469  \n",
       "RandomForestClassifier          0.2377  \n",
       "ExtraTreeClassifier             0.2397  \n",
       "XGBoostClassifier               0.2370  \n",
       "LightGBMClassifier              0.2399  \n",
       "CatBoostClassifier              0.2415  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_nm, y_nm)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "nearmiss_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "nearmiss_pca_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomek Links - slow\n",
    "tl = TomekLinks(sampling_strategy ='majority')\n",
    "X_tl, y_tl = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_tl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs super slow on our big big dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete later\n",
    "scores = []\n",
    "pred_test = lr.predict(X_test.values)\n",
    "pred_test_probs = lr.predict_proba(X_test.values)\n",
    "probs = lr.decision_function(X_test.values)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values.ravel(),pred_test)\n",
    "p,r,t = precision_recall_curve(y_test.values.ravel(),probs)\n",
    "scores.append((\"weighted_base\",\n",
    "                           f1_score(y_test.values.ravel(),pred_test),\n",
    "                           precision_score(y_test.values.ravel(),pred_test),\n",
    "                           recall_score(y_test.values.ravel(),pred_test),\n",
    "                           accuracy_score(y_test.values.ravel(),pred_test),\n",
    "                           auc(fpr, tpr),\n",
    "                           auc(p,r,reorder=True),\n",
    "                           confusion_matrix(y_test.values.ravel(),pred_test)))\n",
    "\n",
    "scores = pd.DataFrame(scores,columns=['Sampling Type','f1','precision','recall','accuracy','auc_roc','auc_pr','confusion_matrix'])\n",
    "### To capture all metrics in one df - for final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smote with Tomek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({1.0: 116015, 0.0: 116015})\n"
     ]
    }
   ],
   "source": [
    "# Smotetomek - Over + Under sampling\n",
    "smotek = SMOTETomek(random_state=42)\n",
    "X_smotek, y_smotek = smotek.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_smotek))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.76      0.47      7015\n",
      "         1.0       0.96      0.79      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.75      0.47      7015\n",
      "         1.0       0.96      0.80      0.87     49939\n",
      "\n",
      "    accuracy                           0.79     56954\n",
      "   macro avg       0.65      0.78      0.67     56954\n",
      "weighted avg       0.88      0.79      0.82     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.69      0.39      7015\n",
      "         1.0       0.94      0.74      0.83     49939\n",
      "\n",
      "    accuracy                           0.74     56954\n",
      "   macro avg       0.61      0.72      0.61     56954\n",
      "weighted avg       0.86      0.74      0.78     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.74      0.45      7015\n",
      "         1.0       0.96      0.78      0.86     49939\n",
      "\n",
      "    accuracy                           0.78     56954\n",
      "   macro avg       0.64      0.76      0.66     56954\n",
      "weighted avg       0.88      0.78      0.81     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.73      0.49      7015\n",
      "         1.0       0.96      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.78      0.68     56954\n",
      "weighted avg       0.88      0.81      0.83     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.54      0.47      7015\n",
      "         1.0       0.93      0.90      0.91     49939\n",
      "\n",
      "    accuracy                           0.85     56954\n",
      "   macro avg       0.68      0.72      0.69     56954\n",
      "weighted avg       0.87      0.85      0.86     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.49      0.46      7015\n",
      "         1.0       0.93      0.91      0.92     49939\n",
      "\n",
      "    accuracy                           0.86     56954\n",
      "   macro avg       0.68      0.70      0.69     56954\n",
      "weighted avg       0.87      0.86      0.86     56954\n",
      "\n",
      "[18:04:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.71      0.48      7015\n",
      "         1.0       0.95      0.83      0.89     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.77      0.68     56954\n",
      "weighted avg       0.88      0.81      0.84     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.73      0.49      7015\n",
      "         1.0       0.96      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.78      0.69     56954\n",
      "weighted avg       0.88      0.81      0.84     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc611e7510>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.70      0.49      7015\n",
      "         1.0       0.95      0.83      0.89     49939\n",
      "\n",
      "    accuracy                           0.82     56954\n",
      "   macro avg       0.66      0.77      0.69     56954\n",
      "weighted avg       0.88      0.82      0.84     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.7946</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>0.8690</td>\n",
       "      <td>0.4704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.9418</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>0.8721</td>\n",
       "      <td>0.4732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.7363</td>\n",
       "      <td>0.9271</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.7428</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.7162</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.3918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.7795</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.7848</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>0.7634</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.4532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.9419</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>0.4862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8528</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.8971</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.9144</td>\n",
       "      <td>0.4737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8586</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>0.4941</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.4627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.9395</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.7072</td>\n",
       "      <td>0.7670</td>\n",
       "      <td>0.8853</td>\n",
       "      <td>0.4811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.8115</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.7330</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>0.4892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.8185</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.8344</td>\n",
       "      <td>0.7049</td>\n",
       "      <td>0.7697</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.4889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.7900         0.9420          0.1033      0.7946   \n",
       "LDA                        0.7942         0.9418          0.1029      0.8003   \n",
       "KNeighborsClassifier       0.7363         0.9271          0.1021      0.7428   \n",
       "ADABoostClassifier         0.7795         0.9388          0.1028      0.7848   \n",
       "GradientBoostClassifier    0.8092         0.9419          0.1020      0.8200   \n",
       "RandomForestClassifier     0.8528         0.9268          0.0974      0.8971   \n",
       "ExtraTreeClassifier        0.8586         0.9230          0.0975      0.9099   \n",
       "XGBoostClassifier          0.8121         0.9395          0.1010      0.8268   \n",
       "LightGBMClassifier         0.8115         0.9423          0.1019      0.8225   \n",
       "CatBoostClassifier         0.8185         0.9401          0.1008      0.8344   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.7572         0.7759        0.8690   \n",
       "LDA                           0.7505         0.7754        0.8721   \n",
       "KNeighborsClassifier          0.6897         0.7162        0.8316   \n",
       "ADABoostClassifier            0.7420         0.7634        0.8619   \n",
       "GradientBoostClassifier       0.7329         0.7764        0.8829   \n",
       "RandomForestClassifier        0.5378         0.7175        0.9144   \n",
       "ExtraTreeClassifier           0.4941         0.7020        0.9186   \n",
       "XGBoostClassifier             0.7072         0.7670        0.8853   \n",
       "LightGBMClassifier            0.7330         0.7778        0.8844   \n",
       "CatBoostClassifier            0.7049         0.7697        0.8896   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.4704  \n",
       "LDA                             0.4732  \n",
       "KNeighborsClassifier            0.3918  \n",
       "ADABoostClassifier              0.4532  \n",
       "GradientBoostClassifier         0.4862  \n",
       "RandomForestClassifier          0.4737  \n",
       "ExtraTreeClassifier             0.4627  \n",
       "XGBoostClassifier               0.4811  \n",
       "LightGBMClassifier              0.4892  \n",
       "CatBoostClassifier              0.4889  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_smotek, y_smotek)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "smotek_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "smotek_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smote+ENN - SmoteENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({0.0: 113555, 1.0: 76641})\n"
     ]
    }
   ],
   "source": [
    "# Smoteenn - Over + Under sampling\n",
    "smotenn = SMOTEENN(random_state=42)\n",
    "X_smotenn, y_smotenn = smotenn.fit_resample(X_train, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_smotenn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model classification report:  DummyClassifier(constant=1, strategy='constant')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      7015\n",
      "         1.0       0.88      1.00      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.44      0.50      0.47     56954\n",
      "weighted avg       0.77      0.88      0.82     56954\n",
      "\n",
      "Current model classification report:  LogisticRegression(solver='liblinear')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.84      0.42      7015\n",
      "         1.0       0.97      0.70      0.81     49939\n",
      "\n",
      "    accuracy                           0.72     56954\n",
      "   macro avg       0.63      0.77      0.62     56954\n",
      "weighted avg       0.88      0.72      0.76     56954\n",
      "\n",
      "Current model classification report:  LinearDiscriminantAnalysis()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.84      0.41      7015\n",
      "         1.0       0.97      0.68      0.80     49939\n",
      "\n",
      "    accuracy                           0.70     56954\n",
      "   macro avg       0.62      0.76      0.60     56954\n",
      "weighted avg       0.88      0.70      0.75     56954\n",
      "\n",
      "Current model classification report:  KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.82      0.38      7015\n",
      "         1.0       0.96      0.65      0.78     49939\n",
      "\n",
      "    accuracy                           0.67     56954\n",
      "   macro avg       0.61      0.74      0.58     56954\n",
      "weighted avg       0.88      0.67      0.73     56954\n",
      "\n",
      "Current model classification report:  AdaBoostClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.84      0.40      7015\n",
      "         1.0       0.97      0.67      0.79     49939\n",
      "\n",
      "    accuracy                           0.69     56954\n",
      "   macro avg       0.61      0.75      0.59     56954\n",
      "weighted avg       0.88      0.69      0.74     56954\n",
      "\n",
      "Current model classification report:  GradientBoostingClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.83      0.42      7015\n",
      "         1.0       0.97      0.71      0.82     49939\n",
      "\n",
      "    accuracy                           0.72     56954\n",
      "   macro avg       0.63      0.77      0.62     56954\n",
      "weighted avg       0.88      0.72      0.77     56954\n",
      "\n",
      "Current model classification report:  RandomForestClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.74      0.48      7015\n",
      "         1.0       0.96      0.81      0.88     49939\n",
      "\n",
      "    accuracy                           0.80     56954\n",
      "   macro avg       0.65      0.77      0.68     56954\n",
      "weighted avg       0.88      0.80      0.83     56954\n",
      "\n",
      "Current model classification report:  ExtraTreesClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.72      0.48      7015\n",
      "         1.0       0.95      0.82      0.88     49939\n",
      "\n",
      "    accuracy                           0.81     56954\n",
      "   macro avg       0.66      0.77      0.68     56954\n",
      "weighted avg       0.88      0.81      0.83     56954\n",
      "\n",
      "[18:45:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.81      0.45      7015\n",
      "         1.0       0.96      0.75      0.84     49939\n",
      "\n",
      "    accuracy                           0.76     56954\n",
      "   macro avg       0.64      0.78      0.65     56954\n",
      "weighted avg       0.88      0.76      0.79     56954\n",
      "\n",
      "Current model classification report:  LGBMClassifier(random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.83      0.44      7015\n",
      "         1.0       0.97      0.73      0.83     49939\n",
      "\n",
      "    accuracy                           0.74     56954\n",
      "   macro avg       0.63      0.78      0.64     56954\n",
      "weighted avg       0.89      0.74      0.78     56954\n",
      "\n",
      "Current model classification report:  <catboost.core.CatBoostClassifier object at 0x7fbc611c9850>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.81      0.45      7015\n",
      "         1.0       0.97      0.75      0.85     49939\n",
      "\n",
      "    accuracy                           0.76     56954\n",
      "   macro avg       0.64      0.78      0.65     56954\n",
      "weighted avg       0.89      0.76      0.80     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.7176</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>0.4226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.6986</td>\n",
       "      <td>0.9385</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.8389</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.6713</td>\n",
       "      <td>0.9329</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.7363</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.3814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoostClassifier</th>\n",
       "      <td>0.6881</td>\n",
       "      <td>0.9370</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.8379</td>\n",
       "      <td>0.7525</td>\n",
       "      <td>0.7895</td>\n",
       "      <td>0.3982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostClassifier</th>\n",
       "      <td>0.7213</td>\n",
       "      <td>0.9411</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.7055</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>0.7696</td>\n",
       "      <td>0.8162</td>\n",
       "      <td>0.4242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.8004</td>\n",
       "      <td>0.9413</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>0.4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.8219</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>0.4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoostClassifier</th>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.7483</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7768</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>0.4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBMClassifier</th>\n",
       "      <td>0.7397</td>\n",
       "      <td>0.9435</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.7794</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>0.4406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.7597</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.7823</td>\n",
       "      <td>0.8459</td>\n",
       "      <td>0.4543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "DummyClassifier            0.8768         0.8768          0.1232      1.0000   \n",
       "LogisticRegression         0.7176         0.9412          0.1084      0.7005   \n",
       "LDA                        0.6986         0.9385          0.1085      0.6788   \n",
       "KNeighborsClassifier       0.6713         0.9329          0.1079      0.6501   \n",
       "ADABoostClassifier         0.6881         0.9370          0.1086      0.6670   \n",
       "GradientBoostClassifier    0.7213         0.9411          0.1080      0.7055   \n",
       "RandomForestClassifier     0.8004         0.9413          0.1023      0.8092   \n",
       "ExtraTreeClassifier        0.8088         0.9400          0.1013      0.8219   \n",
       "XGBoostClassifier          0.7553         0.9426          0.1061      0.7483   \n",
       "LightGBMClassifier         0.7397         0.9435          0.1078      0.7268   \n",
       "CatBoostClassifier         0.7597         0.9440          0.1064      0.7523   \n",
       "\n",
       "                         Recall_Zero  ROC_AUC_Score  F1_Score_One  \\\n",
       "DummyClassifier               0.0000         0.5000        0.9344   \n",
       "LogisticRegression            0.8392         0.7698        0.8131   \n",
       "LDA                           0.8389         0.7589        0.7980   \n",
       "KNeighborsClassifier          0.8225         0.7363        0.7762   \n",
       "ADABoostClassifier            0.8379         0.7525        0.7895   \n",
       "GradientBoostClassifier       0.8336         0.7696        0.8162   \n",
       "RandomForestClassifier        0.7378         0.7735        0.8767   \n",
       "ExtraTreeClassifier           0.7156         0.7687        0.8829   \n",
       "XGBoostClassifier             0.8053         0.7768        0.8428   \n",
       "LightGBMClassifier            0.8321         0.7794        0.8304   \n",
       "CatBoostClassifier            0.8123         0.7823        0.8459   \n",
       "\n",
       "                         F1_Score_Zero  \n",
       "DummyClassifier                 0.0000  \n",
       "LogisticRegression              0.4226  \n",
       "LDA                             0.4067  \n",
       "KNeighborsClassifier            0.3814  \n",
       "ADABoostClassifier              0.3982  \n",
       "GradientBoostClassifier         0.4242  \n",
       "RandomForestClassifier          0.4766  \n",
       "ExtraTreeClassifier             0.4797  \n",
       "XGBoostClassifier               0.4477  \n",
       "LightGBMClassifier              0.4406  \n",
       "CatBoostClassifier              0.4543  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42)\n",
    "#For PCA transformed Data models\n",
    "model_names =[]\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "#svm = SVC(gamma='scale')\n",
    "knn = KNeighborsClassifier()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "models = [dummy,lr,lda,knn,ada,gb,rf,et,xgbc,lgbmc,catb]\n",
    "\n",
    "for model in models: \n",
    "    model.fit(X_smotenn, y_smotenn)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "    precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "    precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "    fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "    recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "    recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "    rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "    #print (f'model : {model} and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "    #print (f'model : {model} and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "    print(\"Current model classification report: \", model)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['DummyClassifier','LogisticRegression','LDA','KNeighborsClassifier','ADABoostClassifier','GradientBoostClassifier','RandomForestClassifier','ExtraTreeClassifier','XGBoostClassifier','LightGBMClassifier','CatBoostClassifier']\n",
    "smotenn_pca_transformed = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "smotenn_pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier with base models -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting - Only PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.24      0.33      7015\n",
      "         1.0       0.90      0.97      0.93     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.71      0.60      0.63     56954\n",
      "weighted avg       0.85      0.88      0.86     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-PCA</th>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.2371</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.3271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "Soft-Voting-PCA    0.8799         0.8998          0.1051      0.9702   \n",
       "\n",
       "                 Recall_Zero  ROC_AUC_Score  F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-PCA       0.2371         0.6036         0.934         0.3271  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model_pca = VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model_pca.fit(X_train,y_train)\n",
    "\n",
    "y_pred = voting_model_pca.predict(X_test)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['Soft-Voting-PCA']\n",
    "Voting_perf = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting - Random Undersampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:11:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.77      0.49      7015\n",
      "         1.0       0.96      0.80      0.88     49939\n",
      "\n",
      "    accuracy                           0.80     56954\n",
      "   macro avg       0.66      0.79      0.68     56954\n",
      "weighted avg       0.89      0.80      0.83     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-RandomUnderSampled</th>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.4858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Accuracy  Precision_One  Precision_Zero  \\\n",
       "Soft-Voting-RandomUnderSampled    0.7994         0.9447          0.1037   \n",
       "\n",
       "                                Recall_One  Recall_Zero  ROC_AUC_Score  \\\n",
       "Soft-Voting-RandomUnderSampled      0.8036       0.7695         0.7865   \n",
       "\n",
       "                                F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-RandomUnderSampled        0.8754         0.4858  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "y_pred = voting_model.predict(X_test)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['Soft-Voting-RandomUnderSampled']\n",
    "Voting_perf_ = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting - Random Under Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:14:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.77      0.48      7015\n",
      "         1.0       0.96      0.80      0.87     49939\n",
      "\n",
      "    accuracy                           0.80     56954\n",
      "   macro avg       0.66      0.79      0.68     56954\n",
      "weighted avg       0.89      0.80      0.83     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hard-Voting-RandomUnderSampled</th>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>0.7853</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.4836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Accuracy  Precision_One  Precision_Zero  \\\n",
       "Hard-Voting-RandomUnderSampled    0.7978         0.9444          0.1037   \n",
       "\n",
       "                                Recall_One  Recall_Zero  ROC_AUC_Score  \\\n",
       "Hard-Voting-RandomUnderSampled      0.8019       0.7686         0.7853   \n",
       "\n",
       "                                F1_Score_One  F1_Score_Zero  \n",
       "Hard-Voting-RandomUnderSampled        0.8743         0.4836  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model_h = VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='hard')\n",
    "\n",
    "voting_model_h.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "y_pred = voting_model_h.predict(X_test)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['Hard-Voting-RandomUnderSampled']\n",
    "Voting_perf__ = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting - SMOTEENN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.81      0.46      7015\n",
      "         1.0       0.97      0.76      0.85     49939\n",
      "\n",
      "    accuracy                           0.77     56954\n",
      "   macro avg       0.64      0.79      0.66     56954\n",
      "weighted avg       0.89      0.77      0.80     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-SmoteENN</th>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8499</td>\n",
       "      <td>0.4608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy  Precision_One  Precision_Zero  Recall_One  \\\n",
       "Soft-Voting-SmoteENN    0.7651         0.9451          0.1065      0.7582   \n",
       "\n",
       "                      Recall_Zero  ROC_AUC_Score  F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-SmoteENN       0.8148         0.7865        0.8499         0.4608  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model.fit(X_smotenn, y_smotenn)\n",
    "\n",
    "y_pred = voting_model.predict(X_test)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model_names = ['Soft-Voting-SmoteENN']\n",
    "Voting_perf_sen = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf_sen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curiosity hasn't killed the cat yet! \n",
    "\n",
    "### Let's see what the classifier would do when given the entire dataset - instead of the PCA reduced one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:40:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.29      0.38      7015\n",
      "         1.0       0.91      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.88     56954\n",
      "   macro avg       0.73      0.63      0.66     56954\n",
      "weighted avg       0.86      0.88      0.87     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-OriginalScaled</th>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.2901</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.3795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  Precision_One  Precision_Zero  \\\n",
       "Soft-Voting-OriginalScaled    0.8832         0.9055          0.1021   \n",
       "\n",
       "                            Recall_One  Recall_Zero  ROC_AUC_Score  \\\n",
       "Soft-Voting-OriginalScaled      0.9665       0.2901         0.6283   \n",
       "\n",
       "                            F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-OriginalScaled        0.9355         0.3795  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.fit_transform(X_test)\n",
    "\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model_none = VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model_none.fit(X_train_ss, y_train)\n",
    "\n",
    "y_pred = voting_model_none.predict(X_test_ss)\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "model_names = ['Soft-Voting-OriginalScaled']\n",
    "Voting_perf_oss = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf_oss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One other curious expedition - Original Scaled Data + RUS + Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({0.0: 16578, 1.0: 16578})\n"
     ]
    }
   ],
   "source": [
    "#Random Under Sampling\n",
    "rus_ = RandomUnderSampler(random_state=42)\n",
    "#rus.fit(X_pca, y)\n",
    "X_resampled_us_ss, y_resampled_us_ss = rus_.fit_resample(X_train_ss, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_resampled_us_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:58:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.86      0.54      7015\n",
      "         1.0       0.98      0.82      0.89     49939\n",
      "\n",
      "    accuracy                           0.82     56954\n",
      "   macro avg       0.69      0.84      0.71     56954\n",
      "weighted avg       0.90      0.82      0.85     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-OriginalScaled-RandomUnderSampled</th>\n",
       "      <td>0.821</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.5409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  Precision_One  \\\n",
       "Soft-Voting-OriginalScaled-RandomUnderSampled     0.821         0.9576   \n",
       "\n",
       "                                               Precision_Zero  Recall_One  \\\n",
       "Soft-Voting-OriginalScaled-RandomUnderSampled          0.1089      0.8161   \n",
       "\n",
       "                                               Recall_Zero  ROC_AUC_Score  \\\n",
       "Soft-Voting-OriginalScaled-RandomUnderSampled       0.8559          0.836   \n",
       "\n",
       "                                               F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-OriginalScaled-RandomUnderSampled        0.8889         0.5409  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model_us_ss= VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model_us_ss.fit(X_resampled_us_ss, y_resampled_us_ss)\n",
    "\n",
    "y_pred = voting_model_us_ss.predict(X_test_ss)\n",
    "\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "model_names = ['Soft-Voting-OriginalScaled-RandomUnderSampled']\n",
    "Voting_perf_ossrus = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf_ossrus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Scaled + SMOTEENN + Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({1.0: 116314, 0.0: 16578})\n",
      "Resample dataset shape Counter({0.0: 113693, 1.0: 76547})\n"
     ]
    }
   ],
   "source": [
    "# Smoteenn - Over + Under sampling\n",
    "smotenn = SMOTEENN(random_state=42)\n",
    "X_os_smotenn, y_os_smotenn = smotenn.fit_resample(X_train_ss, y_train)\n",
    "\n",
    "print('original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape', Counter(y_os_smotenn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:12:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Current model classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.12      1.00      0.22      7015\n",
      "         1.0       0.96      0.00      0.00     49939\n",
      "\n",
      "    accuracy                           0.12     56954\n",
      "   macro avg       0.54      0.50      0.11     56954\n",
      "weighted avg       0.86      0.12      0.03     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Soft-Voting-OriginalScaled-Smoteenn</th>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.2195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Accuracy  Precision_One  Precision_Zero  \\\n",
       "Soft-Voting-OriginalScaled-Smoteenn    0.1245          0.877          0.1231   \n",
       "\n",
       "                                     Recall_One  Recall_Zero  ROC_AUC_Score  \\\n",
       "Soft-Voting-OriginalScaled-Smoteenn      0.0016       0.9996         0.5006   \n",
       "\n",
       "                                     F1_Score_One  F1_Score_Zero  \n",
       "Soft-Voting-OriginalScaled-Smoteenn        0.0032         0.2195  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy =[]\n",
    "precision_one = []\n",
    "precision_zero = []\n",
    "recall_one = []\n",
    "recall_zero = []\n",
    "fscore_one = []\n",
    "fscore_zero = []\n",
    "rocaucscore = []\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lda= LinearDiscriminantAnalysis()\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "et=  ExtraTreesClassifier(random_state=0)\n",
    "xgbc = XGBClassifier(random_state=0)\n",
    "lgbmc = LGBMClassifier(random_state=0)\n",
    "catb = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "voting_model_se_ss= VotingClassifier(estimators=[('lr', lr), ('lda', lda), ('ada', ada),('gb',gb),('rf',rf),('et', et),('xgbc',xgbc),('lgbmc',lgbmc),('catb',catb)], voting='soft')\n",
    "\n",
    "voting_model_se_ss.fit(X_os_smotenn, y_os_smotenn)\n",
    "\n",
    "y_pred = voting_model_se_ss.predict(X_test_ss)\n",
    "\n",
    "accuracy.append(round(accuracy_score(y_test, y_pred),4))\n",
    "precision_one.append(round(average_precision_score(y_test, y_pred),4))\n",
    "precision_zero.append(round(average_precision_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "fscore_one.append(round(f1_score(y_test, y_pred),4))\n",
    "fscore_zero.append(round(f1_score(y_test, y_pred,pos_label= 0),4)) #minority class - female\n",
    "recall_one.append(round(recall_score(y_test, y_pred),4))\n",
    "recall_zero.append(round(recall_score(y_test, y_pred,pos_label=0),4))\n",
    "rocaucscore.append(round(roc_auc_score(y_test, y_pred),4))\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "model_names = ['Soft-Voting-OriginalScaled-Smoteenn']\n",
    "Voting_perf_ossmoe = pd.DataFrame({'Accuracy':accuracy,'Precision_One':precision_one,'Precision_Zero':precision_zero,'Recall_One':recall_one,'Recall_Zero':recall_zero,'ROC_AUC_Score':rocaucscore,'F1_Score_One':fscore_one,'F1_Score_Zero':fscore_zero}, index=model_names)\n",
    "Voting_perf_ossmoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all Voting Classifier outputs into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.2371</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.3271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.4858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>0.7853</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8499</td>\n",
       "      <td>0.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.2901</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.3795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.5409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.2195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision_One  Precision_Zero  Recall_One  Recall_Zero  \\\n",
       "0    0.8799         0.8998          0.1051      0.9702       0.2371   \n",
       "1    0.7994         0.9447          0.1037      0.8036       0.7695   \n",
       "2    0.7978         0.9444          0.1037      0.8019       0.7686   \n",
       "3    0.7651         0.9451          0.1065      0.7582       0.8148   \n",
       "4    0.8832         0.9055          0.1021      0.9665       0.2901   \n",
       "5    0.8210         0.9576          0.1089      0.8161       0.8559   \n",
       "6    0.1245         0.8770          0.1231      0.0016       0.9996   \n",
       "\n",
       "   ROC_AUC_Score  F1_Score_One  F1_Score_Zero  \n",
       "0         0.6036        0.9340         0.3271  \n",
       "1         0.7865        0.8754         0.4858  \n",
       "2         0.7853        0.8743         0.4836  \n",
       "3         0.7865        0.8499         0.4608  \n",
       "4         0.6283        0.9355         0.3795  \n",
       "5         0.8360        0.8889         0.5409  \n",
       "6         0.5006        0.0032         0.2195  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting_perf - Soft PCA\n",
    "# Voting_perf_ - Soft PCA RUS\n",
    "# Voting_perf__ - Hard PCA RUS\n",
    "# Voting_perf_sen - Soft PCA Smoteenn\n",
    "# Voting_perf_oss - Soft original scaled\n",
    "# Voting_perf_ossrus - Soft original scaled rus\n",
    "# Voting_perf_ossmoe - Soft original smoteenn\n",
    "\n",
    "Voting_perf = Voting_perf.append(Voting_perf_, ignore_index = True)\n",
    "Voting_perf = Voting_perf.append(Voting_perf__ , ignore_index = True)\n",
    "Voting_perf = Voting_perf.append(Voting_perf_sen , ignore_index = True)\n",
    "Voting_perf = Voting_perf.append(Voting_perf_oss , ignore_index = True)\n",
    "Voting_perf = Voting_perf.append(Voting_perf_ossrus , ignore_index = True)\n",
    "Voting_perf = Voting_perf.append(Voting_perf_ossmoe , ignore_index = True)\n",
    "\n",
    "Voting_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_perf = Voting_perf.drop(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_One</th>\n",
       "      <th>Precision_Zero</th>\n",
       "      <th>Recall_One</th>\n",
       "      <th>Recall_Zero</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>F1_Score_One</th>\n",
       "      <th>F1_Score_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SoftVoting-PCA</th>\n",
       "      <td>0.8799</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.2371</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.3271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SoftVoting-PCA-RandomUnderSampled</th>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.4858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HardVoting-PCA-RandomUnderSampled</th>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>0.7853</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SoftVoting-PCA-SMOTEENN</th>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.8499</td>\n",
       "      <td>0.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SoftVoting-OriginalScaled</th>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.2901</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.3795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SoftVoting-OriginalScaled-RandomUnderSampled</th>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.5409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Accuracy  Precision_One  \\\n",
       "SoftVoting-PCA                                  0.8799         0.8998   \n",
       "SoftVoting-PCA-RandomUnderSampled               0.7994         0.9447   \n",
       "HardVoting-PCA-RandomUnderSampled               0.7978         0.9444   \n",
       "SoftVoting-PCA-SMOTEENN                         0.7651         0.9451   \n",
       "SoftVoting-OriginalScaled                       0.8832         0.9055   \n",
       "SoftVoting-OriginalScaled-RandomUnderSampled    0.8210         0.9576   \n",
       "\n",
       "                                              Precision_Zero  Recall_One  \\\n",
       "SoftVoting-PCA                                        0.1051      0.9702   \n",
       "SoftVoting-PCA-RandomUnderSampled                     0.1037      0.8036   \n",
       "HardVoting-PCA-RandomUnderSampled                     0.1037      0.8019   \n",
       "SoftVoting-PCA-SMOTEENN                               0.1065      0.7582   \n",
       "SoftVoting-OriginalScaled                             0.1021      0.9665   \n",
       "SoftVoting-OriginalScaled-RandomUnderSampled          0.1089      0.8161   \n",
       "\n",
       "                                              Recall_Zero  ROC_AUC_Score  \\\n",
       "SoftVoting-PCA                                     0.2371         0.6036   \n",
       "SoftVoting-PCA-RandomUnderSampled                  0.7695         0.7865   \n",
       "HardVoting-PCA-RandomUnderSampled                  0.7686         0.7853   \n",
       "SoftVoting-PCA-SMOTEENN                            0.8148         0.7865   \n",
       "SoftVoting-OriginalScaled                          0.2901         0.6283   \n",
       "SoftVoting-OriginalScaled-RandomUnderSampled       0.8559         0.8360   \n",
       "\n",
       "                                              F1_Score_One  F1_Score_Zero  \n",
       "SoftVoting-PCA                                      0.9340         0.3271  \n",
       "SoftVoting-PCA-RandomUnderSampled                   0.8754         0.4858  \n",
       "HardVoting-PCA-RandomUnderSampled                   0.8743         0.4836  \n",
       "SoftVoting-PCA-SMOTEENN                             0.8499         0.4608  \n",
       "SoftVoting-OriginalScaled                           0.9355         0.3795  \n",
       "SoftVoting-OriginalScaled-RandomUnderSampled        0.8889         0.5409  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = ['SoftVoting-PCA','SoftVoting-PCA-RandomUnderSampled','HardVoting-PCA-RandomUnderSampled','SoftVoting-PCA-SMOTEENN','SoftVoting-OriginalScaled','SoftVoting-OriginalScaled-RandomUnderSampled']\n",
    "Voting_perf.index= (idx)\n",
    "Voting_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Performance So Far!\n",
    "### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Submission - File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_days  brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0       1.0  370           370.0    -1.0           1.0              0.0   \n",
       "1       1.0    1             1.0    -1.0           7.0              0.0   \n",
       "\n",
       "   skewness_male  daypart_home  daypart_other  daypart_work  ...  SOCIAL  \\\n",
       "0       0.948649           1.0            0.0           0.0  ...     1.0   \n",
       "1       0.000000           0.0            0.0           1.0  ...     0.0   \n",
       "\n",
       "   SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  IOS  \\\n",
       "0     0.0  0.00000               0.0        0.00000      0.0        1    0   \n",
       "1     0.0  0.04011               0.0        0.02931      0.0        0    1   \n",
       "\n",
       "   SMART PHONE  TABLET  \n",
       "0            1       0  \n",
       "1            0       0  \n",
       "\n",
       "[2 rows x 110 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49737, 110)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf = test_merged.isnull().sum()\n",
    "xdf[xdf > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = test_merged['gender']\n",
    "test_X = test_merged.drop('gender', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>cellular</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "      <td>49737.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>108.355269</td>\n",
       "      <td>3619.267849</td>\n",
       "      <td>29.879145</td>\n",
       "      <td>10.214026</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.311119</td>\n",
       "      <td>0.531156</td>\n",
       "      <td>0.115047</td>\n",
       "      <td>0.353797</td>\n",
       "      <td>0.814192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035410</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.117120</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.169441</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.980377</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.982046</td>\n",
       "      <td>0.007278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>118.827226</td>\n",
       "      <td>12072.591033</td>\n",
       "      <td>88.833113</td>\n",
       "      <td>10.279043</td>\n",
       "      <td>0.035296</td>\n",
       "      <td>0.337824</td>\n",
       "      <td>0.216573</td>\n",
       "      <td>0.121074</td>\n",
       "      <td>0.203920</td>\n",
       "      <td>0.290029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148117</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>0.182367</td>\n",
       "      <td>0.015801</td>\n",
       "      <td>0.270248</td>\n",
       "      <td>0.015573</td>\n",
       "      <td>0.138703</td>\n",
       "      <td>0.128479</td>\n",
       "      <td>0.132787</td>\n",
       "      <td>0.085003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>7.809520</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.401340</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.218440</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>69.000000</td>\n",
       "      <td>1026.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157263</td>\n",
       "      <td>0.530790</td>\n",
       "      <td>0.090910</td>\n",
       "      <td>0.337880</td>\n",
       "      <td>0.981920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>149.000000</td>\n",
       "      <td>3054.000000</td>\n",
       "      <td>29.938600</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602701</td>\n",
       "      <td>0.666670</td>\n",
       "      <td>0.153680</td>\n",
       "      <td>0.463920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1181.000000</td>\n",
       "      <td>622850.000000</td>\n",
       "      <td>8708.700000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>0.958400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           num_days            brq  brq_engagement  distinct_app  \\\n",
       "count  49737.000000   49737.000000    49737.000000  49737.000000   \n",
       "mean     108.355269    3619.267849       29.879145     10.214026   \n",
       "std      118.827226   12072.591033       88.833113     10.279043   \n",
       "min        1.000000       1.000000        1.000000      1.000000   \n",
       "25%       25.000000     296.000000        7.809520      4.000000   \n",
       "50%       69.000000    1026.000000       15.000000      7.000000   \n",
       "75%      149.000000    3054.000000       29.938600     13.000000   \n",
       "max     1181.000000  622850.000000     8708.700000    316.000000   \n",
       "\n",
       "       skewness_female  skewness_male  daypart_home  daypart_other  \\\n",
       "count     49737.000000   49737.000000  49737.000000   49737.000000   \n",
       "mean          0.002058       0.311119      0.531156       0.115047   \n",
       "std           0.035296       0.337824      0.216573       0.121074   \n",
       "min           0.000000       0.000000      0.000000       0.000000   \n",
       "25%           0.000000       0.005957      0.401340       0.035400   \n",
       "50%           0.000000       0.157263      0.530790       0.090910   \n",
       "75%           0.000000       0.602701      0.666670       0.153680   \n",
       "max           0.999292       1.000000      1.000000       1.000000   \n",
       "\n",
       "       daypart_work      cellular  ...        SOCIAL        SPORTS  \\\n",
       "count  49737.000000  49737.000000  ...  49737.000000  49737.000000   \n",
       "mean       0.353797      0.814192  ...      0.035410      0.003049   \n",
       "std        0.203920      0.290029  ...      0.148117      0.040475   \n",
       "min        0.000000      0.000000  ...      0.000000      0.000000   \n",
       "25%        0.218440      0.736000  ...      0.000000      0.000000   \n",
       "50%        0.337880      0.981920  ...      0.000000      0.000000   \n",
       "75%        0.463920      1.000000  ...      0.000000      0.000000   \n",
       "max        1.000000      1.000000  ...      1.000010      1.000000   \n",
       "\n",
       "              TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS       WEATHER  \\\n",
       "count  49737.000000      49737.000000   49737.000000  49737.000000   \n",
       "mean       0.117120          0.000762       0.169441      0.000884   \n",
       "std        0.182367          0.015801       0.270248      0.015573   \n",
       "min        0.000000          0.000000       0.000000      0.000000   \n",
       "25%        0.000000          0.000000       0.000000      0.000000   \n",
       "50%        0.040110          0.000000       0.029310      0.000000   \n",
       "75%        0.146180          0.000000       0.219320      0.000000   \n",
       "max        1.000000          0.961700       1.000010      0.958400   \n",
       "\n",
       "            ANDROID           IOS   SMART PHONE        TABLET  \n",
       "count  49737.000000  49737.000000  49737.000000  49737.000000  \n",
       "mean       0.980377      0.016788      0.982046      0.007278  \n",
       "std        0.138703      0.128479      0.132787      0.085003  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        1.000000      0.000000      1.000000      0.000000  \n",
       "50%        1.000000      0.000000      1.000000      0.000000  \n",
       "75%        1.000000      0.000000      1.000000      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 109 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "      <td>4.973700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.742912e-17</td>\n",
       "      <td>9.143039e-18</td>\n",
       "      <td>-3.285780e-17</td>\n",
       "      <td>-2.357190e-17</td>\n",
       "      <td>1.057164e-17</td>\n",
       "      <td>-4.342944e-17</td>\n",
       "      <td>-2.475764e-16</td>\n",
       "      <td>-1.097165e-16</td>\n",
       "      <td>-1.642890e-16</td>\n",
       "      <td>-3.085776e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.714439e-17</td>\n",
       "      <td>9.857339e-18</td>\n",
       "      <td>8.914463e-17</td>\n",
       "      <td>-3.316138e-17</td>\n",
       "      <td>2.171472e-17</td>\n",
       "      <td>1.250025e-17</td>\n",
       "      <td>3.012917e-16</td>\n",
       "      <td>-1.585746e-17</td>\n",
       "      <td>1.348598e-16</td>\n",
       "      <td>4.285800e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.034659e-01</td>\n",
       "      <td>-2.997123e-01</td>\n",
       "      <td>-3.250976e-01</td>\n",
       "      <td>-8.963984e-01</td>\n",
       "      <td>-5.829958e-02</td>\n",
       "      <td>-9.209600e-01</td>\n",
       "      <td>-2.452578e+00</td>\n",
       "      <td>-9.502379e-01</td>\n",
       "      <td>-1.734992e+00</td>\n",
       "      <td>-2.807307e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.390681e-01</td>\n",
       "      <td>-7.534249e-02</td>\n",
       "      <td>-6.422310e-01</td>\n",
       "      <td>-4.822647e-02</td>\n",
       "      <td>-6.269909e-01</td>\n",
       "      <td>-5.674468e-02</td>\n",
       "      <td>-7.068242e+00</td>\n",
       "      <td>-1.306712e-01</td>\n",
       "      <td>-7.395710e+00</td>\n",
       "      <td>-8.562503e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.014900e-01</td>\n",
       "      <td>-2.752766e-01</td>\n",
       "      <td>-2.484417e-01</td>\n",
       "      <td>-6.045396e-01</td>\n",
       "      <td>-5.829958e-02</td>\n",
       "      <td>-9.033258e-01</td>\n",
       "      <td>-5.994161e-01</td>\n",
       "      <td>-6.578507e-01</td>\n",
       "      <td>-6.637790e-01</td>\n",
       "      <td>-2.696031e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.390681e-01</td>\n",
       "      <td>-7.534249e-02</td>\n",
       "      <td>-6.422310e-01</td>\n",
       "      <td>-4.822647e-02</td>\n",
       "      <td>-6.269909e-01</td>\n",
       "      <td>-5.674468e-02</td>\n",
       "      <td>1.414779e-01</td>\n",
       "      <td>-1.306712e-01</td>\n",
       "      <td>1.352135e-01</td>\n",
       "      <td>-8.562503e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.312007e-01</td>\n",
       "      <td>-2.148084e-01</td>\n",
       "      <td>-1.674972e-01</td>\n",
       "      <td>-3.126807e-01</td>\n",
       "      <td>-5.829958e-02</td>\n",
       "      <td>-4.554379e-01</td>\n",
       "      <td>-1.689144e-03</td>\n",
       "      <td>-1.993643e-01</td>\n",
       "      <td>-7.805450e-02</td>\n",
       "      <td>5.783209e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.390681e-01</td>\n",
       "      <td>-7.534249e-02</td>\n",
       "      <td>-4.222874e-01</td>\n",
       "      <td>-4.822647e-02</td>\n",
       "      <td>-5.185337e-01</td>\n",
       "      <td>-5.674468e-02</td>\n",
       "      <td>1.414779e-01</td>\n",
       "      <td>-1.306712e-01</td>\n",
       "      <td>1.352135e-01</td>\n",
       "      <td>-8.562503e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.420524e-01</td>\n",
       "      <td>-4.682288e-02</td>\n",
       "      <td>6.692977e-04</td>\n",
       "      <td>2.710371e-01</td>\n",
       "      <td>-5.829958e-02</td>\n",
       "      <td>8.631260e-01</td>\n",
       "      <td>6.257279e-01</td>\n",
       "      <td>3.190862e-01</td>\n",
       "      <td>5.400359e-01</td>\n",
       "      <td>6.406602e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.390681e-01</td>\n",
       "      <td>-7.534249e-02</td>\n",
       "      <td>1.593488e-01</td>\n",
       "      <td>-4.822647e-02</td>\n",
       "      <td>1.845694e-01</td>\n",
       "      <td>-5.674468e-02</td>\n",
       "      <td>1.414779e-01</td>\n",
       "      <td>-1.306712e-01</td>\n",
       "      <td>1.352135e-01</td>\n",
       "      <td>-8.562503e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.027018e+00</td>\n",
       "      <td>5.129280e+01</td>\n",
       "      <td>9.769902e+01</td>\n",
       "      <td>2.974878e+01</td>\n",
       "      <td>2.825377e+01</td>\n",
       "      <td>2.039192e+00</td>\n",
       "      <td>2.164858e+00</td>\n",
       "      <td>7.309289e+00</td>\n",
       "      <td>3.168931e+00</td>\n",
       "      <td>6.406602e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.512469e+00</td>\n",
       "      <td>2.463122e+01</td>\n",
       "      <td>4.841281e+00</td>\n",
       "      <td>6.081477e+01</td>\n",
       "      <td>3.073394e+00</td>\n",
       "      <td>6.148594e+01</td>\n",
       "      <td>1.414779e-01</td>\n",
       "      <td>7.652795e+00</td>\n",
       "      <td>1.352135e-01</td>\n",
       "      <td>1.167883e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04   \n",
       "mean   2.742912e-17  9.143039e-18 -3.285780e-17 -2.357190e-17  1.057164e-17   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -9.034659e-01 -2.997123e-01 -3.250976e-01 -8.963984e-01 -5.829958e-02   \n",
       "25%   -7.014900e-01 -2.752766e-01 -2.484417e-01 -6.045396e-01 -5.829958e-02   \n",
       "50%   -3.312007e-01 -2.148084e-01 -1.674972e-01 -3.126807e-01 -5.829958e-02   \n",
       "75%    3.420524e-01 -4.682288e-02  6.692977e-04  2.710371e-01 -5.829958e-02   \n",
       "max    9.027018e+00  5.129280e+01  9.769902e+01  2.974878e+01  2.825377e+01   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04   \n",
       "mean  -4.342944e-17 -2.475764e-16 -1.097165e-16 -1.642890e-16 -3.085776e-17   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -9.209600e-01 -2.452578e+00 -9.502379e-01 -1.734992e+00 -2.807307e+00   \n",
       "25%   -9.033258e-01 -5.994161e-01 -6.578507e-01 -6.637790e-01 -2.696031e-01   \n",
       "50%   -4.554379e-01 -1.689144e-03 -1.993643e-01 -7.805450e-02  5.783209e-01   \n",
       "75%    8.631260e-01  6.257279e-01  3.190862e-01  5.400359e-01  6.406602e-01   \n",
       "max    2.039192e+00  2.164858e+00  7.309289e+00  3.168931e+00  6.406602e-01   \n",
       "\n",
       "       ...           99            100           101           102  \\\n",
       "count  ...  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04   \n",
       "mean   ...  7.714439e-17  9.857339e-18  8.914463e-17 -3.316138e-17   \n",
       "std    ...  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min    ... -2.390681e-01 -7.534249e-02 -6.422310e-01 -4.822647e-02   \n",
       "25%    ... -2.390681e-01 -7.534249e-02 -6.422310e-01 -4.822647e-02   \n",
       "50%    ... -2.390681e-01 -7.534249e-02 -4.222874e-01 -4.822647e-02   \n",
       "75%    ... -2.390681e-01 -7.534249e-02  1.593488e-01 -4.822647e-02   \n",
       "max    ...  6.512469e+00  2.463122e+01  4.841281e+00  6.081477e+01   \n",
       "\n",
       "                103           104           105           106           107  \\\n",
       "count  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04  4.973700e+04   \n",
       "mean   2.171472e-17  1.250025e-17  3.012917e-16 -1.585746e-17  1.348598e-16   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -6.269909e-01 -5.674468e-02 -7.068242e+00 -1.306712e-01 -7.395710e+00   \n",
       "25%   -6.269909e-01 -5.674468e-02  1.414779e-01 -1.306712e-01  1.352135e-01   \n",
       "50%   -5.185337e-01 -5.674468e-02  1.414779e-01 -1.306712e-01  1.352135e-01   \n",
       "75%    1.845694e-01 -5.674468e-02  1.414779e-01 -1.306712e-01  1.352135e-01   \n",
       "max    3.073394e+00  6.148594e+01  1.414779e-01  7.652795e+00  1.352135e-01   \n",
       "\n",
       "                108  \n",
       "count  4.973700e+04  \n",
       "mean   4.285800e-17  \n",
       "std    1.000010e+00  \n",
       "min   -8.562503e-02  \n",
       "25%   -8.562503e-02  \n",
       "50%   -8.562503e-02  \n",
       "75%   -8.562503e-02  \n",
       "max    1.167883e+01  \n",
       "\n",
       "[8 rows x 109 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale my test_X\n",
    "test_X_scaled = StandardScaler().fit_transform(test_X)\n",
    "pd.DataFrame(test_X_scaled).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(test_X_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = voting_model_us_ss.predict(test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49737,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predicted_output.shape)\n",
    "predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49737, 43)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e932096849088aa76a953ce2f69728db6b6cdc056cd91...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49732</th>\n",
       "      <td>d10c52e7c47bad5eb0599766a21885f34ff14aa43970ec...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49733</th>\n",
       "      <td>d28fa808dbfed21864772dad6039592944724432817d60...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49734</th>\n",
       "      <td>9755e3ed26f9104828f2e088cd03bf2c50cc51510b2529...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49735</th>\n",
       "      <td>f77195b5ea5877ce5d8caf78e207b85056ba6d9b684515...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49736</th>\n",
       "      <td>75923ed4a4e8d94e682cc76d14607f632b0603073ae62d...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49737 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ifa  Predicted_Gender\n",
       "0      8e932096849088aa76a953ce2f69728db6b6cdc056cd91...               1.0\n",
       "1      9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...               1.0\n",
       "2      17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...               1.0\n",
       "3      c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...               1.0\n",
       "4      9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...               1.0\n",
       "...                                                  ...               ...\n",
       "49732  d10c52e7c47bad5eb0599766a21885f34ff14aa43970ec...               1.0\n",
       "49733  d28fa808dbfed21864772dad6039592944724432817d60...               1.0\n",
       "49734  9755e3ed26f9104828f2e088cd03bf2c50cc51510b2529...               0.0\n",
       "49735  f77195b5ea5877ce5d8caf78e207b85056ba6d9b684515...               0.0\n",
       "49736  75923ed4a4e8d94e682cc76d14607f632b0603073ae62d...               0.0\n",
       "\n",
       "[49737 rows x 2 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdout = pd.DataFrame({'ifa': test_ifa_vals,'Predicted_Gender':predicted_output})\n",
    "# pdout.rename(columns = {0:'Target'},inplace = True)\n",
    "pdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdout.to_csv('submission_softvoting_original_scaled.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try CV on our promising model choices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input choices\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "# X_train_ss = ss.fit_transform(X_train)\n",
    "# X_test_ss = ss.fit_transform(X_test)\n",
    "# X_resampled_us_ss, y_resampled_us_ss = rus_.fit_resample(X_train_ss, y_train) - RandomUnderSampled-Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 116314, number of negative: 16578\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41809\n",
      "[LightGBM] [Info] Number of data points in the train set: 132892, number of used features: 100\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.875252 -> initscore=1.948217\n",
      "[LightGBM] [Info] Start training from score 1.948217\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[100]\tvalid_0's auc: 0.899257\n",
      "[200]\tvalid_0's auc: 0.900613\n",
      "[300]\tvalid_0's auc: 0.901599\n",
      "[400]\tvalid_0's auc: 0.90253\n",
      "[500]\tvalid_0's auc: 0.903124\n",
      "[600]\tvalid_0's auc: 0.903522\n",
      "[700]\tvalid_0's auc: 0.903658\n",
      "[800]\tvalid_0's auc: 0.903689\n",
      "[900]\tvalid_0's auc: 0.903829\n",
      "[1000]\tvalid_0's auc: 0.903947\n",
      "[1100]\tvalid_0's auc: 0.904074\n",
      "[1200]\tvalid_0's auc: 0.904113\n",
      "[1300]\tvalid_0's auc: 0.904193\n",
      "[1400]\tvalid_0's auc: 0.904232\n",
      "[1500]\tvalid_0's auc: 0.904267\n",
      "[1600]\tvalid_0's auc: 0.904246\n",
      "[1700]\tvalid_0's auc: 0.904277\n",
      "[1800]\tvalid_0's auc: 0.904261\n",
      "[1900]\tvalid_0's auc: 0.904224\n",
      "Early stopping, best iteration is:\n",
      "[1735]\tvalid_0's auc: 0.904287\n"
     ]
    }
   ],
   "source": [
    "#LGB\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label = y_train)\n",
    "validation_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "evals_result = {}\n",
    "bst = lgb.train({\n",
    "    'boosting': 'gbdt', #'dart', # Dropouts meet Multiple Additive Regression Trees, default='gbdt'\n",
    "    'learning_rate': 0.01, # smaller increases accuracy, default=0.1\n",
    "    'max_bin': 511, # larger increases accuracy, default=255\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 60, # larger increases accuracy, default=31\n",
    "    'num_trees': 3000,\n",
    "    'num_iteration': 600, # default=100\n",
    "    'objective': 'binary',\n",
    "    },\n",
    "    train_data,\n",
    "    num_boost_round=500, # may be redundant with params#num_iteration\n",
    "    valid_sets=[validation_data],\n",
    "    early_stopping_rounds=250,\n",
    "    evals_result= evals_result,\n",
    "    verbose_eval=100, # logs every 100 trees\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM - Model Performances \n",
    "# X_train_ss - Scaled Original Input - [100]\tvalid_0's auc: 0.895576\n",
    "# X_resampled_us_ss - [100]\tvalid_0's auc: 0.893168\n",
    "# X_train - Original as is - [100]\tvalid_0's auc: 0.89935\n",
    "\n",
    "# 3000 trees 60 leaves 500 booster\n",
    "#Early stopping, best iteration is:\n",
    "#[1735]\tvalid_0's auc: 0.904287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = bst.predict(X_test, num_iteration= bst.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99635774, 0.98685247, 0.74159135, 0.60834773, 0.60833654,\n",
       "       0.99299134, 0.99446803, 0.96280425, 0.90839676, 0.99590389,\n",
       "       0.94869562, 0.99384073, 0.99761119, 0.99899433, 0.9848509 ,\n",
       "       0.57359152, 0.9987835 , 0.98437677, 0.73187407, 0.99719917])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lgb[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56954,)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test  Accuracy Score : 0.89\n",
      "Train Accuracy Score : 0.92\n"
     ]
    }
   ],
   "source": [
    "test_preds = bst.predict(X_test)\n",
    "train_preds = bst.predict(X_train)\n",
    "\n",
    "test_preds = [1 if pred > 0.5 else 0 for pred in test_preds]\n",
    "train_preds = [1 if pred > 0.5 else 0 for pred in train_preds]\n",
    "\n",
    "print(\"\\nTest  Accuracy Score : %.2f\"%accuracy_score(y_test, test_preds))\n",
    "print(\"Train Accuracy Score : %.2f\"%accuracy_score(y_train, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.32      0.41      7015\n",
      "         1.0       0.91      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.64      0.67     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6427"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "round(roc_auc_score(y_test, test_preds),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042868864519347"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_lgb)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99635774, 0.98685247, 0.74159135, ..., 0.90375854, 0.63345098,\n",
       "       0.98352079])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426723981732411"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred_lgb\n",
    "y_pred=y_pred.round(0)\n",
    "#converting from float to integer\n",
    "#y_pred=y_pred.astype(int)\n",
    "#roc_auc_score metric\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.32      0.41      7015\n",
      "         1.0       0.91      0.97      0.94     49939\n",
      "\n",
      "    accuracy                           0.89     56954\n",
      "   macro avg       0.74      0.64      0.67     56954\n",
      "weighted avg       0.87      0.89      0.87     56954\n",
      "\n",
      "0.9043\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(round(roc_auc_score(y_test, y_pred_lgb),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(accuracy_score(y_test, y_pred),4)\n",
    "round(average_precision_score(y_test, y_pred),4)\n",
    "round(average_precision_score(y_test, y_pred,pos_label= 0),4) #minority class - female\n",
    "round(f1_score(y_test, y_pred),4)\n",
    "round(f1_score(y_test, y_pred,pos_label= 0),4) #minority class - female\n",
    "round(recall_score(y_test, y_pred),4)\n",
    "round(recall_score(y_test, y_pred,pos_label=0),4)\n",
    "round(roc_auc_score(y_test, y_pred),4)\n",
    "print(\"Current model classification report: \")\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('auc', 0.9042868864519347)])})\n"
     ]
    }
   ],
   "source": [
    "print(bst.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lgbm model - Top 20 Features: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>feature_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewness_male</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dev_cat_iab19_29</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>GAME_CASUAL</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>VIDEO_PLAYERS</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dev_cat_iab9_23</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_days</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dev_cat_iab1</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dev_cat_utilities</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dev_cat_iab9_30</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dev_cat_iab1_6</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>dev_cat_games</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>DATING</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dev_cat_iab3</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>SOCIAL</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dev_cat_iab9</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brq</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6014</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PHOTOGRAPHY</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>TOOLS</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>GAME_SPORTS</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               columns  feature_imp\n",
       "5        skewness_male         1655\n",
       "21    dev_cat_iab19_29         1416\n",
       "75         GAME_CASUAL         1406\n",
       "103      VIDEO_PLAYERS         1214\n",
       "27     dev_cat_iab9_23         1153\n",
       "0             num_days         1054\n",
       "11        dev_cat_iab1          998\n",
       "36   dev_cat_utilities          841\n",
       "28     dev_cat_iab9_30          828\n",
       "13      dev_cat_iab1_6          814\n",
       "31       dev_cat_games          780\n",
       "63              DATING          761\n",
       "23        dev_cat_iab3          742\n",
       "99              SOCIAL          735\n",
       "26        dev_cat_iab9          729\n",
       "1                  brq          719\n",
       "50                6014          675\n",
       "96         PHOTOGRAPHY          590\n",
       "101              TOOLS          556\n",
       "82         GAME_SPORTS          547"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Lgbm model - Top 20 Features: \")\n",
    "pd.DataFrame({'columns' : X_train.columns ,'feature_imp' :bst.feature_importance()}).sort_values(by = 'feature_imp',ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "lgb.plot_importance(bst, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')\n",
    "test_x = test.iloc[:, 1:].values # Drop the ID_code\n",
    "ypred = bst.predict(test_x)\n",
    "test_code = test.iloc[:, 0]\n",
    "submission = pd.concat([test_code, pd.Series(ypred, name='target')], axis=1)\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_auc</th>\n",
       "      <th>PR_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.886803</td>\n",
       "      <td>0.909915</td>\n",
       "      <td>0.966599</td>\n",
       "      <td>0.904287</td>\n",
       "      <td>0.985097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall   ROC_auc    PR_auc\n",
       "0  0.886803   0.909915  0.966599  0.904287  0.985097"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_old(model, testing_set_x, testing_set_y):\n",
    "    predictions = model.predict(testing_set_x)\n",
    "    \n",
    "    accuracy  = accuracy_score(testing_set_y, predictions[:] >= 0.5)\n",
    "    roc_auc   = roc_auc_score(testing_set_y, predictions[:])\n",
    "    precision = precision_score(testing_set_y, predictions[:] >= 0.5)\n",
    "    recall    = recall_score(testing_set_y, predictions[:] >= 0.5)\n",
    "    pr_auc    = average_precision_score(testing_set_y, predictions[:])\n",
    "    \n",
    "    result = pd.DataFrame([[accuracy, precision, recall, roc_auc, pr_auc]], columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    return(result)\n",
    "out_old = evaluate_old(bst,X_test,y_test)\n",
    "out_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_auc</th>\n",
       "      <th>PR_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.886803</td>\n",
       "      <td>0.909915</td>\n",
       "      <td>0.966599</td>\n",
       "      <td>0.904287</td>\n",
       "      <td>0.985097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall   ROC_auc    PR_auc\n",
       "0  0.886803   0.909915  0.966599  0.904287  0.985097"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "results = results.append(out_old)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "[ Experiment Results ]\n",
      "Accuracy:   0.8868033851880465\n",
      "Precision:  0.9099151743638078\n",
      "Recall:     0.9665992510863253\n",
      "ROC Auc:    0.9042868864519347\n",
      "PR Auc:     0.9850967709983607\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "value=%{x}<br>metric=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "lightpink"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "type": "box",
         "x": [
          0.8868033851880465,
          0.9099151743638078,
          0.9665992510863253,
          0.9042868864519347,
          0.9850967709983607
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "Accuracy",
          "Precision",
          "Recall",
          "ROC_auc",
          "PR_auc"
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "paper_bgcolor": "rgba(240, 240, 240, 100)",
        "plot_bgcolor": "rgba(0, 0, 0, 00)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Metric Values Across 100 Runs"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Metric"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bc2cd81d-f497-41a1-b060-6af6fe23f186\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bc2cd81d-f497-41a1-b060-6af6fe23f186\")) {                    Plotly.newPlot(                        \"bc2cd81d-f497-41a1-b060-6af6fe23f186\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"value=%{x}<br>metric=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"lightpink\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"type\":\"box\",\"x\":[0.8868033851880465,0.9099151743638078,0.9665992510863253,0.9042868864519347,0.9850967709983607],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"Accuracy\",\"Precision\",\"Recall\",\"ROC_auc\",\"PR_auc\"],\"y0\":\" \",\"yaxis\":\"y\"}],                        {\"boxmode\":\"group\",\"legend\":{\"tracegroupgap\":0},\"paper_bgcolor\":\"rgba(240, 240, 240, 100)\",\"plot_bgcolor\":\"rgba(0, 0, 0, 00)\",\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Distribution of Metric Values Across 100 Runs\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Metric\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bc2cd81d-f497-41a1-b060-6af6fe23f186');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_results(results, color = 'lightpink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Predictions -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_days</th>\n",
       "      <th>brq</th>\n",
       "      <th>brq_engagement</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_app</th>\n",
       "      <th>skewness_female</th>\n",
       "      <th>skewness_male</th>\n",
       "      <th>daypart_home</th>\n",
       "      <th>daypart_other</th>\n",
       "      <th>daypart_work</th>\n",
       "      <th>...</th>\n",
       "      <th>SOCIAL</th>\n",
       "      <th>SPORTS</th>\n",
       "      <th>TOOLS</th>\n",
       "      <th>TRAVEL_AND_LOCAL</th>\n",
       "      <th>VIDEO_PLAYERS</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ANDROID</th>\n",
       "      <th>IOS</th>\n",
       "      <th>SMART PHONE</th>\n",
       "      <th>TABLET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>370</td>\n",
       "      <td>370.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948649</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.0</td>\n",
       "      <td>6448</td>\n",
       "      <td>37.48837</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207196</td>\n",
       "      <td>0.56620</td>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.39813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>598</td>\n",
       "      <td>37.37500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.73746</td>\n",
       "      <td>0.08528</td>\n",
       "      <td>0.17726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_days   brq  brq_engagement  gender  distinct_app  skewness_female  \\\n",
       "0       1.0   370       370.00000    -1.0           1.0              0.0   \n",
       "1       1.0     1         1.00000    -1.0           7.0              0.0   \n",
       "2     172.0  6448        37.48837    -1.0          12.0              0.0   \n",
       "3      16.0   598        37.37500    -1.0           2.0              0.0   \n",
       "4       3.0     4         1.33333    -1.0           2.0              0.0   \n",
       "\n",
       "   skewness_male  daypart_home  daypart_other  daypart_work  ...  SOCIAL  \\\n",
       "0       0.948649       1.00000        0.00000       0.00000  ...     1.0   \n",
       "1       0.000000       0.00000        0.00000       1.00000  ...     0.0   \n",
       "2       0.207196       0.56620        0.03567       0.39813  ...     0.0   \n",
       "3       0.765886       0.73746        0.08528       0.17726  ...     0.0   \n",
       "4       0.500000       0.25000        0.00000       0.75000  ...     0.0   \n",
       "\n",
       "   SPORTS    TOOLS  TRAVEL_AND_LOCAL  VIDEO_PLAYERS  WEATHER  ANDROID  IOS  \\\n",
       "0     0.0  0.00000               0.0        0.00000      0.0        1    0   \n",
       "1     0.0  0.04011               0.0        0.02931      0.0        0    1   \n",
       "2     0.0  0.06166               0.0        0.48771      0.0        1    0   \n",
       "3     0.0  0.00000               0.0        0.71591      0.0        1    0   \n",
       "4     0.0  0.58448               0.0        0.41552      0.0        1    0   \n",
       "\n",
       "   SMART PHONE  TABLET  \n",
       "0            1       0  \n",
       "1            0       0  \n",
       "2            1       0  \n",
       "3            1       0  \n",
       "4            1       0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ifa_vals - ifa values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = test_merged['gender']\n",
    "test_X = test_merged.drop('gender', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94692582, 0.77437735, 0.99547982, ..., 0.46059863, 0.78101956,\n",
       "       0.55543941])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bst predictions\n",
    "predicted_probs = bst.predict(test_X)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49737"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>Predicted_Probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e932096849088aa76a953ce2f69728db6b6cdc056cd91...</td>\n",
       "      <td>0.946926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...</td>\n",
       "      <td>0.774377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...</td>\n",
       "      <td>0.995480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...</td>\n",
       "      <td>0.996855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...</td>\n",
       "      <td>0.998650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49732</th>\n",
       "      <td>d10c52e7c47bad5eb0599766a21885f34ff14aa43970ec...</td>\n",
       "      <td>0.560122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49733</th>\n",
       "      <td>d28fa808dbfed21864772dad6039592944724432817d60...</td>\n",
       "      <td>0.906669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49734</th>\n",
       "      <td>9755e3ed26f9104828f2e088cd03bf2c50cc51510b2529...</td>\n",
       "      <td>0.460599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49735</th>\n",
       "      <td>f77195b5ea5877ce5d8caf78e207b85056ba6d9b684515...</td>\n",
       "      <td>0.781020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49736</th>\n",
       "      <td>75923ed4a4e8d94e682cc76d14607f632b0603073ae62d...</td>\n",
       "      <td>0.555439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49737 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ifa  Predicted_Probs\n",
       "0      8e932096849088aa76a953ce2f69728db6b6cdc056cd91...         0.946926\n",
       "1      9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...         0.774377\n",
       "2      17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...         0.995480\n",
       "3      c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...         0.996855\n",
       "4      9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...         0.998650\n",
       "...                                                  ...              ...\n",
       "49732  d10c52e7c47bad5eb0599766a21885f34ff14aa43970ec...         0.560122\n",
       "49733  d28fa808dbfed21864772dad6039592944724432817d60...         0.906669\n",
       "49734  9755e3ed26f9104828f2e088cd03bf2c50cc51510b2529...         0.460599\n",
       "49735  f77195b5ea5877ce5d8caf78e207b85056ba6d9b684515...         0.781020\n",
       "49736  75923ed4a4e8d94e682cc76d14607f632b0603073ae62d...         0.555439\n",
       "\n",
       "[49737 rows x 2 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_df = pd.DataFrame({'ifa': test_ifa_vals,'Predicted_Probs':predicted_probs})\n",
    "# pdout.rename(columns = {0:'Target'},inplace = True)\n",
    "bst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_df.to_csv('lightgbm_model_predicted_probs.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifa</th>\n",
       "      <th>Predicted_Probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8e932096849088aa76a953ce2f69728db6b6cdc056cd91...</td>\n",
       "      <td>0.946926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...</td>\n",
       "      <td>0.774377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...</td>\n",
       "      <td>0.995480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...</td>\n",
       "      <td>0.996855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...</td>\n",
       "      <td>0.998650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ifa  Predicted_Probs\n",
       "0  8e932096849088aa76a953ce2f69728db6b6cdc056cd91...         0.946926\n",
       "1  9ba004d2e45aa4ae05935634f6198bf81e8766cb76a2bc...         0.774377\n",
       "2  17a74969d7681ff8e206aeb7ba0e83300c8d4ed1f6db80...         0.995480\n",
       "3  c8f3ddc5e2b0b3deacdb49181bc170363e5d01c992a660...         0.996855\n",
       "4  9c0c1acf953e443c577a30350a723ddf0a818e0763dce3...         0.998650"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM - Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute   import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve, roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost  import XGBClassifier\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_set_x, testing_set_y):\n",
    "    predictions = model.predict_proba(testing_set_x)\n",
    "    \n",
    "    accuracy  = accuracy_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    roc_auc   = roc_auc_score(testing_set_y, predictions[:,1])\n",
    "    precision = precision_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    recall    = recall_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    pr_auc    = average_precision_score(testing_set_y, predictions[:,1])\n",
    "    \n",
    "    result = pd.DataFrame([[accuracy, precision, recall, roc_auc, pr_auc]], columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    return(result)\n",
    "def run_experiment(df, model_class, n = 100, **kwargs):\n",
    "    results = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    for i in range(n):\n",
    "        # Compose dataset\n",
    "        train_x, test_x = train_test_split(df,\n",
    "                               test_size = 0.3,\n",
    "                               stratify  = df['gender'],\n",
    "                               random_state = i\n",
    "                                )\n",
    "        \n",
    "        train_y = train_x.pop('gender')\n",
    "        test_y  = test_x.pop('gender')\n",
    "        \n",
    "        # Train Model\n",
    "        model = model_class(**kwargs)\n",
    "        model.fit(train_x, train_y)\n",
    "         \n",
    "        # Evaluate results\n",
    "        current_result = evaluate(model, test_x, test_y)\n",
    "        results = results.append(current_result)\n",
    "        \n",
    "    return(results.reset_index(drop=True))\n",
    "def print_results(df, plot = True, extras = False, color='dodgerblue'):\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n",
    "    print('[ Experiment Results ]')\n",
    "    print('Accuracy:   {}'.format(df.Accuracy.mean()))\n",
    "    print('Precision:  {}'.format(df.Precision.mean()))\n",
    "    print('Recall:     {}'.format(df.Recall.mean()))\n",
    "    print('ROC Auc:    {}'.format(df.ROC_auc.mean()))\n",
    "    print('PR Auc:     {}'.format(df.PR_auc.mean()))\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n",
    "    \n",
    "    if plot:\n",
    "        fig = px.box(df.melt(var_name='metric'),\n",
    "                       y = 'metric',\n",
    "                       x = 'value',\n",
    "                       title = 'Distribution of Metric Values Across 100 Runs',\n",
    "                       color_discrete_sequence=[color]\n",
    "                      )\n",
    "\n",
    "        fig.update_xaxes(title='Metric')\n",
    "        fig.update_yaxes(title='Value')\n",
    "\n",
    "        fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 00)',\n",
    "                           'paper_bgcolor': 'rgba(240, 240, 240, 100)'})\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "    if extras:\n",
    "        print('Also, the maximum results were:')\n",
    "        print('    Accuracy:   {}'.format(df.Accuracy.max()))\n",
    "        print('    Precision:  {}'.format(df.Precision.max()))\n",
    "        print('    Recall:     {}'.format(df.Recall.max()))\n",
    "        print('    ROC Auc:    {}'.format(df.ROC_auc.max()))\n",
    "        print('    PR Auc:     {}'.format(df.PR_auc.max()))\n",
    "\n",
    "def evaluate(model, testing_set_x, testing_set_y):\n",
    "    predictions = model.predict_proba(testing_set_x)\n",
    "    \n",
    "    accuracy  = accuracy_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    roc_auc   = roc_auc_score(testing_set_y, predictions[:,1])\n",
    "    precision = precision_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    recall    = recall_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    pr_auc    = average_precision_score(testing_set_y, predictions[:,1])\n",
    "    \n",
    "    result = pd.DataFrame([[accuracy, precision, recall, roc_auc, pr_auc]], columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optimal = {\n",
    "              'n_estimators':2100,\n",
    "              'max_depth':27,\n",
    "              'max_features':0.15,\n",
    "              'max_samples':0.5363991145732665,\n",
    "              'min_samples_split':2,\n",
    "              'min_samples_leaf':4,\n",
    "              'n_jobs':-1,\n",
    "              'random_state':451,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-377-e2031064376c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_experiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrf_optimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_experiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'#3F3F3F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-375-9407ea4a8f6c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(df, model_class, n, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    457\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 )\n\u001b[0;32m--> 459\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_experiment = run_experiment(new_df, model_class = RandomForestClassifier, **rf_optimal)\n",
    "print_results(rf_experiment, color = '#3F3F3F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "optimal_1 = {'learning_rate': 0.02956340635276464,\n",
    " 'n_estimators': 3831,\n",
    " 'num_leaves': 101,\n",
    " 'max_depth': 28,\n",
    " 'max_bin': 211,\n",
    " 'bagging_freq': 9,\n",
    " 'bagging_fraction': 0.9292245982209768,\n",
    " 'feature_fraction': 0.95,\n",
    " 'lambda_l1': 2.50667180728151,\n",
    " 'lambda_l2': 4.010110517090694,\n",
    " 'drop_rate': 0.5917712341785191,\n",
    " 'min_child_samples': 15,\n",
    " 'min_child_weight': 3,\n",
    " 'min_split_gain': 0.0,\n",
    " 'scale_pos_weight': 0.283126887443018,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'bagging_seed': 42,\n",
    " 'metric': 'auc',\n",
    " 'verbosity': -1,\n",
    " 'random_state': 451,\n",
    " 'max_drop': 50}\n",
    "\n",
    "optimal_2 = {'learning_rate': 0.05744913989406643,\n",
    " 'n_estimators': 2067,\n",
    " 'num_leaves': 8,\n",
    " 'max_depth': 27,\n",
    " 'max_bin': 384,\n",
    " 'bagging_freq': 5,\n",
    " 'bagging_fraction': 0.7038650070406707,\n",
    " 'feature_fraction': 0.4806588217742334,\n",
    " 'lambda_l1': 2.841137907985995,\n",
    " 'lambda_l2': 5.983397074528167,\n",
    " 'drop_rate': 0.490746746058113,\n",
    " 'min_child_samples': 3,\n",
    " 'min_child_weight': 0,\n",
    " 'min_split_gain': 0.0,\n",
    " 'scale_pos_weight': 9.91024410907254,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'bagging_seed': 42,\n",
    " 'metric': 'auc',\n",
    " 'verbosity': -1,\n",
    " 'random_state': 451,\n",
    " 'max_drop': 50}\n",
    "\n",
    "optimal_3 = {'learning_rate': 0.05744913989406643,\n",
    " 'n_estimators': 2067,\n",
    " 'num_leaves': 8,\n",
    " 'max_depth': 27,\n",
    " 'max_bin': 384,\n",
    " 'bagging_freq': 5,\n",
    " 'bagging_fraction': 0.7038650070406707,\n",
    " 'feature_fraction': 0.4806588217742334,\n",
    " 'lambda_l1': 2.841137907985995,\n",
    " 'lambda_l2': 5.983397074528167,\n",
    " 'drop_rate': 0.490746746058113,\n",
    " 'min_child_samples': 3,\n",
    " 'min_child_weight': 0,\n",
    " 'min_split_gain': 0.0,\n",
    " 'scale_pos_weight': 0.91024410907254,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'bagging_seed': 42,\n",
    " 'metric': 'auc',\n",
    " 'verbosity': -1,\n",
    " 'random_state': 451,\n",
    " 'max_drop': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9292245982209768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9292245982209768\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.010110517090694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.010110517090694\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.50667180728151, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.50667180728151\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "[ Experiment Results ]\n",
      "Accuracy:   0.8717006707167186\n",
      "Precision:  0.943733789883218\n",
      "Recall:     0.9076068650252629\n",
      "ROC Auc:    0.8992661187424948\n",
      "PR Auc:     0.9841779384156354\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "value=%{x}<br>metric=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#8400E8"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "type": "box",
         "x": [
          0.8723180110264424,
          0.8739509077501141,
          0.8718615022649858,
          0.8721951048214348,
          0.8719141763528462,
          0.8694209361941215,
          0.8734066088422235,
          0.8704919759806159,
          0.8700179091898725,
          0.8684201285247743,
          0.8719141763528462,
          0.8698247708677178,
          0.8705270920391895,
          0.8703690697756084,
          0.872686729641465,
          0.8703690697756084,
          0.8722126628507216,
          0.8715981318256839,
          0.8728798679636197,
          0.8710362748885065,
          0.8711942971520876,
          0.8706499982441971,
          0.8703339537170348,
          0.8687537310812234,
          0.8715630157671103,
          0.871756154089265,
          0.8725462654071707,
          0.8703515117463216,
          0.8737753274572462,
          0.8706324402149103,
          0.8732661446079292,
          0.8728096358464726,
          0.873195912490782,
          0.8715103416792499,
          0.8703866278048952,
          0.8701232573655933,
          0.8729149840221934,
          0.8724935913193104,
          0.870913368683499,
          0.8720195245285669,
          0.8710538329177933,
          0.8729852161393405,
          0.8724760332900235,
          0.870825578537065,
          0.8733539347543632,
          0.8715805737963971,
          0.8721775467921481,
          0.8728798679636197,
          0.8697369807212838,
          0.8701408153948801,
          0.8741791621308425,
          0.8744952066580047,
          0.8732134705200688,
          0.8705095340099027,
          0.8708431365663518,
          0.8733363767250764,
          0.871756154089265,
          0.8727218457000386,
          0.871246971239948,
          0.873195912490782,
          0.8707553464199178,
          0.8737577694279594,
          0.8723180110264424,
          0.8711767391228008,
          0.8723706851143028,
          0.8688415212276575,
          0.8724058011728764,
          0.8723180110264424,
          0.872686729641465,
          0.8732485865786425,
          0.8711591810935141,
          0.8719844084699934,
          0.871334761386382,
          0.8716508059135443,
          0.8719668504407065,
          0.8694911683112687,
          0.8715981318256839,
          0.8710187168592197,
          0.8718966183235594,
          0.8710187168592197,
          0.8721248727042876,
          0.8705622080977631,
          0.8704568599220424,
          0.8721424307335744,
          0.8724409172314499,
          0.8691575657548197,
          0.8724584752607367,
          0.8723355690557292,
          0.8708080205077782,
          0.8727920778171858,
          0.8726340555536046,
          0.8711591810935141,
          0.8709484847420725,
          0.8712645292692348,
          0.8734592829300839,
          0.8732134705200688,
          0.8717737121185518,
          0.8716508059135443,
          0.8724935913193104,
          0.8718790602942725,
          0.9432745130680872,
          0.9446029531207697,
          0.943705710712797,
          0.9438019628680377,
          0.9439497883520654,
          0.9419716315011143,
          0.9438641754400566,
          0.9427452287690641,
          0.943045501011829,
          0.9421239307323179,
          0.9423080918250754,
          0.943847472509094,
          0.944451415903292,
          0.9428291544217262,
          0.9429675971151248,
          0.9444409580587805,
          0.9448970213477044,
          0.9435586402384376,
          0.9448107499061053,
          0.9448011880608254,
          0.9434021263289556,
          0.9430697713201726,
          0.9434738138476632,
          0.9428117885414709,
          0.9428547645308305,
          0.9440874572310773,
          0.9434005451859251,
          0.9421828171828172,
          0.9451106337719756,
          0.9438087283357695,
          0.9432276297743236,
          0.9444351778491306,
          0.9455563447958886,
          0.9437747653806048,
          0.9429411274182788,
          0.9441458773342255,
          0.9428909755692205,
          0.9428440786465477,
          0.9442169480193883,
          0.9436607979997916,
          0.9435591523444018,
          0.9448733464090473,
          0.9449514461731231,
          0.9436919545027653,
          0.9439345695198851,
          0.9440201969621098,
          0.9450992685475444,
          0.9437552039966695,
          0.9414969947798598,
          0.9426659997499062,
          0.9452854581091789,
          0.9432549068426076,
          0.9432056690426217,
          0.9448596370030773,
          0.944621047348168,
          0.945082001418854,
          0.9432920987962847,
          0.9435599675344946,
          0.9425753698271019,
          0.9435732113144759,
          0.9435576421491915,
          0.9447199533255543,
          0.9444027204072266,
          0.9441973142869077,
          0.9425965553778073,
          0.9429287654063088,
          0.9453002529105616,
          0.9436992293272235,
          0.9441121923774198,
          0.9440384735494347,
          0.9426246566219928,
          0.9432151703753044,
          0.9437075042756434,
          0.9433773592766969,
          0.9428636798535713,
          0.9423818557044882,
          0.9420476497102382,
          0.9430392933038877,
          0.9430432430182711,
          0.9441868237456658,
          0.9438157757632594,
          0.9436557972527243,
          0.9445583497217457,
          0.9436320459942508,
          0.9445592070944183,
          0.942580537472876,
          0.9446902654867256,
          0.9453329152294345,
          0.9436167104878761,
          0.9447678664580073,
          0.9440716816003334,
          0.9449023135171317,
          0.9428315561479507,
          0.9445930694310914,
          0.9439415693863537,
          0.9437035804189985,
          0.9445152128970284,
          0.9442658539640129,
          0.9452873563218391,
          0.9427656344238088,
          0.9088539578153821,
          0.9093953003448553,
          0.9078314219263774,
          0.9081321677760847,
          0.9076309246932392,
          0.9067487368674312,
          0.909535648408052,
          0.907209880503649,
          0.9063076429545273,
          0.9053653059587777,
          0.9094153500681691,
          0.9052049081722672,
          0.9054054054054054,
          0.9069692838238832,
          0.9096358970246211,
          0.905224957895581,
          0.9069692838238832,
          0.9076710241398669,
          0.9078715213730051,
          0.9056460020851712,
          0.9073502285668458,
          0.9070494827171385,
          0.906227444061272,
          0.9050244606624429,
          0.9083928141791643,
          0.9072900793969043,
          0.9089943058785789,
          0.9076509744165531,
          0.9086334108589301,
          0.906227444061272,
          0.9100569412142112,
          0.9081923169460261,
          0.9074504771834149,
          0.907330178843532,
          0.9068690352073141,
          0.9052450076188948,
          0.9099967920442698,
          0.909535648408052,
          0.9061271954447029,
          0.9080720186061433,
          0.9070093832705108,
          0.9079316705429465,
          0.9072299302269629,
          0.9065883390809207,
          0.9093953003448553,
          0.9071497313337076,
          0.9067086374208035,
          0.9090143556018927,
          0.9076509744165531,
          0.9068690352073141,
          0.9089341567086374,
          0.9115205710161199,
          0.9100168417675836,
          0.9049442617691876,
          0.9056059026385436,
          0.9081321677760847,
          0.9081522174993986,
          0.9090344053252065,
          0.908312615285909,
          0.9095957975779935,
          0.9066484882508621,
          0.9090344053252065,
          0.9076309246932392,
          0.9064680407410378,
          0.9096559467479349,
          0.9050044109391291,
          0.906768786590745,
          0.9083928141791643,
          0.9083928141791643,
          0.9091547036650894,
          0.9081522174993986,
          0.9085131125190472,
          0.9071898307803352,
          0.9079316705429465,
          0.908874007538696,
          0.9063878418477825,
          0.9093151014516,
          0.9075306760766702,
          0.9085933114123025,
          0.9062875932312134,
          0.9080319191595156,
          0.9063076429545273,
          0.9052049081722672,
          0.9082524661159675,
          0.9076108749699254,
          0.9057663004250541,
          0.9074905766300425,
          0.9066484882508621,
          0.9066484882508621,
          0.9078113722030636,
          0.9083727644558505,
          0.9056861015317988,
          0.9076710241398669,
          0.9061472451680167,
          0.9095155986847382,
          0.9094754992381106,
          0.9068489854840003,
          0.9069692838238832,
          0.9068890849306279,
          0.908874007538696,
          0.9000858792705033,
          0.9009452753368052,
          0.8975197328028499,
          0.8988661533046148,
          0.8981526970158102,
          0.897820984286297,
          0.8992870601091272,
          0.8980758605161088,
          0.8974915405350792,
          0.8967539922857231,
          0.8969428882696622,
          0.8984801621386397,
          0.8986949403381647,
          0.8996005416971813,
          0.900263501180016,
          0.898162680803578,
          0.9032433296966983,
          0.8996485287468843,
          0.9011402219295318,
          0.9004269880062657,
          0.8990115945300774,
          0.8973229463831534,
          0.8980275023850617,
          0.89675072903604,
          0.8985526943780169,
          0.9002225250183042,
          0.9009663986466034,
          0.8985499976647369,
          0.9006008268689453,
          0.898297294102032,
          0.9004156544455171,
          0.8988286655908064,
          0.9004305146953764,
          0.898405011084738,
          0.8992455556522193,
          0.898260205796388,
          0.9002015900818603,
          0.8984680170144932,
          0.8994143551735911,
          0.9016421250272872,
          0.8984769725386889,
          0.9012649661638399,
          0.9007037481209971,
          0.8989443664877763,
          0.8998684411825644,
          0.8988148605149985,
          0.8986781127906448,
          0.8996269182157792,
          0.8976846019779237,
          0.8976750558395274,
          0.9026272865097644,
          0.9026050726173898,
          0.8997027207865447,
          0.899308649395117,
          0.8995269287896147,
          0.9007007752212206,
          0.899261522064406,
          0.898397201380418,
          0.8972944297732919,
          0.9008150796059563,
          0.897246633929625,
          0.9009738910905375,
          0.9009348907245324,
          0.8982207167927349,
          0.899151728725782,
          0.8976245646989218,
          0.8998996162644988,
          0.901108253696633,
          0.9016654932375835,
          0.9008283747990014,
          0.8994832714943741,
          0.8988529869986014,
          0.8994208930036856,
          0.8995402367297289,
          0.8997269572138791,
          0.8980696541098102,
          0.8976749269524958,
          0.8991379732219091,
          0.8992126767120521,
          0.8973446036535118,
          0.8985378029686553,
          0.897976346980523,
          0.8977956133698148,
          0.899563749406808,
          0.8979206224598917,
          0.8977895245198201,
          0.9003817585725059,
          0.9014062900859914,
          0.8993628031935703,
          0.9007048415362555,
          0.9001388929144448,
          0.8982635540265318,
          0.8975799881983669,
          0.8998024722674198,
          0.89897016089022,
          0.9005129570727889,
          0.8993101478839038,
          0.8966369996857536,
          0.9013572733563753,
          0.899286858988704,
          0.9842828738999161,
          0.9845648448075983,
          0.9837879046969087,
          0.9840616176264088,
          0.9839556126033748,
          0.983969848887456,
          0.9840793993982158,
          0.9838617083277051,
          0.9839102451755675,
          0.9838707116919392,
          0.9837890877134935,
          0.9841068579905973,
          0.9840698296279631,
          0.9843314077936471,
          0.984380836146826,
          0.9839810532998792,
          0.9851006729982178,
          0.984274614913855,
          0.9844973808461733,
          0.9845610851998501,
          0.984140705085948,
          0.9836736845201481,
          0.9840655914962582,
          0.9839141402268006,
          0.9840475854506592,
          0.9845099264428299,
          0.9845506547424614,
          0.984182302422768,
          0.9844166792000681,
          0.9840247228334154,
          0.9843921391549626,
          0.9841120272746784,
          0.9843169758957364,
          0.9839888483277144,
          0.9843099156463038,
          0.9838312391818569,
          0.9841987591092135,
          0.9839582519704311,
          0.984263348351549,
          0.9846138358872357,
          0.9839321443648548,
          0.9845132368766427,
          0.9844191712192918,
          0.9839522735402957,
          0.984276369360858,
          0.9841491167696228,
          0.9839318105775806,
          0.9841972719807619,
          0.9839399327740056,
          0.9839128111881863,
          0.9847351757468612,
          0.9847126860428185,
          0.9841809483994124,
          0.9840742733454978,
          0.9842656340559122,
          0.9843802760700917,
          0.9841331395390762,
          0.983845358134906,
          0.9836522166957145,
          0.984420518903294,
          0.9837694614290261,
          0.9845049013103155,
          0.9845525768879768,
          0.9839028687710536,
          0.9840488733384501,
          0.9839926508801994,
          0.9842202221048525,
          0.9845785122618483,
          0.9846216229234135,
          0.9844307443868345,
          0.9843350252193066,
          0.9841111690536202,
          0.9842656004200954,
          0.9841856670891775,
          0.9842374529793895,
          0.9840587866954298,
          0.9838822348382441,
          0.9841770587795158,
          0.9842051343145,
          0.9837367153523335,
          0.9840115771825383,
          0.9840724999957486,
          0.9839371443470469,
          0.9842933480988454,
          0.9837565368192194,
          0.9840230413140202,
          0.9842344052932082,
          0.9846518700468767,
          0.9843345005258606,
          0.9845216417287814,
          0.9843240140102891,
          0.9839340027196365,
          0.9839267973463999,
          0.9841547526690735,
          0.9841042065000449,
          0.984357117349768,
          0.9843026556080201,
          0.9835285415604285,
          0.984618756038617,
          0.984315860921221
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc"
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "paper_bgcolor": "rgba(240, 240, 240, 100)",
        "plot_bgcolor": "rgba(0, 0, 0, 00)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Metric Values Across 100 Runs"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Metric"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"b0a015ac-2979-46b7-a0c7-b07e2925af53\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b0a015ac-2979-46b7-a0c7-b07e2925af53\")) {                    Plotly.newPlot(                        \"b0a015ac-2979-46b7-a0c7-b07e2925af53\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"value=%{x}<br>metric=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#8400E8\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"type\":\"box\",\"x\":[0.8723180110264424,0.8739509077501141,0.8718615022649858,0.8721951048214348,0.8719141763528462,0.8694209361941215,0.8734066088422235,0.8704919759806159,0.8700179091898725,0.8684201285247743,0.8719141763528462,0.8698247708677178,0.8705270920391895,0.8703690697756084,0.872686729641465,0.8703690697756084,0.8722126628507216,0.8715981318256839,0.8728798679636197,0.8710362748885065,0.8711942971520876,0.8706499982441971,0.8703339537170348,0.8687537310812234,0.8715630157671103,0.871756154089265,0.8725462654071707,0.8703515117463216,0.8737753274572462,0.8706324402149103,0.8732661446079292,0.8728096358464726,0.873195912490782,0.8715103416792499,0.8703866278048952,0.8701232573655933,0.8729149840221934,0.8724935913193104,0.870913368683499,0.8720195245285669,0.8710538329177933,0.8729852161393405,0.8724760332900235,0.870825578537065,0.8733539347543632,0.8715805737963971,0.8721775467921481,0.8728798679636197,0.8697369807212838,0.8701408153948801,0.8741791621308425,0.8744952066580047,0.8732134705200688,0.8705095340099027,0.8708431365663518,0.8733363767250764,0.871756154089265,0.8727218457000386,0.871246971239948,0.873195912490782,0.8707553464199178,0.8737577694279594,0.8723180110264424,0.8711767391228008,0.8723706851143028,0.8688415212276575,0.8724058011728764,0.8723180110264424,0.872686729641465,0.8732485865786425,0.8711591810935141,0.8719844084699934,0.871334761386382,0.8716508059135443,0.8719668504407065,0.8694911683112687,0.8715981318256839,0.8710187168592197,0.8718966183235594,0.8710187168592197,0.8721248727042876,0.8705622080977631,0.8704568599220424,0.8721424307335744,0.8724409172314499,0.8691575657548197,0.8724584752607367,0.8723355690557292,0.8708080205077782,0.8727920778171858,0.8726340555536046,0.8711591810935141,0.8709484847420725,0.8712645292692348,0.8734592829300839,0.8732134705200688,0.8717737121185518,0.8716508059135443,0.8724935913193104,0.8718790602942725,0.9432745130680872,0.9446029531207697,0.943705710712797,0.9438019628680377,0.9439497883520654,0.9419716315011143,0.9438641754400566,0.9427452287690641,0.943045501011829,0.9421239307323179,0.9423080918250754,0.943847472509094,0.944451415903292,0.9428291544217262,0.9429675971151248,0.9444409580587805,0.9448970213477044,0.9435586402384376,0.9448107499061053,0.9448011880608254,0.9434021263289556,0.9430697713201726,0.9434738138476632,0.9428117885414709,0.9428547645308305,0.9440874572310773,0.9434005451859251,0.9421828171828172,0.9451106337719756,0.9438087283357695,0.9432276297743236,0.9444351778491306,0.9455563447958886,0.9437747653806048,0.9429411274182788,0.9441458773342255,0.9428909755692205,0.9428440786465477,0.9442169480193883,0.9436607979997916,0.9435591523444018,0.9448733464090473,0.9449514461731231,0.9436919545027653,0.9439345695198851,0.9440201969621098,0.9450992685475444,0.9437552039966695,0.9414969947798598,0.9426659997499062,0.9452854581091789,0.9432549068426076,0.9432056690426217,0.9448596370030773,0.944621047348168,0.945082001418854,0.9432920987962847,0.9435599675344946,0.9425753698271019,0.9435732113144759,0.9435576421491915,0.9447199533255543,0.9444027204072266,0.9441973142869077,0.9425965553778073,0.9429287654063088,0.9453002529105616,0.9436992293272235,0.9441121923774198,0.9440384735494347,0.9426246566219928,0.9432151703753044,0.9437075042756434,0.9433773592766969,0.9428636798535713,0.9423818557044882,0.9420476497102382,0.9430392933038877,0.9430432430182711,0.9441868237456658,0.9438157757632594,0.9436557972527243,0.9445583497217457,0.9436320459942508,0.9445592070944183,0.942580537472876,0.9446902654867256,0.9453329152294345,0.9436167104878761,0.9447678664580073,0.9440716816003334,0.9449023135171317,0.9428315561479507,0.9445930694310914,0.9439415693863537,0.9437035804189985,0.9445152128970284,0.9442658539640129,0.9452873563218391,0.9427656344238088,0.9088539578153821,0.9093953003448553,0.9078314219263774,0.9081321677760847,0.9076309246932392,0.9067487368674312,0.909535648408052,0.907209880503649,0.9063076429545273,0.9053653059587777,0.9094153500681691,0.9052049081722672,0.9054054054054054,0.9069692838238832,0.9096358970246211,0.905224957895581,0.9069692838238832,0.9076710241398669,0.9078715213730051,0.9056460020851712,0.9073502285668458,0.9070494827171385,0.906227444061272,0.9050244606624429,0.9083928141791643,0.9072900793969043,0.9089943058785789,0.9076509744165531,0.9086334108589301,0.906227444061272,0.9100569412142112,0.9081923169460261,0.9074504771834149,0.907330178843532,0.9068690352073141,0.9052450076188948,0.9099967920442698,0.909535648408052,0.9061271954447029,0.9080720186061433,0.9070093832705108,0.9079316705429465,0.9072299302269629,0.9065883390809207,0.9093953003448553,0.9071497313337076,0.9067086374208035,0.9090143556018927,0.9076509744165531,0.9068690352073141,0.9089341567086374,0.9115205710161199,0.9100168417675836,0.9049442617691876,0.9056059026385436,0.9081321677760847,0.9081522174993986,0.9090344053252065,0.908312615285909,0.9095957975779935,0.9066484882508621,0.9090344053252065,0.9076309246932392,0.9064680407410378,0.9096559467479349,0.9050044109391291,0.906768786590745,0.9083928141791643,0.9083928141791643,0.9091547036650894,0.9081522174993986,0.9085131125190472,0.9071898307803352,0.9079316705429465,0.908874007538696,0.9063878418477825,0.9093151014516,0.9075306760766702,0.9085933114123025,0.9062875932312134,0.9080319191595156,0.9063076429545273,0.9052049081722672,0.9082524661159675,0.9076108749699254,0.9057663004250541,0.9074905766300425,0.9066484882508621,0.9066484882508621,0.9078113722030636,0.9083727644558505,0.9056861015317988,0.9076710241398669,0.9061472451680167,0.9095155986847382,0.9094754992381106,0.9068489854840003,0.9069692838238832,0.9068890849306279,0.908874007538696,0.9000858792705033,0.9009452753368052,0.8975197328028499,0.8988661533046148,0.8981526970158102,0.897820984286297,0.8992870601091272,0.8980758605161088,0.8974915405350792,0.8967539922857231,0.8969428882696622,0.8984801621386397,0.8986949403381647,0.8996005416971813,0.900263501180016,0.898162680803578,0.9032433296966983,0.8996485287468843,0.9011402219295318,0.9004269880062657,0.8990115945300774,0.8973229463831534,0.8980275023850617,0.89675072903604,0.8985526943780169,0.9002225250183042,0.9009663986466034,0.8985499976647369,0.9006008268689453,0.898297294102032,0.9004156544455171,0.8988286655908064,0.9004305146953764,0.898405011084738,0.8992455556522193,0.898260205796388,0.9002015900818603,0.8984680170144932,0.8994143551735911,0.9016421250272872,0.8984769725386889,0.9012649661638399,0.9007037481209971,0.8989443664877763,0.8998684411825644,0.8988148605149985,0.8986781127906448,0.8996269182157792,0.8976846019779237,0.8976750558395274,0.9026272865097644,0.9026050726173898,0.8997027207865447,0.899308649395117,0.8995269287896147,0.9007007752212206,0.899261522064406,0.898397201380418,0.8972944297732919,0.9008150796059563,0.897246633929625,0.9009738910905375,0.9009348907245324,0.8982207167927349,0.899151728725782,0.8976245646989218,0.8998996162644988,0.901108253696633,0.9016654932375835,0.9008283747990014,0.8994832714943741,0.8988529869986014,0.8994208930036856,0.8995402367297289,0.8997269572138791,0.8980696541098102,0.8976749269524958,0.8991379732219091,0.8992126767120521,0.8973446036535118,0.8985378029686553,0.897976346980523,0.8977956133698148,0.899563749406808,0.8979206224598917,0.8977895245198201,0.9003817585725059,0.9014062900859914,0.8993628031935703,0.9007048415362555,0.9001388929144448,0.8982635540265318,0.8975799881983669,0.8998024722674198,0.89897016089022,0.9005129570727889,0.8993101478839038,0.8966369996857536,0.9013572733563753,0.899286858988704,0.9842828738999161,0.9845648448075983,0.9837879046969087,0.9840616176264088,0.9839556126033748,0.983969848887456,0.9840793993982158,0.9838617083277051,0.9839102451755675,0.9838707116919392,0.9837890877134935,0.9841068579905973,0.9840698296279631,0.9843314077936471,0.984380836146826,0.9839810532998792,0.9851006729982178,0.984274614913855,0.9844973808461733,0.9845610851998501,0.984140705085948,0.9836736845201481,0.9840655914962582,0.9839141402268006,0.9840475854506592,0.9845099264428299,0.9845506547424614,0.984182302422768,0.9844166792000681,0.9840247228334154,0.9843921391549626,0.9841120272746784,0.9843169758957364,0.9839888483277144,0.9843099156463038,0.9838312391818569,0.9841987591092135,0.9839582519704311,0.984263348351549,0.9846138358872357,0.9839321443648548,0.9845132368766427,0.9844191712192918,0.9839522735402957,0.984276369360858,0.9841491167696228,0.9839318105775806,0.9841972719807619,0.9839399327740056,0.9839128111881863,0.9847351757468612,0.9847126860428185,0.9841809483994124,0.9840742733454978,0.9842656340559122,0.9843802760700917,0.9841331395390762,0.983845358134906,0.9836522166957145,0.984420518903294,0.9837694614290261,0.9845049013103155,0.9845525768879768,0.9839028687710536,0.9840488733384501,0.9839926508801994,0.9842202221048525,0.9845785122618483,0.9846216229234135,0.9844307443868345,0.9843350252193066,0.9841111690536202,0.9842656004200954,0.9841856670891775,0.9842374529793895,0.9840587866954298,0.9838822348382441,0.9841770587795158,0.9842051343145,0.9837367153523335,0.9840115771825383,0.9840724999957486,0.9839371443470469,0.9842933480988454,0.9837565368192194,0.9840230413140202,0.9842344052932082,0.9846518700468767,0.9843345005258606,0.9845216417287814,0.9843240140102891,0.9839340027196365,0.9839267973463999,0.9841547526690735,0.9841042065000449,0.984357117349768,0.9843026556080201,0.9835285415604285,0.984618756038617,0.984315860921221],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\"],\"y0\":\" \",\"yaxis\":\"y\"}],                        {\"boxmode\":\"group\",\"legend\":{\"tracegroupgap\":0},\"paper_bgcolor\":\"rgba(240, 240, 240, 100)\",\"plot_bgcolor\":\"rgba(0, 0, 0, 00)\",\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Distribution of Metric Values Across 100 Runs\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Metric\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b0a015ac-2979-46b7-a0c7-b07e2925af53');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model #1\n",
    "lgbm_experiment_1 = run_experiment(new_df, model_class = LGBMClassifier, **optimal_1)\n",
    "print_results(lgbm_experiment_1, color = '#8400E8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "[ Experiment Results ]\n",
      "Accuracy:   0.8758369912561015\n",
      "Precision:  0.8758446431422978\n",
      "Recall:     0.9999671184537654\n",
      "ROC Auc:    0.9015745346283023\n",
      "PR Auc:     0.9844541530205316\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "value=%{x}<br>metric=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#00E800"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "type": "box",
         "x": [
          0.8758471749130877,
          0.8758120588545142,
          0.8757945008252274,
          0.875829616883801,
          0.8758822909716614,
          0.875829616883801,
          0.8758471749130877,
          0.8758471749130877,
          0.875741826737367,
          0.8758647329423745,
          0.8758120588545142,
          0.8758998490009481,
          0.8758471749130877,
          0.8758471749130877,
          0.8758120588545142,
          0.8758120588545142,
          0.8758822909716614,
          0.8758120588545142,
          0.875829616883801,
          0.875829616883801,
          0.875829616883801,
          0.8758120588545142,
          0.8758822909716614,
          0.875829616883801,
          0.8758120588545142,
          0.875741826737367,
          0.8758120588545142,
          0.8758647329423745,
          0.8759174070302349,
          0.8757945008252274,
          0.8758471749130877,
          0.8758822909716614,
          0.8757769427959405,
          0.875829616883801,
          0.875829616883801,
          0.8759700811180953,
          0.8758120588545142,
          0.875829616883801,
          0.8758647329423745,
          0.875829616883801,
          0.8759349650595217,
          0.8758822909716614,
          0.875829616883801,
          0.8757769427959405,
          0.8757945008252274,
          0.8757769427959405,
          0.8758471749130877,
          0.8757945008252274,
          0.8757945008252274,
          0.8759525230888086,
          0.8758647329423745,
          0.875741826737367,
          0.8758822909716614,
          0.8758647329423745,
          0.8758822909716614,
          0.875829616883801,
          0.875829616883801,
          0.8757945008252274,
          0.8758471749130877,
          0.8758647329423745,
          0.8758471749130877,
          0.8758471749130877,
          0.875829616883801,
          0.8759349650595217,
          0.875829616883801,
          0.8757945008252274,
          0.8758998490009481,
          0.8758647329423745,
          0.8758120588545142,
          0.8758822909716614,
          0.8758822909716614,
          0.8758471749130877,
          0.8758120588545142,
          0.8759174070302349,
          0.875829616883801,
          0.875829616883801,
          0.8758998490009481,
          0.8757945008252274,
          0.8759174070302349,
          0.8758822909716614,
          0.8758471749130877,
          0.8758471749130877,
          0.8757945008252274,
          0.8757593847666538,
          0.8757769427959405,
          0.875829616883801,
          0.8757945008252274,
          0.875741826737367,
          0.8757945008252274,
          0.8757945008252274,
          0.8757945008252274,
          0.8758822909716614,
          0.8757593847666538,
          0.8759174070302349,
          0.8757769427959405,
          0.8758647329423745,
          0.8757593847666538,
          0.875829616883801,
          0.875829616883801,
          0.8759174070302349,
          0.8758451137062078,
          0.8758275529019229,
          0.8757857769973661,
          0.8758429334082607,
          0.8758626745104926,
          0.8758297334316721,
          0.8758319138848403,
          0.8758451137062078,
          0.8757924312933533,
          0.8758604945209328,
          0.8758011554197616,
          0.8759176648284098,
          0.8758319138848403,
          0.8758847187340838,
          0.8758407530337355,
          0.8758275529019229,
          0.8758758758758759,
          0.8758275529019229,
          0.8758561343121071,
          0.8758561343121071,
          0.8758165343822435,
          0.8758407530337355,
          0.8758758758758759,
          0.8758297334316721,
          0.8758011554197616,
          0.8757792333222119,
          0.8758143536972974,
          0.8758472939275805,
          0.8759066402065295,
          0.8758121729357637,
          0.8758451137062078,
          0.8758758758758759,
          0.8757835958489174,
          0.8758693361433088,
          0.8758429334082607,
          0.8759659969088099,
          0.8758011554197616,
          0.8758297334316721,
          0.8758604945209328,
          0.8758429334082607,
          0.8759220231822972,
          0.8758626745104926,
          0.8758297334316721,
          0.8757703991150287,
          0.8758253722955887,
          0.8757967935097547,
          0.8758847187340838,
          0.8758121729357637,
          0.8758121729357637,
          0.8759506120800182,
          0.8758736960415862,
          0.8757660362780734,
          0.8758758758758759,
          0.8758604945209328,
          0.8758626745104926,
          0.8758297334316721,
          0.8758297334316721,
          0.8758121729357637,
          0.8758847187340838,
          0.8758604945209328,
          0.8758583144548057,
          0.8758451137062078,
          0.8758429334082607,
          0.8759088195005444,
          0.8758429334082607,
          0.8758253722955887,
          0.8759176648284098,
          0.8758736960415862,
          0.875853954092833,
          0.8759022813888547,
          0.8758626745104926,
          0.8758715161307318,
          0.8758011554197616,
          0.875893437296946,
          0.8758429334082607,
          0.8758297334316721,
          0.8758912577710652,
          0.8758121729357637,
          0.875893437296946,
          0.8758758758758759,
          0.8758451137062078,
          0.8758319138848403,
          0.8758253722955887,
          0.8758474129755173,
          0.8757835958489174,
          0.8758561343121071,
          0.875838572582628,
          0.8757660362780734,
          0.8757857769973661,
          0.8757857769973661,
          0.8758253722955887,
          0.8759022813888547,
          0.8757682177348551,
          0.8759066402065295,
          0.8758099920976381,
          0.8758604945209328,
          0.8758342114506498,
          0.8758165343822435,
          0.8758165343822435,
          0.8759066402065295,
          0.9999799502766862,
          0.9999599005533724,
          1,
          0.9999599005533724,
          1,
          0.9999799502766862,
          1,
          0.9999799502766862,
          0.9999198011067447,
          0.9999799502766862,
          1,
          0.9999398508300585,
          1,
          0.9999198011067447,
          0.9999398508300585,
          0.9999599005533724,
          0.9999799502766862,
          0.9999599005533724,
          0.9999398508300585,
          0.9999398508300585,
          1,
          0.9999398508300585,
          0.9999799502766862,
          0.9999799502766862,
          1,
          0.9999398508300585,
          0.9999799502766862,
          1,
          0.9999799502766862,
          0.9999599005533724,
          0.9999799502766862,
          0.9999799502766862,
          0.9999799502766862,
          0.9999198011067447,
          0.9999599005533724,
          0.9999599005533724,
          1,
          0.9999799502766862,
          0.9999799502766862,
          0.9999599005533724,
          0.9999799502766862,
          1,
          0.9999799502766862,
          1,
          0.9999398508300585,
          0.9999599005533724,
          0.9999198011067447,
          0.9999599005533724,
          0.9999599005533724,
          0.9999599005533724,
          0.9999599005533724,
          0.9999599005533724,
          0.9999799502766862,
          0.9999799502766862,
          1,
          0.9999799502766862,
          0.9999799502766862,
          0.9999599005533724,
          0.9999198011067447,
          0.9999799502766862,
          0.9999599005533724,
          0.9999799502766862,
          0.9999599005533724,
          1,
          0.9999599005533724,
          0.9999398508300585,
          0.9999398508300585,
          0.9999599005533724,
          0.9999198011067447,
          0.9999398508300585,
          1,
          0.9999398508300585,
          1,
          1,
          0.9999599005533724,
          0.9999799502766862,
          0.9999799502766862,
          0.9999599005533724,
          1,
          0.9999799502766862,
          0.9999799502766862,
          1,
          0.9999398508300585,
          0.9998596519368033,
          0.9999799502766862,
          0.9999398508300585,
          0.9999198011067447,
          0.9999599005533724,
          1,
          1,
          0.9999398508300585,
          0.9999398508300585,
          0.9999799502766862,
          0.9999799502766862,
          0.9999398508300585,
          0.9999799502766862,
          0.9998797016601171,
          1,
          1,
          0.9999799502766862,
          0.901994837844931,
          0.9021538561152993,
          0.8995714797960316,
          0.9013614147374837,
          0.90046191355919,
          0.9011938814249731,
          0.9020883843358486,
          0.9001169325471107,
          0.9001175344920392,
          0.899588933366277,
          0.9004395056280973,
          0.9003175289241194,
          0.9009451124575895,
          0.9018550818122757,
          0.903245969756338,
          0.9002314224725185,
          0.9048336965247139,
          0.9014457946693956,
          0.9043531858415482,
          0.9020919520988484,
          0.9015310314876174,
          0.9006064554081122,
          0.8994239537166044,
          0.8997602058190495,
          0.9009431862338182,
          0.9033079771656822,
          0.9033659267580378,
          0.9009554984862034,
          0.9031425726137075,
          0.9009882400412929,
          0.9016517943873511,
          0.9015982609462595,
          0.9031596239431066,
          0.9007075325841712,
          0.9024344417670942,
          0.9003356184314778,
          0.9025893158236722,
          0.9007952763259779,
          0.9017957243769579,
          0.904779280703174,
          0.8998451692834566,
          0.9031075351698433,
          0.9027043496240271,
          0.9011366116763017,
          0.90238506245418,
          0.9019619504067176,
          0.900466679546683,
          0.9016296796388471,
          0.9001770548065731,
          0.9008009204448961,
          0.9040553590706593,
          0.9046953313389288,
          0.9007133169208492,
          0.9020849327128113,
          0.9014892480115309,
          0.901836257223934,
          0.9012204533986303,
          0.9002579717847194,
          0.9003794301078882,
          0.9031190146137159,
          0.8997684857485841,
          0.9046447042295863,
          0.9033459931746867,
          0.8991725149464199,
          0.9003544911754137,
          0.8999146733857581,
          0.9019880833146622,
          0.9038248863964207,
          0.9040009489144833,
          0.9020516076252265,
          0.9021808204154158,
          0.9008920549070766,
          0.9010960618332333,
          0.9019636698447018,
          0.902818332499354,
          0.9007585137787658,
          0.8994698417489332,
          0.9017691311581855,
          0.9022539347709475,
          0.9000176498751091,
          0.900745438118577,
          0.9003396181784853,
          0.9004367834206792,
          0.9014613092688006,
          0.8993320218544364,
          0.8998838155642099,
          0.9022650119739735,
          0.9045475885593276,
          0.9020366156556534,
          0.9034215493021167,
          0.9019910236385955,
          0.9005038216732851,
          0.9003797856094813,
          0.9019020321003604,
          0.9015143597376084,
          0.9029978338933848,
          0.9006768220620879,
          0.8989888126849584,
          0.9030597449915407,
          0.902336411140544,
          0.9845049286797478,
          0.9844794224514358,
          0.9840603884479437,
          0.9843627180253552,
          0.9841137997521303,
          0.9844242043079374,
          0.9844850146783419,
          0.9841940059609922,
          0.9842346150362629,
          0.9842620235940033,
          0.9842424995781003,
          0.9842730600191565,
          0.9844254420100511,
          0.9845630541125974,
          0.9848285205594906,
          0.9841561161851586,
          0.985102390224469,
          0.9843942714955958,
          0.9849800739371344,
          0.9846653157615926,
          0.9843856735851537,
          0.9841941325726766,
          0.9842212790196962,
          0.984266251850799,
          0.9843530608323251,
          0.9848227429212489,
          0.9849095755369301,
          0.9843834866240734,
          0.9847602367001093,
          0.9843080544241676,
          0.9843651678531511,
          0.9845434324028195,
          0.9846932041639141,
          0.9842606870054713,
          0.9847214350763649,
          0.9841674910750103,
          0.9845864878154188,
          0.9841758022470399,
          0.9845749814933451,
          0.9851115219608225,
          0.9840120024082692,
          0.984735439559884,
          0.98471103340335,
          0.9844021096604865,
          0.9845746230384693,
          0.9846161630800565,
          0.984276213088148,
          0.9843884095260406,
          0.9842207827917389,
          0.9844532820243963,
          0.9848628741597112,
          0.9850459019746909,
          0.9842579722528841,
          0.9844775318150405,
          0.9844692592423481,
          0.984358874467701,
          0.9843905335192632,
          0.9841278327065175,
          0.9839756511369704,
          0.9846816129731825,
          0.9840641797901774,
          0.9850377766557837,
          0.9849343726662927,
          0.984007359294139,
          0.9841735532527958,
          0.9842666901678228,
          0.9845288605669953,
          0.9849375608822742,
          0.984889354353942,
          0.9845271126442947,
          0.9845530261873354,
          0.9844161155814396,
          0.9843787237915462,
          0.9845751792849557,
          0.9846756373180036,
          0.9842685431769631,
          0.9840109024045106,
          0.9845238455987815,
          0.9845993410937407,
          0.9840580747649688,
          0.9842334884716969,
          0.9842327179770815,
          0.9843901060119085,
          0.9844088422098392,
          0.9837492140500956,
          0.9841736143077278,
          0.9845187504698067,
          0.9851777676788447,
          0.9845520635476823,
          0.9849101893975104,
          0.9844134293579038,
          0.9842966999301919,
          0.9843050713667051,
          0.9844352474481093,
          0.98443289829935,
          0.9846570170396893,
          0.9843385622372238,
          0.983851686229664,
          0.984714429889376,
          0.9846026238487956
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc"
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "paper_bgcolor": "rgba(240, 240, 240, 100)",
        "plot_bgcolor": "rgba(0, 0, 0, 00)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Metric Values Across 100 Runs"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Metric"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"4153d486-5640-4be1-aee7-5d28dbe913dc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4153d486-5640-4be1-aee7-5d28dbe913dc\")) {                    Plotly.newPlot(                        \"4153d486-5640-4be1-aee7-5d28dbe913dc\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"value=%{x}<br>metric=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#00E800\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"type\":\"box\",\"x\":[0.8758471749130877,0.8758120588545142,0.8757945008252274,0.875829616883801,0.8758822909716614,0.875829616883801,0.8758471749130877,0.8758471749130877,0.875741826737367,0.8758647329423745,0.8758120588545142,0.8758998490009481,0.8758471749130877,0.8758471749130877,0.8758120588545142,0.8758120588545142,0.8758822909716614,0.8758120588545142,0.875829616883801,0.875829616883801,0.875829616883801,0.8758120588545142,0.8758822909716614,0.875829616883801,0.8758120588545142,0.875741826737367,0.8758120588545142,0.8758647329423745,0.8759174070302349,0.8757945008252274,0.8758471749130877,0.8758822909716614,0.8757769427959405,0.875829616883801,0.875829616883801,0.8759700811180953,0.8758120588545142,0.875829616883801,0.8758647329423745,0.875829616883801,0.8759349650595217,0.8758822909716614,0.875829616883801,0.8757769427959405,0.8757945008252274,0.8757769427959405,0.8758471749130877,0.8757945008252274,0.8757945008252274,0.8759525230888086,0.8758647329423745,0.875741826737367,0.8758822909716614,0.8758647329423745,0.8758822909716614,0.875829616883801,0.875829616883801,0.8757945008252274,0.8758471749130877,0.8758647329423745,0.8758471749130877,0.8758471749130877,0.875829616883801,0.8759349650595217,0.875829616883801,0.8757945008252274,0.8758998490009481,0.8758647329423745,0.8758120588545142,0.8758822909716614,0.8758822909716614,0.8758471749130877,0.8758120588545142,0.8759174070302349,0.875829616883801,0.875829616883801,0.8758998490009481,0.8757945008252274,0.8759174070302349,0.8758822909716614,0.8758471749130877,0.8758471749130877,0.8757945008252274,0.8757593847666538,0.8757769427959405,0.875829616883801,0.8757945008252274,0.875741826737367,0.8757945008252274,0.8757945008252274,0.8757945008252274,0.8758822909716614,0.8757593847666538,0.8759174070302349,0.8757769427959405,0.8758647329423745,0.8757593847666538,0.875829616883801,0.875829616883801,0.8759174070302349,0.8758451137062078,0.8758275529019229,0.8757857769973661,0.8758429334082607,0.8758626745104926,0.8758297334316721,0.8758319138848403,0.8758451137062078,0.8757924312933533,0.8758604945209328,0.8758011554197616,0.8759176648284098,0.8758319138848403,0.8758847187340838,0.8758407530337355,0.8758275529019229,0.8758758758758759,0.8758275529019229,0.8758561343121071,0.8758561343121071,0.8758165343822435,0.8758407530337355,0.8758758758758759,0.8758297334316721,0.8758011554197616,0.8757792333222119,0.8758143536972974,0.8758472939275805,0.8759066402065295,0.8758121729357637,0.8758451137062078,0.8758758758758759,0.8757835958489174,0.8758693361433088,0.8758429334082607,0.8759659969088099,0.8758011554197616,0.8758297334316721,0.8758604945209328,0.8758429334082607,0.8759220231822972,0.8758626745104926,0.8758297334316721,0.8757703991150287,0.8758253722955887,0.8757967935097547,0.8758847187340838,0.8758121729357637,0.8758121729357637,0.8759506120800182,0.8758736960415862,0.8757660362780734,0.8758758758758759,0.8758604945209328,0.8758626745104926,0.8758297334316721,0.8758297334316721,0.8758121729357637,0.8758847187340838,0.8758604945209328,0.8758583144548057,0.8758451137062078,0.8758429334082607,0.8759088195005444,0.8758429334082607,0.8758253722955887,0.8759176648284098,0.8758736960415862,0.875853954092833,0.8759022813888547,0.8758626745104926,0.8758715161307318,0.8758011554197616,0.875893437296946,0.8758429334082607,0.8758297334316721,0.8758912577710652,0.8758121729357637,0.875893437296946,0.8758758758758759,0.8758451137062078,0.8758319138848403,0.8758253722955887,0.8758474129755173,0.8757835958489174,0.8758561343121071,0.875838572582628,0.8757660362780734,0.8757857769973661,0.8757857769973661,0.8758253722955887,0.8759022813888547,0.8757682177348551,0.8759066402065295,0.8758099920976381,0.8758604945209328,0.8758342114506498,0.8758165343822435,0.8758165343822435,0.8759066402065295,0.9999799502766862,0.9999599005533724,1.0,0.9999599005533724,1.0,0.9999799502766862,1.0,0.9999799502766862,0.9999198011067447,0.9999799502766862,1.0,0.9999398508300585,1.0,0.9999198011067447,0.9999398508300585,0.9999599005533724,0.9999799502766862,0.9999599005533724,0.9999398508300585,0.9999398508300585,1.0,0.9999398508300585,0.9999799502766862,0.9999799502766862,1.0,0.9999398508300585,0.9999799502766862,1.0,0.9999799502766862,0.9999599005533724,0.9999799502766862,0.9999799502766862,0.9999799502766862,0.9999198011067447,0.9999599005533724,0.9999599005533724,1.0,0.9999799502766862,0.9999799502766862,0.9999599005533724,0.9999799502766862,1.0,0.9999799502766862,1.0,0.9999398508300585,0.9999599005533724,0.9999198011067447,0.9999599005533724,0.9999599005533724,0.9999599005533724,0.9999599005533724,0.9999599005533724,0.9999799502766862,0.9999799502766862,1.0,0.9999799502766862,0.9999799502766862,0.9999599005533724,0.9999198011067447,0.9999799502766862,0.9999599005533724,0.9999799502766862,0.9999599005533724,1.0,0.9999599005533724,0.9999398508300585,0.9999398508300585,0.9999599005533724,0.9999198011067447,0.9999398508300585,1.0,0.9999398508300585,1.0,1.0,0.9999599005533724,0.9999799502766862,0.9999799502766862,0.9999599005533724,1.0,0.9999799502766862,0.9999799502766862,1.0,0.9999398508300585,0.9998596519368033,0.9999799502766862,0.9999398508300585,0.9999198011067447,0.9999599005533724,1.0,1.0,0.9999398508300585,0.9999398508300585,0.9999799502766862,0.9999799502766862,0.9999398508300585,0.9999799502766862,0.9998797016601171,1.0,1.0,0.9999799502766862,0.901994837844931,0.9021538561152993,0.8995714797960316,0.9013614147374837,0.90046191355919,0.9011938814249731,0.9020883843358486,0.9001169325471107,0.9001175344920392,0.899588933366277,0.9004395056280973,0.9003175289241194,0.9009451124575895,0.9018550818122757,0.903245969756338,0.9002314224725185,0.9048336965247139,0.9014457946693956,0.9043531858415482,0.9020919520988484,0.9015310314876174,0.9006064554081122,0.8994239537166044,0.8997602058190495,0.9009431862338182,0.9033079771656822,0.9033659267580378,0.9009554984862034,0.9031425726137075,0.9009882400412929,0.9016517943873511,0.9015982609462595,0.9031596239431066,0.9007075325841712,0.9024344417670942,0.9003356184314778,0.9025893158236722,0.9007952763259779,0.9017957243769579,0.904779280703174,0.8998451692834566,0.9031075351698433,0.9027043496240271,0.9011366116763017,0.90238506245418,0.9019619504067176,0.900466679546683,0.9016296796388471,0.9001770548065731,0.9008009204448961,0.9040553590706593,0.9046953313389288,0.9007133169208492,0.9020849327128113,0.9014892480115309,0.901836257223934,0.9012204533986303,0.9002579717847194,0.9003794301078882,0.9031190146137159,0.8997684857485841,0.9046447042295863,0.9033459931746867,0.8991725149464199,0.9003544911754137,0.8999146733857581,0.9019880833146622,0.9038248863964207,0.9040009489144833,0.9020516076252265,0.9021808204154158,0.9008920549070766,0.9010960618332333,0.9019636698447018,0.902818332499354,0.9007585137787658,0.8994698417489332,0.9017691311581855,0.9022539347709475,0.9000176498751091,0.900745438118577,0.9003396181784853,0.9004367834206792,0.9014613092688006,0.8993320218544364,0.8998838155642099,0.9022650119739735,0.9045475885593276,0.9020366156556534,0.9034215493021167,0.9019910236385955,0.9005038216732851,0.9003797856094813,0.9019020321003604,0.9015143597376084,0.9029978338933848,0.9006768220620879,0.8989888126849584,0.9030597449915407,0.902336411140544,0.9845049286797478,0.9844794224514358,0.9840603884479437,0.9843627180253552,0.9841137997521303,0.9844242043079374,0.9844850146783419,0.9841940059609922,0.9842346150362629,0.9842620235940033,0.9842424995781003,0.9842730600191565,0.9844254420100511,0.9845630541125974,0.9848285205594906,0.9841561161851586,0.985102390224469,0.9843942714955958,0.9849800739371344,0.9846653157615926,0.9843856735851537,0.9841941325726766,0.9842212790196962,0.984266251850799,0.9843530608323251,0.9848227429212489,0.9849095755369301,0.9843834866240734,0.9847602367001093,0.9843080544241676,0.9843651678531511,0.9845434324028195,0.9846932041639141,0.9842606870054713,0.9847214350763649,0.9841674910750103,0.9845864878154188,0.9841758022470399,0.9845749814933451,0.9851115219608225,0.9840120024082692,0.984735439559884,0.98471103340335,0.9844021096604865,0.9845746230384693,0.9846161630800565,0.984276213088148,0.9843884095260406,0.9842207827917389,0.9844532820243963,0.9848628741597112,0.9850459019746909,0.9842579722528841,0.9844775318150405,0.9844692592423481,0.984358874467701,0.9843905335192632,0.9841278327065175,0.9839756511369704,0.9846816129731825,0.9840641797901774,0.9850377766557837,0.9849343726662927,0.984007359294139,0.9841735532527958,0.9842666901678228,0.9845288605669953,0.9849375608822742,0.984889354353942,0.9845271126442947,0.9845530261873354,0.9844161155814396,0.9843787237915462,0.9845751792849557,0.9846756373180036,0.9842685431769631,0.9840109024045106,0.9845238455987815,0.9845993410937407,0.9840580747649688,0.9842334884716969,0.9842327179770815,0.9843901060119085,0.9844088422098392,0.9837492140500956,0.9841736143077278,0.9845187504698067,0.9851777676788447,0.9845520635476823,0.9849101893975104,0.9844134293579038,0.9842966999301919,0.9843050713667051,0.9844352474481093,0.98443289829935,0.9846570170396893,0.9843385622372238,0.983851686229664,0.984714429889376,0.9846026238487956],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\"],\"y0\":\" \",\"yaxis\":\"y\"}],                        {\"boxmode\":\"group\",\"legend\":{\"tracegroupgap\":0},\"paper_bgcolor\":\"rgba(240, 240, 240, 100)\",\"plot_bgcolor\":\"rgba(0, 0, 0, 00)\",\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Distribution of Metric Values Across 100 Runs\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Metric\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4153d486-5640-4be1-aee7-5d28dbe913dc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model #2\n",
    "lgbm_experiment_2 = run_experiment(new_df, model_class = LGBMClassifier, **optimal_2)\n",
    "print_results(lgbm_experiment_2, color = '#00E800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "[ Experiment Results ]\n",
      "Accuracy:   0.8847383853636265\n",
      "Precision:  0.9170859508565923\n",
      "Recall:     0.9546970486807284\n",
      "ROC Auc:    0.9028618809346247\n",
      "PR Auc:     0.9847611705340068\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "value=%{x}<br>metric=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#00A4E8"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "type": "box",
         "x": [
          0.8857499034308389,
          0.8851880464936616,
          0.8841521227657407,
          0.8832566632721144,
          0.8852231625522351,
          0.882694806334937,
          0.8851529304350879,
          0.8846261895564842,
          0.8831513150963936,
          0.8821505074270464,
          0.8841872388243144,
          0.8827825964813709,
          0.8844857253221898,
          0.8855216490501107,
          0.8838360782385785,
          0.8846261895564842,
          0.885574323137971,
          0.8841170067071672,
          0.8867507111001861,
          0.8835902658285634,
          0.8846086315271974,
          0.8847842118200653,
          0.8832391052428276,
          0.8822558556027671,
          0.8851002563472276,
          0.8863117603680163,
          0.8853811848158163,
          0.8833971275064086,
          0.8856445552551182,
          0.8844330512343295,
          0.8849422340836465,
          0.8840994486778804,
          0.8862766443094428,
          0.885574323137971,
          0.8840994486778804,
          0.8847842118200653,
          0.8851353724058012,
          0.8846261895564842,
          0.8852056045229484,
          0.886083505987288,
          0.8833444534185483,
          0.8849597921129333,
          0.8845735154686238,
          0.8862942023387295,
          0.885065140288654,
          0.88590792569442,
          0.8840818906485935,
          0.8867331530708993,
          0.8835902658285634,
          0.8835024756821295,
          0.8861361800751484,
          0.8865575727780314,
          0.8826421322470765,
          0.8849422340836465,
          0.883537591740703,
          0.887014081539488,
          0.8855918811672577,
          0.884555957439337,
          0.8850124662007937,
          0.8848368859079256,
          0.8838360782385785,
          0.8863117603680163,
          0.8862239702215823,
          0.8848368859079256,
          0.8855392070793974,
          0.8842399129121747,
          0.8860659479580012,
          0.8856972293429786,
          0.885995715840854,
          0.8857674614601257,
          0.8836604979457106,
          0.8841696807950276,
          0.8842399129121747,
          0.8858728096358465,
          0.8848544439372125,
          0.8844857253221898,
          0.8841696807950276,
          0.8839941005021597,
          0.8852933946693823,
          0.8851704884643747,
          0.8845208413807635,
          0.8847139797029181,
          0.8830459669206728,
          0.8842223548828879,
          0.8844330512343295,
          0.8844857253221898,
          0.8861010640165747,
          0.8863819924851635,
          0.8846613056150577,
          0.8826772483056502,
          0.8853285107279559,
          0.8826245742177898,
          0.8840643326193068,
          0.8855918811672577,
          0.8846788636443446,
          0.8857674614601257,
          0.8845735154686238,
          0.8843101450293219,
          0.883958984443586,
          0.8855918811672577,
          0.917515451412288,
          0.9165497222168822,
          0.9170489149288825,
          0.9165333693075604,
          0.9171623832900183,
          0.9167390255668114,
          0.9160665936977936,
          0.9163221750081717,
          0.9179414779430252,
          0.9175114137584153,
          0.9161858604508039,
          0.917746489769079,
          0.9175941822109913,
          0.9169487613478997,
          0.9165864180886715,
          0.9183977121215049,
          0.917773110053379,
          0.9169332151527997,
          0.9170764502497118,
          0.9177873096839014,
          0.9170583249865166,
          0.9186545524840518,
          0.9157945363601385,
          0.9165380162099576,
          0.9167660523987228,
          0.9182583890677101,
          0.9166474432910419,
          0.9158731074815798,
          0.9174094680707698,
          0.9180571649285438,
          0.9165592969365973,
          0.9170280427869326,
          0.9178042628569228,
          0.9176604311557208,
          0.9168191712739111,
          0.9177823218488368,
          0.9161289083506184,
          0.9165624579156967,
          0.9176271055776125,
          0.9164395131876991,
          0.9163969795037756,
          0.9175887727956201,
          0.9174732945123597,
          0.9176771759633522,
          0.9161224646588814,
          0.9177871094502447,
          0.9171550859875067,
          0.9164508890039703,
          0.9166441866738603,
          0.9169575530827532,
          0.9162365942098498,
          0.9159560810163451,
          0.9152597777179556,
          0.9176354907744809,
          0.9176050217286336,
          0.9178063746705907,
          0.9183712852180625,
          0.9164759065114937,
          0.9161655940831812,
          0.9168382763332628,
          0.917938710051397,
          0.918484234885739,
          0.9184279543350817,
          0.9178999363266251,
          0.9166618616540775,
          0.9180559043406031,
          0.9175761775519239,
          0.9166282376450695,
          0.9182138932269073,
          0.9171315354133805,
          0.9154978288437152,
          0.9172595977709647,
          0.9167356684831796,
          0.9181706000308499,
          0.9168077900084673,
          0.917272219116825,
          0.9159602483516907,
          0.9164084303724377,
          0.9170242653991957,
          0.9177849760913157,
          0.916809456883772,
          0.9174377361400262,
          0.9169963525483914,
          0.9160130718954248,
          0.9161764140423732,
          0.9175297498505275,
          0.918062166380822,
          0.9190238063058714,
          0.9165977259172327,
          0.9161592415600431,
          0.9170595593187723,
          0.9168291223532591,
          0.916848087852808,
          0.917887432536623,
          0.9157035899404876,
          0.9166026281410897,
          0.9171194081609062,
          0.9173850159097483,
          0.9167902899527984,
          0.9171316427444491,
          0.955429465073382,
          0.9559307081562275,
          0.9540059347181009,
          0.9535247413585692,
          0.95520891811693,
          0.952562354639506,
          0.9564921004090143,
          0.9555096639666373,
          0.9516400673670703,
          0.9509182773277729,
          0.9551287192236747,
          0.9514195204106183,
          0.9537452883150213,
          0.9558505092629722,
          0.9541863822279253,
          0.9529232496591548,
          0.9548881225439089,
          0.95410618333467,
          0.9572339401716257,
          0.9523819071296816,
          0.9545673269708878,
          0.9528029513192718,
          0.9544269789076911,
          0.9522616087897987,
          0.9555497634132649,
          0.95520891811693,
          0.9560510064961103,
          0.9545272275242601,
          0.955429465073382,
          0.953103697168979,
          0.9556099125832064,
          0.9539658352714733,
          0.9557302109230893,
          0.9550284706071056,
          0.9542264816745529,
          0.953885636378218,
          0.9563918517924452,
          0.95520891811693,
          0.9546074264175154,
          0.957193840724998,
          0.9538054374849627,
          0.9543467800144358,
          0.9540059347181009,
          0.9559106584329137,
          0.95631165289919,
          0.9552891170101853,
          0.9537853877616489,
          0.9579958296575507,
          0.9538054374849627,
          0.9533041944021172,
          0.9575146362980191,
          0.9583968241238271,
          0.9543467800144358,
          0.9542665811211806,
          0.9525423049161922,
          0.9566524981955249,
          0.9541663325046115,
          0.9552289678402438,
          0.956191354559307,
          0.9551287192236747,
          0.9525022054695645,
          0.9549282219905365,
          0.9548881225439089,
          0.9538054374849627,
          0.9562314540059347,
          0.9528631004892133,
          0.9557502606464031,
          0.9564720506857005,
          0.954868072820595,
          0.9559307081562275,
          0.9553292164568129,
          0.9537653380383351,
          0.9545071778009463,
          0.954767824204026,
          0.9551888683936162,
          0.9541462827812976,
          0.9553893656267544,
          0.9546074264175154,
          0.9554695645200096,
          0.9543668297377497,
          0.954767824204026,
          0.9542264816745529,
          0.9526826529793889,
          0.9553893656267544,
          0.9554495147966958,
          0.9538254872082765,
          0.9551888683936162,
          0.9543467800144358,
          0.95520891811693,
          0.9532640949554896,
          0.9554695645200096,
          0.9523618574063678,
          0.9541462827812976,
          0.954767824204026,
          0.9563517523458176,
          0.9565923490255834,
          0.954447028631005,
          0.9537853877616489,
          0.9540861336113562,
          0.9557101611997755,
          0.9040401263231146,
          0.903685893771569,
          0.9013722950691097,
          0.9017826586311561,
          0.9021190240408816,
          0.9019927968975379,
          0.9033561129311912,
          0.9012935082678396,
          0.9012790247646886,
          0.9003155842879151,
          0.9018343338328448,
          0.9017456043176962,
          0.9023626715191793,
          0.9028718489443535,
          0.9035831566438484,
          0.901648912133399,
          0.9064419899808716,
          0.9028620266194607,
          0.9052259266728307,
          0.9034846090528302,
          0.9025048820708019,
          0.901486562628979,
          0.9015978615380951,
          0.9008577100539658,
          0.9025641219498162,
          0.904884240069937,
          0.9045080712288543,
          0.9019608385790261,
          0.9042239546389257,
          0.9022079844196144,
          0.9032867787898107,
          0.9026885333439874,
          0.9041578242609062,
          0.902068202892821,
          0.9036848584262919,
          0.9018470851509427,
          0.9040347937992183,
          0.9016956967095862,
          0.9027980363327048,
          0.9061238939538124,
          0.9014628148392925,
          0.9043415817596672,
          0.9038135783298105,
          0.9026420008764998,
          0.9040167779415923,
          0.9033604115261513,
          0.9018673076678594,
          0.9031928952097329,
          0.9014278878700274,
          0.9018998764293458,
          0.9055981085139748,
          0.9056974308435244,
          0.9022439538158618,
          0.903069340701872,
          0.9031286570633005,
          0.9032928789705337,
          0.9022627939839546,
          0.9015536518698613,
          0.9018090592275512,
          0.9042709927401532,
          0.9006585696755136,
          0.9056855675712387,
          0.9051437760616661,
          0.9006701780064177,
          0.9020825730886914,
          0.9016435866912078,
          0.9033256091948949,
          0.9044945494212479,
          0.9054490981091711,
          0.9036389335690971,
          0.9027440609932185,
          0.9025353574802781,
          0.90264749344693,
          0.9036192648415144,
          0.9046738879360627,
          0.9016104839691612,
          0.9005187300787388,
          0.9029643912494962,
          0.9035877923279686,
          0.9010295164672983,
          0.902542151668095,
          0.9015415549013092,
          0.9019408625054447,
          0.9026417459351184,
          0.9005454238577227,
          0.9015880831197737,
          0.9036816008419728,
          0.9056014765728926,
          0.9030190421836435,
          0.9042496201543376,
          0.9033642016546897,
          0.9017373654620509,
          0.9017371657579688,
          0.9028993925279424,
          0.9024339786235844,
          0.9039621935754727,
          0.9023651713610591,
          0.900536986714336,
          0.9042197027832188,
          0.9034249173610347,
          0.9849573731709553,
          0.9848024685438327,
          0.9844715227715535,
          0.9844987554758431,
          0.9845181175666055,
          0.9846364820266416,
          0.9847876869827811,
          0.9844686913565024,
          0.9845120610657392,
          0.9844485760360607,
          0.9845629646614693,
          0.9845865303061554,
          0.9847592713877763,
          0.9847831474907565,
          0.9849684488217577,
          0.9844543506695083,
          0.9854883077623273,
          0.9847243823200403,
          0.9851129554491468,
          0.9849903647119258,
          0.9846726497150278,
          0.9844140703283996,
          0.9846539255233,
          0.9845706556975284,
          0.9847660601838817,
          0.9851701701014632,
          0.9851269074583549,
          0.9846718214411497,
          0.9850209917142043,
          0.9846047397175187,
          0.9847550790130036,
          0.9848525233678042,
          0.9849175455504702,
          0.9845854562569935,
          0.9850294453829216,
          0.9845805650009677,
          0.9849022748335386,
          0.9844451491957469,
          0.9847991884684815,
          0.9854125870713528,
          0.9844180797966229,
          0.9850194299798312,
          0.9849780998659512,
          0.9846763054669397,
          0.9849622269637472,
          0.9849206229277745,
          0.9845564527949315,
          0.9847634526512126,
          0.984521686940308,
          0.9846895775591717,
          0.9851838912145998,
          0.9852891481583705,
          0.9846243257185078,
          0.9847342377437569,
          0.9849044910764625,
          0.9847112535442077,
          0.9846090729689879,
          0.9844170355453127,
          0.9843484213111185,
          0.985003834712102,
          0.9843103069260581,
          0.9853115455150196,
          0.985265472170803,
          0.9843438834337134,
          0.9845492790935214,
          0.9846395525976503,
          0.9848243142578739,
          0.9851196347716431,
          0.9852361480989097,
          0.9848894882538448,
          0.9847753349546094,
          0.9848124428593568,
          0.9847615525810964,
          0.9849205218659927,
          0.9850994970628401,
          0.9845139409334513,
          0.9843192148647392,
          0.9848340730374824,
          0.9848653829689024,
          0.9843335258946997,
          0.9846533549926086,
          0.9845680933685337,
          0.9847393138098942,
          0.9847264696770767,
          0.9840434400270219,
          0.9845901836842192,
          0.9848373044059695,
          0.9853733401535314,
          0.9848479253583862,
          0.98512370846225,
          0.9848140980904774,
          0.9845758863705449,
          0.9845631612521514,
          0.9846927372136542,
          0.9846657037331166,
          0.9849434356640906,
          0.9846953271891952,
          0.9842156354259822,
          0.9850511221593258,
          0.984855788681051
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Accuracy",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Precision",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "Recall",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "ROC_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc",
          "PR_auc"
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "paper_bgcolor": "rgba(240, 240, 240, 100)",
        "plot_bgcolor": "rgba(0, 0, 0, 00)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Metric Values Across 100 Runs"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Metric"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"76534bca-43a2-46cc-bcab-5e9c20f6f9c2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"76534bca-43a2-46cc-bcab-5e9c20f6f9c2\")) {                    Plotly.newPlot(                        \"76534bca-43a2-46cc-bcab-5e9c20f6f9c2\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"value=%{x}<br>metric=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#00A4E8\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"type\":\"box\",\"x\":[0.8857499034308389,0.8851880464936616,0.8841521227657407,0.8832566632721144,0.8852231625522351,0.882694806334937,0.8851529304350879,0.8846261895564842,0.8831513150963936,0.8821505074270464,0.8841872388243144,0.8827825964813709,0.8844857253221898,0.8855216490501107,0.8838360782385785,0.8846261895564842,0.885574323137971,0.8841170067071672,0.8867507111001861,0.8835902658285634,0.8846086315271974,0.8847842118200653,0.8832391052428276,0.8822558556027671,0.8851002563472276,0.8863117603680163,0.8853811848158163,0.8833971275064086,0.8856445552551182,0.8844330512343295,0.8849422340836465,0.8840994486778804,0.8862766443094428,0.885574323137971,0.8840994486778804,0.8847842118200653,0.8851353724058012,0.8846261895564842,0.8852056045229484,0.886083505987288,0.8833444534185483,0.8849597921129333,0.8845735154686238,0.8862942023387295,0.885065140288654,0.88590792569442,0.8840818906485935,0.8867331530708993,0.8835902658285634,0.8835024756821295,0.8861361800751484,0.8865575727780314,0.8826421322470765,0.8849422340836465,0.883537591740703,0.887014081539488,0.8855918811672577,0.884555957439337,0.8850124662007937,0.8848368859079256,0.8838360782385785,0.8863117603680163,0.8862239702215823,0.8848368859079256,0.8855392070793974,0.8842399129121747,0.8860659479580012,0.8856972293429786,0.885995715840854,0.8857674614601257,0.8836604979457106,0.8841696807950276,0.8842399129121747,0.8858728096358465,0.8848544439372125,0.8844857253221898,0.8841696807950276,0.8839941005021597,0.8852933946693823,0.8851704884643747,0.8845208413807635,0.8847139797029181,0.8830459669206728,0.8842223548828879,0.8844330512343295,0.8844857253221898,0.8861010640165747,0.8863819924851635,0.8846613056150577,0.8826772483056502,0.8853285107279559,0.8826245742177898,0.8840643326193068,0.8855918811672577,0.8846788636443446,0.8857674614601257,0.8845735154686238,0.8843101450293219,0.883958984443586,0.8855918811672577,0.917515451412288,0.9165497222168822,0.9170489149288825,0.9165333693075604,0.9171623832900183,0.9167390255668114,0.9160665936977936,0.9163221750081717,0.9179414779430252,0.9175114137584153,0.9161858604508039,0.917746489769079,0.9175941822109913,0.9169487613478997,0.9165864180886715,0.9183977121215049,0.917773110053379,0.9169332151527997,0.9170764502497118,0.9177873096839014,0.9170583249865166,0.9186545524840518,0.9157945363601385,0.9165380162099576,0.9167660523987228,0.9182583890677101,0.9166474432910419,0.9158731074815798,0.9174094680707698,0.9180571649285438,0.9165592969365973,0.9170280427869326,0.9178042628569228,0.9176604311557208,0.9168191712739111,0.9177823218488368,0.9161289083506184,0.9165624579156967,0.9176271055776125,0.9164395131876991,0.9163969795037756,0.9175887727956201,0.9174732945123597,0.9176771759633522,0.9161224646588814,0.9177871094502447,0.9171550859875067,0.9164508890039703,0.9166441866738603,0.9169575530827532,0.9162365942098498,0.9159560810163451,0.9152597777179556,0.9176354907744809,0.9176050217286336,0.9178063746705907,0.9183712852180625,0.9164759065114937,0.9161655940831812,0.9168382763332628,0.917938710051397,0.918484234885739,0.9184279543350817,0.9178999363266251,0.9166618616540775,0.9180559043406031,0.9175761775519239,0.9166282376450695,0.9182138932269073,0.9171315354133805,0.9154978288437152,0.9172595977709647,0.9167356684831796,0.9181706000308499,0.9168077900084673,0.917272219116825,0.9159602483516907,0.9164084303724377,0.9170242653991957,0.9177849760913157,0.916809456883772,0.9174377361400262,0.9169963525483914,0.9160130718954248,0.9161764140423732,0.9175297498505275,0.918062166380822,0.9190238063058714,0.9165977259172327,0.9161592415600431,0.9170595593187723,0.9168291223532591,0.916848087852808,0.917887432536623,0.9157035899404876,0.9166026281410897,0.9171194081609062,0.9173850159097483,0.9167902899527984,0.9171316427444491,0.955429465073382,0.9559307081562275,0.9540059347181009,0.9535247413585692,0.95520891811693,0.952562354639506,0.9564921004090143,0.9555096639666373,0.9516400673670703,0.9509182773277729,0.9551287192236747,0.9514195204106183,0.9537452883150213,0.9558505092629722,0.9541863822279253,0.9529232496591548,0.9548881225439089,0.95410618333467,0.9572339401716257,0.9523819071296816,0.9545673269708878,0.9528029513192718,0.9544269789076911,0.9522616087897987,0.9555497634132649,0.95520891811693,0.9560510064961103,0.9545272275242601,0.955429465073382,0.953103697168979,0.9556099125832064,0.9539658352714733,0.9557302109230893,0.9550284706071056,0.9542264816745529,0.953885636378218,0.9563918517924452,0.95520891811693,0.9546074264175154,0.957193840724998,0.9538054374849627,0.9543467800144358,0.9540059347181009,0.9559106584329137,0.95631165289919,0.9552891170101853,0.9537853877616489,0.9579958296575507,0.9538054374849627,0.9533041944021172,0.9575146362980191,0.9583968241238271,0.9543467800144358,0.9542665811211806,0.9525423049161922,0.9566524981955249,0.9541663325046115,0.9552289678402438,0.956191354559307,0.9551287192236747,0.9525022054695645,0.9549282219905365,0.9548881225439089,0.9538054374849627,0.9562314540059347,0.9528631004892133,0.9557502606464031,0.9564720506857005,0.954868072820595,0.9559307081562275,0.9553292164568129,0.9537653380383351,0.9545071778009463,0.954767824204026,0.9551888683936162,0.9541462827812976,0.9553893656267544,0.9546074264175154,0.9554695645200096,0.9543668297377497,0.954767824204026,0.9542264816745529,0.9526826529793889,0.9553893656267544,0.9554495147966958,0.9538254872082765,0.9551888683936162,0.9543467800144358,0.95520891811693,0.9532640949554896,0.9554695645200096,0.9523618574063678,0.9541462827812976,0.954767824204026,0.9563517523458176,0.9565923490255834,0.954447028631005,0.9537853877616489,0.9540861336113562,0.9557101611997755,0.9040401263231146,0.903685893771569,0.9013722950691097,0.9017826586311561,0.9021190240408816,0.9019927968975379,0.9033561129311912,0.9012935082678396,0.9012790247646886,0.9003155842879151,0.9018343338328448,0.9017456043176962,0.9023626715191793,0.9028718489443535,0.9035831566438484,0.901648912133399,0.9064419899808716,0.9028620266194607,0.9052259266728307,0.9034846090528302,0.9025048820708019,0.901486562628979,0.9015978615380951,0.9008577100539658,0.9025641219498162,0.904884240069937,0.9045080712288543,0.9019608385790261,0.9042239546389257,0.9022079844196144,0.9032867787898107,0.9026885333439874,0.9041578242609062,0.902068202892821,0.9036848584262919,0.9018470851509427,0.9040347937992183,0.9016956967095862,0.9027980363327048,0.9061238939538124,0.9014628148392925,0.9043415817596672,0.9038135783298105,0.9026420008764998,0.9040167779415923,0.9033604115261513,0.9018673076678594,0.9031928952097329,0.9014278878700274,0.9018998764293458,0.9055981085139748,0.9056974308435244,0.9022439538158618,0.903069340701872,0.9031286570633005,0.9032928789705337,0.9022627939839546,0.9015536518698613,0.9018090592275512,0.9042709927401532,0.9006585696755136,0.9056855675712387,0.9051437760616661,0.9006701780064177,0.9020825730886914,0.9016435866912078,0.9033256091948949,0.9044945494212479,0.9054490981091711,0.9036389335690971,0.9027440609932185,0.9025353574802781,0.90264749344693,0.9036192648415144,0.9046738879360627,0.9016104839691612,0.9005187300787388,0.9029643912494962,0.9035877923279686,0.9010295164672983,0.902542151668095,0.9015415549013092,0.9019408625054447,0.9026417459351184,0.9005454238577227,0.9015880831197737,0.9036816008419728,0.9056014765728926,0.9030190421836435,0.9042496201543376,0.9033642016546897,0.9017373654620509,0.9017371657579688,0.9028993925279424,0.9024339786235844,0.9039621935754727,0.9023651713610591,0.900536986714336,0.9042197027832188,0.9034249173610347,0.9849573731709553,0.9848024685438327,0.9844715227715535,0.9844987554758431,0.9845181175666055,0.9846364820266416,0.9847876869827811,0.9844686913565024,0.9845120610657392,0.9844485760360607,0.9845629646614693,0.9845865303061554,0.9847592713877763,0.9847831474907565,0.9849684488217577,0.9844543506695083,0.9854883077623273,0.9847243823200403,0.9851129554491468,0.9849903647119258,0.9846726497150278,0.9844140703283996,0.9846539255233,0.9845706556975284,0.9847660601838817,0.9851701701014632,0.9851269074583549,0.9846718214411497,0.9850209917142043,0.9846047397175187,0.9847550790130036,0.9848525233678042,0.9849175455504702,0.9845854562569935,0.9850294453829216,0.9845805650009677,0.9849022748335386,0.9844451491957469,0.9847991884684815,0.9854125870713528,0.9844180797966229,0.9850194299798312,0.9849780998659512,0.9846763054669397,0.9849622269637472,0.9849206229277745,0.9845564527949315,0.9847634526512126,0.984521686940308,0.9846895775591717,0.9851838912145998,0.9852891481583705,0.9846243257185078,0.9847342377437569,0.9849044910764625,0.9847112535442077,0.9846090729689879,0.9844170355453127,0.9843484213111185,0.985003834712102,0.9843103069260581,0.9853115455150196,0.985265472170803,0.9843438834337134,0.9845492790935214,0.9846395525976503,0.9848243142578739,0.9851196347716431,0.9852361480989097,0.9848894882538448,0.9847753349546094,0.9848124428593568,0.9847615525810964,0.9849205218659927,0.9850994970628401,0.9845139409334513,0.9843192148647392,0.9848340730374824,0.9848653829689024,0.9843335258946997,0.9846533549926086,0.9845680933685337,0.9847393138098942,0.9847264696770767,0.9840434400270219,0.9845901836842192,0.9848373044059695,0.9853733401535314,0.9848479253583862,0.98512370846225,0.9848140980904774,0.9845758863705449,0.9845631612521514,0.9846927372136542,0.9846657037331166,0.9849434356640906,0.9846953271891952,0.9842156354259822,0.9850511221593258,0.984855788681051],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Accuracy\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Precision\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"Recall\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"ROC_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\",\"PR_auc\"],\"y0\":\" \",\"yaxis\":\"y\"}],                        {\"boxmode\":\"group\",\"legend\":{\"tracegroupgap\":0},\"paper_bgcolor\":\"rgba(240, 240, 240, 100)\",\"plot_bgcolor\":\"rgba(0, 0, 0, 00)\",\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Distribution of Metric Values Across 100 Runs\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Metric\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('76534bca-43a2-46cc-bcab-5e9c20f6f9c2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also, the maximum results were:\n",
      "    Accuracy:   0.887014081539488\n",
      "    Precision:  0.9190238063058714\n",
      "    Recall:     0.9583968241238271\n",
      "    ROC Auc:    0.9064419899808716\n",
      "    PR Auc:     0.9854883077623273\n"
     ]
    }
   ],
   "source": [
    "# Model #3\n",
    "lgbm_experiment_3 = run_experiment(new_df, model_class = LGBMClassifier, **optimal_3)\n",
    "print_results(lgbm_experiment_3, extras=True, color = '#00A4E8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4806588217742334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4806588217742334\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7038650070406707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7038650070406707\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.983397074528167, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.983397074528167\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.841137907985995, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.841137907985995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.7038650070406707, bagging_freq=5,\n",
       "               bagging_seed=42, drop_rate=0.490746746058113,\n",
       "               feature_fraction=0.4806588217742334, lambda_l1=2.841137907985995,\n",
       "               lambda_l2=5.983397074528167, learning_rate=0.05744913989406643,\n",
       "               max_bin=384, max_depth=27, max_drop=50, metric='auc',\n",
       "               min_child_samples=3, min_child_weight=0, n_estimators=2067,\n",
       "               num_leaves=8, random_state=451,\n",
       "               scale_pos_weight=0.91024410907254, verbosity=-1)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets train a single model first\n",
    "train_x, test_x = train_test_split(new_df,\n",
    "                                   test_size = 0.3,\n",
    "                                   stratify  = new_df['gender'],\n",
    "                                   random_state = 451\n",
    "                                  )\n",
    "        \n",
    "train_y = train_x.pop('gender')\n",
    "test_y  = test_x.pop('gender')\n",
    "\n",
    "\n",
    "model = LGBMClassifier(**optimal_3)\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract shap values\n",
    "explainer   = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average feature contribution\n",
    "plt.title('Average Feature Contribution for each Class')\n",
    "shap.summary_plot(shap_values, train_x, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC_auc</th>\n",
       "      <th>PR_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.885996</td>\n",
       "      <td>0.916816</td>\n",
       "      <td>0.956612</td>\n",
       "      <td>0.903868</td>\n",
       "      <td>0.984853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall   ROC_auc    PR_auc\n",
       "0  0.885996   0.916816  0.956612  0.903868  0.984853"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = evaluate(model,test_x,test_y)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps to better this model prediction\n",
    "## --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To further enhance the predictions - One can find the best individual models and run a GridSearchCV on the overall Voting classifier\n",
    "\n",
    "#### Hyper Parameter Tuning - GridSearchCV to rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier - GridSearchCV - HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-63f061fbf8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgsABC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABC_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgsABC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled_us\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_resampled_us\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mada_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsABC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Taking - RUS - Data\n",
    "ABC = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "\n",
    "ABC_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                  \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "                  \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "                  \"n_estimators\" :[5,6,7,8,9,10,20,30,40,50],\n",
    "                  \"learning_rate\":  [0.001, 0.01, 0.1, 0,2, 0.3]}\n",
    "\n",
    "gsABC = GridSearchCV(ABC, param_grid = ABC_param_grid, cv = 5, scoring = \"roc_auc\", n_jobs = 6, verbose = 1)\n",
    "\n",
    "gsABC.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "ada_best = gsABC.best_estimator_\n",
    "print(ada_best)\n",
    "print(gsABC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ada_best.predict(X_test)\n",
    "\n",
    "print (f'model : ABC and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model : ABC and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model : ABC and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Classifer - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "ex_param_grid = {\"max_depth\": [3, 4, 5],\n",
    "                 \"max_features\": [3, 10, 15],\n",
    "                 \"min_samples_split\": [2, 3, 4],\n",
    "                 \"min_samples_leaf\": [1, 2],\n",
    "                 \"bootstrap\": [False,True],\n",
    "                 \"n_estimators\" :[100,200,300],\n",
    "                 \"criterion\": [\"gini\",\"entropy\"]}\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC, param_grid = ex_param_grid, cv = 5, scoring = \"roc_auc\", n_jobs = 6, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "ext_best = gsExtC.best_estimator_\n",
    "\n",
    "print(gsExtC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ext_best.predict(X_test)\n",
    "\n",
    "print (f'model : ETC  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  ETC and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  ETC and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test = {\"max_depth\": [24,26],\n",
    "           \"max_features\": [6,8,10,15,20],\n",
    "           \"min_samples_split\": [3,4],\n",
    "           \"min_samples_leaf\": [3,4],\n",
    "           \"bootstrap\": [True],\n",
    "           \"n_estimators\" :[50,80],\n",
    "           \"criterion\": [\"gini\",\"entropy\"],\n",
    "           \"max_leaf_nodes\":[26,28],\n",
    "           \"min_impurity_decrease\":[0.0],\n",
    "           \"min_weight_fraction_leaf\":[0.0]}\n",
    "\n",
    "tuning = GridSearchCV(estimator = RandomForestClassifier(), param_grid = rf_test, scoring = 'roc_auc', n_jobs = 6, cv = 5)\n",
    "\n",
    "tuning.fit(X_resampled_us,np.ravel(y_resampled_us))\n",
    "\n",
    "rf_best = tuning.best_estimator_\n",
    "\n",
    "print(tuning.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "print (f'model : RF  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  RF and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  RF and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM = GradientBoostingClassifier()\n",
    "\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "                 'n_estimators' : [450,460,500],\n",
    "                 'learning_rate': [0.1,0.11],\n",
    "                 'max_depth': [7,8],\n",
    "                 'min_samples_leaf': [30,40],\n",
    "                 'max_features': [0.1,0.4,0.6]}\n",
    "\n",
    "gsGBC = GridSearchCV(GBM, param_grid = gb_param_grid, cv = 5, scoring = \"roc_auc\", n_jobs = 6, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "gbm_best = gsGBC.best_estimator_\n",
    "\n",
    "print(gsGBC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm_best.predict(X_test)\n",
    "\n",
    "print (f'model : GB  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  GB and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  GB and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {'learning_rate': [0.1,0.04,0.01], \n",
    "                  'max_depth': [5,6,7],\n",
    "                  'n_estimators': [350,400,450,2000], \n",
    "                  'gamma': [0,1,5,8],\n",
    "                  'subsample': [0.8,0.95,1.0]}\n",
    "\n",
    "gsXBC = GridSearchCV(XGB, param_grid = xgb_param_grid, cv = 5, scoring = \"roc_auc\", n_jobs = 6, verbose = 1)\n",
    "\n",
    "gsXBC.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "xgb_best = gsXBC.best_estimator_\n",
    "\n",
    "print(gsXBC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_best.predict(X_test)\n",
    "\n",
    "print (f'model : XGB  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  XGB and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  XGB and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM Classifier - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM = LGBMClassifier(random_state=0)\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    'num_leaves': [31, 127],\n",
    "    'reg_alpha': [0.1, 0.5],\n",
    "    'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "    'lambda_l1': [0, 1, 1.5],\n",
    "    'lambda_l2': [0, 1]\n",
    "    }\n",
    "\n",
    "gsLGBM = GridSearchCV(LGBM, param_grid= lgbm_param_grid, cv= 5,scoring = \"roc_auc\", n_jobs = 6, verbose = 1 )\n",
    "\n",
    "gsLGBM.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "lgbm_best = gsLGBM.best_estimator_\n",
    "\n",
    "print(lgbm_best.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm_best.predict(X_test)\n",
    "\n",
    "print (f'model : LGBM  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  LGBM and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  LGBM and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATB = CatBoostClassifier(verbose=False,random_state=0)\n",
    "\n",
    "catb_param_grid = {'learning_rate': [0.1,0.05,0.01], \n",
    "                  'max_depth': [5,6,7],\n",
    "                  'n_estimators': [350,400,50,100]}\n",
    "\n",
    "gsCB = GridSearchCV(CATB, param_grid= catb_param_grid, cv= 5,scoring = \"roc_auc\", n_jobs = 6)\n",
    "\n",
    "gsCB.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "cb_best = gsCB.best_estimator_\n",
    "\n",
    "print(cb_best.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cb_best.predict(X_test)\n",
    "\n",
    "print (f'model : CB  and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  CB and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  CB and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "\n",
    "lr_param_grid = {'C':np.logspace(-3,3,7), 'penalty':['l1','l2']}}\n",
    "\n",
    "gslr = GridSearchCV(LR, param_grid= catb_param_grid, cv= 5,scoring = \"roc_auc\", n_jobs = 6)\n",
    "\n",
    "gslr.fit(X_resampled_us,y_resampled_us)\n",
    "\n",
    "lr_best = gslr.best_estimator_\n",
    "\n",
    "print(lr_best.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_best.predict(X_test)\n",
    "\n",
    "print (f'model : LR and  roc_auc_score score is : {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print (f'model :  LR and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print (f'model :  LR and  average_precision_score score is : {round(average_precision_score(y_test, y_pred),4)}')\n",
    "print(\"Current model classification report: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
